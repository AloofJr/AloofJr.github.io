<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[【从入门到放弃-Java】码出高效-计算机基础-二进制和浮点数]]></title>
    <url>%2Fd839ff09.html</url>
    <content type="text"><![CDATA[前言最近在学习孤尽大神内网分享的资料时，被安利了一本书【码出高效】，这也是他在【阿里巴巴Java开发手册】一书后，结合阿里的生产经验和历史故障给出的最佳研发实践。 刚拿到书，打开目录，会觉得这是一本基础入门书，里面的东西大家都懂，但是没翻几页你就会被里面生动的案例和技术细节吸引，有很多在日常开发中会经常用到，但一直被忽略的点，这些点往往是引发故障的点。现在就将我在学习过程中的笔记和收获记录下来和大家一起分享。 本篇就从计算机入门的二进制和浮点数开始。 二进制起源简单来说，计算机是由晶体管和电路板组合起来的电子设备，信息存储和逻辑计算的元数据归根结底都是0和1的信号处理。 只有0和1，进位规则是逢二进一，借位规则是借一当二，这就是二进制。 编码方式通过符号位和数字实际值可以表示数，有以下三种基本编码方式 原码正数部分是数值本身，符号位为0；负数数值部分是数值本身，符合位为1。八位二进制数表示范围是[-127, 127]。这是最符合人类认知的编码方式。 反码正数部分是数值本身，符号位为0；负数数值部分是正数的基础上对各位取反，符号位为1。八位二进制的表示范围是[-127, 127]。 补码正数部分是数值本身，符号位为0；负数树值部分是正数的基础上对各位取反后加一，符号位为1.八位二进制的表示范围是[-128, 127]。 加减运算因为计算机中只有加法器，没有减法器。通过原码相加，结果会出错。如: 1 - 2 = 1 + ( -2 ) = -1 通过原码计算为[0000 0001] + [1000 0010] = [1000 0011] = -3，结果明显是不对的。 通过反码计算为[0000 0001] + [1111 1101] = [1111 1110] = -1，结果正确。 但是反码在某些情况出现新的问题。如： 2 - 2 = 2 + ( -2 ) = 0 通过原码计算为[0000 0010] + [1000 0010] = [1000 0100] = -4，结果不正确。 通过反码计算为[0000 0010] + [1111 1101] = [1111 1111] = -0，反码有+0和-0的区分，用反码计算也是有歧义的。 通过补码计算为[0000 0010] + [1111 1110] = [0000 0000] = 0，补码中只有0，结果符合预期。 加减法是一个高频运算，使用同一个运算器，可以减少中间变量的存储及转换成本，也降低了CPU设计的复杂度。 位运算左移：&lt;&lt;，相当于乘2 右移：&gt;&gt;，相当于除2（最后一位是奇数时，存在误差），带符号位右移动，负数最高位补1，正数最高位补0。所以可以通过使用 ((b &gt;&gt; 31) ^ (a &gt;&gt; 31)) == 0 来判断两个整数正负是否相同。 无符号右移：&gt;&gt;&gt;，无符号位右移，正数和负数最高位都补0，主要用于一些数据转换，如加密、压缩、影音编码等场景。 取反：~，按位取反，0取反是1 与：&amp;，按位与，相同位都为1才，就为1 或：|，按位或，相同位只要有一个是1，就为1 异或：^，按位异或，相同位相等的时候为1，不相同的时候为0 浮点数浮点数采用科学技术法来表示的，由符号位、有效数字、指数三部分组成。但编码过程中经常会出现丢失精度的问题，如下：12345float a = 1f;float b = 0.9f;float c = (a - b);//c = 0.100000024System.out.println(c); 常见的浮点数有单精度和双精度浮点数，占用字节数和取值范围不同。单精度四个字节，双精度八个字节。 单精度浮点数的构成是1位符号位，8位阶码位（指数），23位尾数位（有效数字）。 阶码使用移码来表示，尾数使用原码来表示。移码范围是[0, 255]去掉特殊的0（计算机认为全零是机器0）和255（计算机认为是无穷大），阶码的取值范围是[1, 254]，根据移码的定义，[x]=x+2^(n-1)，n是8，所以x的取值范围是[-126, 127] 尾数位是原码表示的，1.111….111（23个1）, 1 &lt;= a &lt; 2, 因此规格化后的尾数首个1会被省略，因此实际能表示24位尾数，最大值无限接近于2，因此单精度浮点数能表示的最大值为2^127，约等于(1.7*10^38)。 加减运算小数加减需要将小数点对齐后进行同位相加减，因此浮点数加减需要先将指数对齐，再进行同位加减 零值检测参与运算的两个数中，只要有一个是0就直接得出结果。因为浮点数运算过程比较复杂 对阶操作小数点需要对齐，当阶码大小不相等时，需要先对阶，通过移动尾数改变阶码大小，尾数向右移一位，阶码值加一，反之减一。移动尾数过程中可能存在部分二进制被移出。但如果向左移动，会使高位移出，误差更大，因此规定在对阶时，选择阶码小的数进行操作 尾数求和对接完成后，按位相加即可，如9.8*10^38 + 6.5*10^37 = 9.8*10^38 + 0.65*10^38 = 10.45^38 结果格式化求和完毕后，如果整数为不在[1, 9]，则需要左右调整，尾数向右移动称为右规，向左移动称为左规。10.45^38要调整为1.045^39 结果舍入因为对接或右规时，尾数需要移动，移出的位会被丢弃，为了减少精度损失，需要先将移出的这部分数据保存出来，称为保护位，格式化后再根据保护位进行舍入处理。 示例1.0 - 0.9121.0 - 0.9 = 1.0 + (-0.9)1.0的浮点数标识 二进制表示121.0: [0011 1111 1000 0000 0000 0000 0000 0000]-0.9: [1011 1111 0110 0110 0110 0110 0110 0110] 对阶1.0的阶码是127，-0.9的阶码是126，比较阶码后需要向右移动-0.9位数的补码，使其变成127，最高位补1。12-0.9的尾数位补码为：[0001 1001 1001 1001 1001 1010]对阶后为：[1000 1100 1100 1100 1100 1101] 尾数求和尾数转换成补码相加123 [1000 1100 1100 1100 1100 1101]+[1000 0000 0000 0000 0000 0000]=[0000 1100 1100 1100 1100 1101] 规格化尾数的最高位必须是1，所以需要将结果向左移动4位，阶码减4。1234567891011符号位为1移动后阶码等于123（二进制[1111011]）尾数为[1100 1100 1100 1100 1101 0000]隐藏最高位后是[100 1100 1100 1100 1101 0000]最终1.0-0.9二进制表示[1011 1101 1100 1100 1100 1100 1101 0000]尾数小数点后对应的十进制是 (1 + 2^-1 + 2^-4 + 2^-5 + 2^-8 + 2^-9 + 2^-12 + 2^-13 + 2^-16 + 2^-17 + 2^-19) * 2^-4 = 0.100000024 所以通过上面的实例分析，我们了解了 为什么浮点数1.0-0.9 结果为 0.100000024 总结二进制和浮点数是我们最初接触计算机时学习的基础理论，日常开发中也会经常遇到，但很容易忽视。下面总结下我学完本章内容收获到有意思的知识点： 反码和补码诞生的原因：计算机中只有加法器，没有减法器。通过原码相加，结果会出错，所以出现了反码，又因为反码存在+0和-0问题，所以出现了补码。 判断两个数符号是否相同：可以通过使用 ((b &gt;&gt; 31) ^ (a &gt;&gt; 31)) == 0 来判断两个整数正负是否相同 浮点数分单精度和双精度：单精度浮点数占四个字节32位，从左到右，1位符号位，8位阶码位（指数的移码），23位尾数位 浮点数加减运算：0值检测 -&gt; 对阶 -&gt; 尾数求和 -&gt; 规格化 -&gt; 结果舍入]]></content>
      <tags>
        <tag>Java</tag>
        <tag>码出高效</tag>
        <tag>浮点数</tag>
        <tag>二进制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【从入门到放弃-Kubernetes】Kubernetes进阶-CustomResources]]></title>
    <url>%2F12a70df6.html</url>
    <content type="text"><![CDATA[前言上文【从入门到放弃-Kubernetes】Kubernetes进阶-pod水平自动伸缩(hpa)，我们学习了如何对标准资源Deployment、replication controller、replica set等内置资源进行水平自动伸缩。 实际在生产中（一般较大的公司）不太会用到这些内置标准资源，都会使用CustomResources自定义资源进行灵活的扩展，来满足复杂多变的业务需求。 CustomResources是Kubernetes API的扩展，可以通过动态注册，控制资源在集群中的运行状态。 安装CustomResources后，就可以和内置资源如pod一样，通过kubectl创建和访问对象。 下面我们将CustomResources简称为CR。 定义CRD添加CR通常有两种方式，通过CRD（CustomResourceDefinitions）的方式和使用聚合API的方式。 CRD的方式使用简单，聚合API的方式更灵活。 选型建议如下： 更详细的对比可以在官方文档中查看详情。 这里我们使用CRD的方式创建，有两种版本的CRD定义方式分别如下 apiextensions.k8s.io/v112345678910111213141516171819202122232425262728293031323334353637383940apiVersion: apiextensions.k8s.io/v1kind: CustomResourceDefinitionmetadata: # name must match the spec fields below, and be in the form: &lt;plural&gt;.&lt;group&gt; name: crontabs.stable.example.comspec: # group name to use for REST API: /apis/&lt;group&gt;/&lt;version&gt; group: stable.example.com # list of versions supported by this CustomResourceDefinition versions: - name: v1 # Each version can be enabled/disabled by Served flag. served: true # One and only one version must be marked as the storage version. storage: true schema: openAPIV3Schema: type: object properties: spec: type: object properties: cronSpec: type: string image: type: string replicas: type: integer # either Namespaced or Cluster scope: Namespaced names: # plural name to be used in the URL: /apis/&lt;group&gt;/&lt;version&gt;/&lt;plural&gt; plural: crontabs # singular name to be used as an alias on the CLI and for display singular: crontab # kind is normally the CamelCased singular type. Your resource manifests use this. kind: CronTab # shortNames allow shorter string to match your resource on the CLI shortNames: - ct apiextensions.k8s.io/v1beta1123456789101112131415161718192021222324252627282930313233343536373839404142# Deprecated in v1.16 in favor of apiextensions.k8s.io/v1apiVersion: apiextensions.k8s.io/v1beta1kind: CustomResourceDefinitionmetadata: # name must match the spec fields below, and be in the form: &lt;plural&gt;.&lt;group&gt; name: crontabs.stable.example.comspec: # group name to use for REST API: /apis/&lt;group&gt;/&lt;version&gt; group: stable.example.com # list of versions supported by this CustomResourceDefinition versions: - name: v1 # Each version can be enabled/disabled by Served flag. served: true # One and only one version must be marked as the storage version. storage: true # either Namespaced or Cluster scope: Namespaced names: # plural name to be used in the URL: /apis/&lt;group&gt;/&lt;version&gt;/&lt;plural&gt; plural: crontabs # singular name to be used as an alias on the CLI and for display singular: crontab # kind is normally the CamelCased singular type. Your resource manifests use this. kind: CronTab # shortNames allow shorter string to match your resource on the CLI shortNames: - ct preserveUnknownFields: false validation: openAPIV3Schema: type: object properties: spec: type: object properties: cronSpec: type: string image: type: string replicas: type: integer 创建并验证1234567891011# 使用上面任意一版本的yaml文件kubectl apply -f crd.yaml# 开启api server http代理kubectl proxy --port=8080 &amp;# 访问/apis/stable.example.com/v1/namespaces/*/crontabs查看curl http://127.0.0.1:8080/apis/stable.example.com/v1/namespaces/*/crontabs# 查看crdkubectl get crd 可以看到类似以下输出 创建CR对象1234567apiVersion: "stable.example.com/v1"kind: CronTabmetadata: name: my-new-cron-objectspec: cronSpec: "* * * * */5" image: my-awesome-cron-image 12345678# 创建CR对象kubectl apply -f cr-object.yaml# 查看crontab资源kubectl get crontab# 查看crontab yaml文件kubectl get crontab -o yaml 删除CRD12345# 删除CRDkubectl delete -f crd.yaml# 再次查看crontab资源，会看到Error from server (NotFound): Unable to list "stable.example.com/v1, Resource=crontabs": the server could not find the requested resource (get crontabs.stable.example.com)kubectl get crontabs 总结本文学习了如何通过CRD来创建CR，实际上通常还需要通过Custom controllers监听资源的变化，在周期内的各个阶段做相应的处理。接下里会开始学习Custom controllers的开发。]]></content>
      <tags>
        <tag>Kubernetes</tag>
        <tag>k8s</tag>
        <tag>CustomResources</tag>
        <tag>CR</tag>
        <tag>CRD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【从入门到放弃-Kubernetes】Kubernetes进阶-pod水平自动伸缩(hpa)]]></title>
    <url>%2Fd75d9387.html</url>
    <content type="text"><![CDATA[前言前面 Kubernetes入门 系列，我们学习了k8s的基础知识和简单用法，今天开始我们继续进一步学习，了解一些复杂和高级用法。本文我们学习pod水平自动扩缩容即hpa。hpa全称是Horizontal Pod Autoscaler，可以基于CPU的利用率或其它指标自动伸缩replication controller、deployment和 replica set 中的 pod 数量。我们可以根据标准应用系统的CPU等指标的周期性变化情况，通过HPA进行动态扩缩容以此来提高集群的资源利用率。 准备hpa通过metrics-server，监控集群运行指标触发扩缩容的，使用前需要先确保metrics-server是可用状态。1minikube addons list 观察metrics-server是否是可用状态，如果是disabled。则需要执行以下命令开启。1minikube addons enable metrics-server 此时执行kubectl top命令即可查看pod、node的cpu、memory的使用消耗情况。 如果想禁用metrics-server，disable即可1minikube addons disable metrics-server 启动应用并暴露服务我们参考官方demo，使用PHP提供http服务，进行CPU密集型计算。观察CPU和pod自动扩缩容情况。 准备镜像这里创建一个CPU密集型PHP脚本，并打成镜像。 也可以直接用我打好的镜像 registry.cn-hangzhou.aliyuncs.com/larswang/php-hpa:1.0，跳过这一步，直接看Deployment。 文件及源码地址见：AloofJr 1234# DockerfileFROM php:7.4.6-apacheADD index.php /var/www/html/index.phpRUN chmod a+rx index.php 12345678// index.php&lt;?php $x = 0.0001; for ($i = 0; $i &lt;= 1000000; $i++) &#123; $x += sqrt($x); &#125; echo "OK!";?&gt; Deployment123456789101112131415161718192021222324apiVersion: apps/v1kind: Deploymentmetadata: name: php-hpa-deployment labels: app: php-hpaspec: replicas: 1 selector: matchLabels: app: php-hpa template: metadata: labels: app: php-hpa spec: containers: - name: php-hpa image: registry.cn-hangzhou.aliyuncs.com/larswang/php-hpa:1.0 ports: - containerPort: 80 resources: requests: cpu: 200m Service 暴露服务12345678910apiVersion: v1kind: Servicemetadata: name: php-hpa-servicespec: type: ClusterIP selector: app: php-hpa ports: - port: 80 HPA创建HPA创建hpa设置pod扩缩最多和最少的数量以及执行扩缩容的条件。123456789101112apiVersion: autoscaling/v1kind: HorizontalPodAutoscalermetadata: name: php-hpaspec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: php-hpa-deployment minReplicas: 1 #pod最小数量 maxReplicas: 10 #pod最大数量 targetCPUUtilizationPercentage: 10 #cpu使用率超过10%进行扩容，小于10%缩容 观察HPA运行情况1kubectl get hpa 可以看到以下指标TARGETS：目前CPU使用率是0%，10%是我们设置的扩缩容阙值。MINPODS：pod最小数量MAXPODS：pod最大数量REPLICAS：当前副本数，因为我们的cpu使用率是0%，当前副本数是1。 增加负载我们使用busybox，不断的对php-hpa-service发起http调用，增加服务端压力。 12345# 使用busybox并进入容器kubectl run -i --rm --tty load-generator --image=busybox /bin/sh# 不断发起http请求while true; do wget -q -O- http://php-hpa-service; done 查看hpa情况 cpu利用率143%了，此时REPLICAS还是1，再等一会儿 可以看到REPLICAS已经扩到了10个pod，cpu负载平均到了35%，虽然比我们配置的10%高，但是扩到了最多MAXPODS 10个，所以不会继续扩容了 降低负载停掉load-generator发压，等几分钟（可以通过–horizontal-pod-autoscaler-downscale-stabilization设置缩容冷却时间，默认五分钟）。 可以看到REPLICAS缩到了1个pod。 总结上面介绍了根据CPU利用率进行HPA的操作，其实在实际工作中，HPA是一个非常复杂的课题，因为影响应用运行状态的不单单是CPU，会和内存、ERROR数、线程数等多项指标相关。 在扩容时，还要考虑资源额度、数据库连接数等。 在缩容时则需要更小心了，要时刻关注应用的运行状态，是很容易引发容量故障的。 更复杂的还有，应用系统是否非标、是CPU密集型还是内存密集型应用、系统的流量变化周期、如何应对突发流量等等。 欢迎感兴趣或有实战经验的大神们来交流探讨。 文件用到的yaml配置可以在AloofJr找到。]]></content>
      <tags>
        <tag>Kubernetes</tag>
        <tag>k8s</tag>
        <tag>PHP</tag>
        <tag>hpa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【从入门到放弃-Kubernetes】Kubernetes入门-部署SpringBoot]]></title>
    <url>%2Fa8be4035.html</url>
    <content type="text"><![CDATA[前言在上文【从入门到放弃-Kubernetes】Kubernetes入门-部署MySQL中，我们学习了如何部署MySQL，本文我们将基于上文中的内容，学习如何部署SpringBoot应用，并调用我们部署好的MySQL服务。 SpringBoot镜像准备我们可以参考前面【从入门到放弃-Kubernetes】Kubernetes入门-应用部署中构建代码镜像的方式。 有区别的地方是nodejs中的server.js源码可以直接运行，而java需要多一步编译打包的动作。 你也可以直接用我打好的镜像：registry.cn-hangzhou.aliyuncs.com/larswang/springboot-demo:2.0，跳过这一步，直接看后面的【部署服务】章节。 准备源码首先需要有SpringBoot项目源代码，这里使用的是我的一个demo。里面只做了简单的对MySQL数据库增删改查的动作，并对外提供了http操作接口，方便我们验证。 源码在：https://github.com/AloofJr/springboot-demo 打包我们知道SpringBoot都是以fat jar的方式运行。源码在开发环境测试没问题后，就可以开始准备打包了。 这里我用的maven直接打包 1mvn clean &amp;&amp; mvn package -e -X 正常运行无报错后，你会看到 ./target/目录下生成了 demo-0.0.1-SNAPSHOT.jar 文件，这就是我们打包后执行用的SpringBoot fat jar。 注意： 其实可以将打包的流程放在Dockerfile中，但是因为网络等原因，速度非常慢，且容易出现pom依赖下载不下来，导致打包失败的情况。 这里我直接在本地打包好，跟随源文件一起上传了。 我们在标准的开发流程中，也为了节省时间，也通常是先将项目独立编译打包好后，直接使用打包好的jar文件进行部署的。 Dockerfile1234567891011121314151617# 基于openjdk基础镜像FROM openjdk:8-jdk-alpine# 添加bash命令RUN apk add --no-cache bash# copy jar文件COPY ./target/demo-0.0.1-SNAPSHOT.jar /home/admin/myapp/# 进入工作目录WORKDIR /home/admin/myapp/# 暴露8080端口EXPOSE 8080# 运行jar，启动服务CMD ["java", "-jar", "demo-0.0.1-SNAPSHOT.jar"] 这里我们基于openjdk:8-jdk-alpine镜像构建，dockerfile写好后，可以先在本地使用如下命令构建、启动测试一下。 12345# build docker imagedocker build -t springboot-demo .# rundocker run -it -p 8080:8080 --rm --name my-running-demo springboot-demo 访问http://127.0.0.1:8080/ 出现404页面就说明镜像构建没问题了。 将dockerfile放在源代码仓库的根目录，如Dockerfile 参考构建镜像，将镜像在阿里云镜像服务仓库中构建并上传。 部署服务准备MySQL服务部署服务前，请保证k8s集群中，MySQL服务是启动的，如果没启动，请参考【从入门到放弃-Kubernetes】Kubernetes入门-部署MySQL部署。 在MySQL的主库中，准备我们demo中要用到的数据库及表结构。 12345# 进入MySQL主库的podkubectl exec -it mysql-0 bash# 通过客户端连接mysql -uroot -p 导入demo.sql 123456789101112CREATE DATABASE IF NOT EXISTS `test` /*!40100 DEFAULT CHARACTER SET utf8 */ /*!80016 DEFAULT ENCRYPTION='N' */;USE test;CREATE TABLE IF NOT EXISTS `users` ( `id` bigint NOT NULL AUTO_INCREMENT COMMENT '主键id', `userName` varchar(32) DEFAULT NULL COMMENT '用户名', `passWord` varchar(32) DEFAULT NULL COMMENT '密码', `user_sex` varchar(32) DEFAULT NULL, `nick_name` varchar(32) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8; 这里要特别注意，因为我们的MySQL服务，在K8s集群中，提供服务的service名称为mysql,所以要注意下我们SpringBoot项目连接MySQL服务的地址和账号密码，避免连接失败导致无法读写。如果你使用的是自己的MySQL服务，请记得修改application.properties文件。 DeploymentSpringBoot服务是无状态的，我们采用Deployment的方式部署。yaml文件如下。123456789101112131415161718192021apiVersion: apps/v1kind: Deploymentmetadata: name: springboot-deployment labels: app: springbootspec: replicas: 2 selector: matchLabels: app: springboot template: metadata: labels: app: springboot spec: containers: - name: springboot image: registry.cn-hangzhou.aliyuncs.com/larswang/springboot-demo:2.0 ports: - containerPort: 8080 apply123kubectl apply -f springboot.yamlkubectl get pods 查看pod状态，直到MySQL和SpringBoot服务都准备就绪 对外暴露服务123kubectl expose deployment springboot-deployment --name=my-springboot --type=LoadBalancerminikube tunnel 访问服务此时访问http://127.0.0.1:8080/ 出现404页面就说明，我们的服务已经ready了。（因为我们的demo中，没有设置默认index页面，因此直接访问会404） 写入数据访问http://127.0.0.1:8080/addUsers，是我们demo中，写入mock数据的接口。返回true。此时查看数据库记录。 读取数据访问http://127.0.0.1:8080/getUsersByCondition?id=3，返回数据库中的结果。 总结本文，我们学习了SpringBoot项目的打包和构建镜像，以及如何和MySQL服务部署在同一个k8s集群中，并正常访问MySQL服务的。 操作中如果遇到问题，可以回过头翻看下之前的系列文章，值得注意的是，在SpringBoot项目中，链接MySQL用的host，一定要是service名，而不是pod名，k8s是通过service提供服务的。我在操作的时候，就因为这个点，使用了pod名mysql-0，就一直访问不通，浪费了较多时间。 至此，我们学习了无状态应用、有状态应用 以及 无状态应用和有状态应用组合部署。应该对k8s的基础用法有了比较清晰的认识。希望大家能多实际操作，多练。]]></content>
      <tags>
        <tag>Java</tag>
        <tag>Kubernetes</tag>
        <tag>k8s</tag>
        <tag>MySQL</tag>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【从入门到放弃-Kubernetes】Kubernetes入门-部署MySQL]]></title>
    <url>%2Fde774324.html</url>
    <content type="text"><![CDATA[前言上文【从入门到放弃-Kubernetes】Kubernetes入门-有状态应用扩缩容中，介绍了如何部署有状态应用。有状态应用中，MySQL是我们最常见也是最常用的。本文我们就实战部署一个一主多从的MySQL集群。 配置准备configMap12345678910111213141516#application/mysql/mysql-configmap.yamlapiVersion: v1kind: ConfigMapmetadata: name: mysql labels: app: mysqldata: master.cnf: | # Apply this config only on the master. [mysqld] log-bin slave.cnf: | # Apply this config only on slaves. [mysqld] super-read-only configMap可以将配置文件和镜像解耦开。上面的配置意思是，创建一个master.cnf文件配置内容为：log-bin，即开启bin-log日志，供主节点使用。创建一个slave.cnf文件配置内容为：super-read-only，设为该节点只读，供备用节点使用。 service123456789101112131415161718192021222324252627282930# application/mysql/mysql-services.yaml# Headless service for stable DNS entries of StatefulSet members.apiVersion: v1kind: Servicemetadata: name: mysql labels: app: mysqlspec: ports: - name: mysql port: 3306 clusterIP: None selector: app: mysql---# Client service for connecting to any MySQL instance for reads.# For writes, you must instead connect to the master: mysql-0.mysql.apiVersion: v1kind: Servicemetadata: name: mysql-read labels: app: mysqlspec: ports: - name: mysql port: 3306 selector: app: mysql 创建一个服务名为mysql的headless类型的service。创建一个服务名为mysql-read的service StatefulSet123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181#application/mysql/mysql-statefulset.yamlapiVersion: apps/v1kind: StatefulSetmetadata: name: mysqlspec: selector: matchLabels: app: mysql serviceName: mysql replicas: 3 template: metadata: labels: app: mysql spec: # 设置初始化容器，进行一些准备工作 initContainers: - name: init-mysql image: mysql:5.7 # 为每个MySQL节点配置service-id # 如果节点序号是0，则使用master的配置， 其余节点使用slave的配置 command: - bash - "-c" - | set -ex # Generate mysql server-id from pod ordinal index. [[ `hostname` =~ -([0-9]+)$ ]] || exit 1 ordinal=$&#123;BASH_REMATCH[1]&#125; echo [mysqld] &gt; /mnt/conf.d/server-id.cnf # Add an offset to avoid reserved server-id=0 value. echo server-id=$((100 + $ordinal)) &gt;&gt; /mnt/conf.d/server-id.cnf # Copy appropriate conf.d files from config-map to emptyDir. if [[ $ordinal -eq 0 ]]; then cp /mnt/config-map/master.cnf /mnt/conf.d/ else cp /mnt/config-map/slave.cnf /mnt/conf.d/ fi volumeMounts: - name: conf mountPath: /mnt/conf.d - name: config-map mountPath: /mnt/config-map - name: clone-mysql image: gcr.io/google-samples/xtrabackup:1.0 # 为除了节点序号为0的主节点外的其它节点，备份前一个节点的数据 command: - bash - "-c" - | set -ex # Skip the clone if data already exists. [[ -d /var/lib/mysql/mysql ]] &amp;&amp; exit 0 # Skip the clone on master (ordinal index 0). [[ `hostname` =~ -([0-9]+)$ ]] || exit 1 ordinal=$&#123;BASH_REMATCH[1]&#125; [[ $ordinal -eq 0 ]] &amp;&amp; exit 0 # Clone data from previous peer. ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql # Prepare the backup. xtrabackup --prepare --target-dir=/var/lib/mysql volumeMounts: - name: data mountPath: /var/lib/mysql subPath: mysql - name: conf mountPath: /etc/mysql/conf.d containers: - name: mysql image: mysql:5.7 # 设置支持免密登录 env: - name: MYSQL_ALLOW_EMPTY_PASSWORD value: "1" ports: - name: mysql containerPort: 3306 volumeMounts: - name: data mountPath: /var/lib/mysql subPath: mysql - name: conf mountPath: /etc/mysql/conf.d resources: # 设置启动pod需要的资源，官方文档上需要500m cpu，1Gi memory。 # 我本地测试的时候，会因为资源不足，报1 Insufficient cpu, 1 Insufficient memory错误，所以我改小了点 requests: # m是千分之一的意思，100m表示需要0.1个cpu cpu: 100m # Mi是兆的意思，需要100M 内存 memory: 100Mi livenessProbe: # 使用mysqladmin ping命令，对MySQL节点进行探活检测 # 在节点部署完30秒后开始，每10秒检测一次，超时时间为5秒 exec: command: ["mysqladmin", "ping"] initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 readinessProbe: # 对节点服务可用性进行检测， 启动5秒后开始，每2秒检测一次，超时时间1秒 exec: # Check we can execute queries over TCP (skip-networking is off). command: ["mysql", "-h", "127.0.0.1", "-e", "SELECT 1"] initialDelaySeconds: 5 periodSeconds: 2 timeoutSeconds: 1 - name: xtrabackup image: gcr.io/google-samples/xtrabackup:1.0 ports: - name: xtrabackup containerPort: 3307 # 开始进行备份文件校验、解析和开始同步 command: - bash - "-c" - | set -ex cd /var/lib/mysql # Determine binlog position of cloned data, if any. if [[ -f xtrabackup_slave_info &amp;&amp; "x$(&lt;xtrabackup_slave_info)" != "x" ]]; then # XtraBackup already generated a partial "CHANGE MASTER TO" query # because we're cloning from an existing slave. (Need to remove the tailing semicolon!) cat xtrabackup_slave_info | sed -E 's/;$//g' &gt; change_master_to.sql.in # Ignore xtrabackup_binlog_info in this case (it's useless). rm -f xtrabackup_slave_info xtrabackup_binlog_info elif [[ -f xtrabackup_binlog_info ]]; then # We're cloning directly from master. Parse binlog position. [[ `cat xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1 rm -f xtrabackup_binlog_info xtrabackup_slave_info echo "CHANGE MASTER TO MASTER_LOG_FILE='$&#123;BASH_REMATCH[1]&#125;',\ MASTER_LOG_POS=$&#123;BASH_REMATCH[2]&#125;" &gt; change_master_to.sql.in fi # Check if we need to complete a clone by starting replication. if [[ -f change_master_to.sql.in ]]; then echo "Waiting for mysqld to be ready (accepting connections)" until mysql -h 127.0.0.1 -e "SELECT 1"; do sleep 1; done echo "Initializing replication from clone position" mysql -h 127.0.0.1 \ -e "$(&lt;change_master_to.sql.in), \ MASTER_HOST='mysql-0.mysql', \ MASTER_USER='root', \ MASTER_PASSWORD='', \ MASTER_CONNECT_RETRY=10; \ START SLAVE;" || exit 1 # In case of container restart, attempt this at-most-once. mv change_master_to.sql.in change_master_to.sql.orig fi # Start a server to send backups when requested by peers. exec ncat --listen --keep-open --send-only --max-conns=1 3307 -c \ "xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1 --user=root" volumeMounts: - name: data mountPath: /var/lib/mysql subPath: mysql - name: conf mountPath: /etc/mysql/conf.d resources: requests: cpu: 100m memory: 100Mi volumes: - name: conf emptyDir: &#123;&#125; - name: config-map configMap: name: mysql # 设置PVC volumeClaimTemplates: - metadata: name: data spec: accessModes: ["ReadWriteOnce"] resources: requests: storage: 1Gi 主从节点的配置和启动都在上面的yaml文件中定义好了，接下来需要逐个创建即可。 创建所需资源12345678//创建configMapkubectl apply -f configMap.yaml//创建servicekubectl apply -f service.yaml//创建statefulSetkubectl apply -f statefulSet.yaml 执行完毕后可以使用以下命令监测创建情况。1kubectl get pods --watch 测试主库进入pod进行操作进入到pod mysql-0中，进行测试1kubectl exec -it mysql-0 bash 用mysql-client链接mysql-01mysql -h mysql-0 创建库、表1234567891011//创建数据库testcreate database test;//使用test库use test;//创建message表create table message (message varchar(50));//查看message表结构show create table message; 插入数据12345//插入insert into message value("hello aloofjr");//查看select * from message; 测试备库连接mysql-11mysql -h mysql-1.mysql 查看库、表结构1234567891011//查看数据库列表show databases;//使用test库use test;//查看表列表show tables;//查看message表结构show create table message; 读取数据12//查看select * from message; 写入数据1insert into message values("hello world"); 此时会报错 ERROR 1290 (HY000): The MySQL server is running with the –super-read-only option so it cannot execute this statement 这是因为mysql-1是一个只读备库，无法进行写操作。 测试mysql-read服务12kubectl run mysql-client-loop --image=mysql:5.7 -i -t --rm --restart=Never --\ bash -ic "while sleep 1; do mysql -h mysql-read -e 'SELECT @@server_id,NOW()'; done" 每秒查询一次数据库，可以观察到，调度到不同的server-id，即pod节点 扩缩容12345//扩容至5副本kubectl scale statefulset mysql --replicas=5//缩容只2副本kubectl scale statefulset mysql --replicas=2 清理123kubectl delete statefulset mysqlkubectl delete configmap,service,pvc -l app=mysql 总结上面就是通过k8s部署一个一主多从mysql集群的过程，其中有几个重要知识点： 通过configMap可以将配置和镜像解耦 通过initContainers在pod启动前，做一些初始化工作 通过requests设置pod所需的cpu和memory 通过livenessProbe进行pod节点探活 通过readnessProbe进行pod可用性检测 下文，我们来学习如何通过k8s部署一个可读写数据库的SpringBoot应用。 本文中用到的yaml文件见我的GitHub仓库AloofJr]]></content>
      <tags>
        <tag>Kubernetes</tag>
        <tag>k8s</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【从入门到放弃-Kubernetes】Kubernetes入门-有状态应用扩缩容]]></title>
    <url>%2F404dae0e.html</url>
    <content type="text"><![CDATA[前言前文【从入门到放弃-Kubernetes】Kubernetes入门-无状态应用扩缩容中，我们学习了如何通过yaml文件部署一个无状态应用，并对其进行扩缩容。 无状态应用，在出现故障、或者pod删除时，相关资源都会释放，如果我们想保留pod中的资源，或者部署如MySQL等的数据相关的有状态应用时，删除资源显然是不合理的。本文我们来学习有状态应用的部署和扩缩容。 StatefulSetStatefulSet和Deployment类似，都是用来管理基于相同容器定义的一组pod，区别是StatefulSet为每个pod维护了一个固定的ID，这些pod是基于相同的声明创建的，但是缺不能互相替换，每个POD都有一个固定不变的ID标识。 在 StatefulSet 对象 中定义你期望的状态，然后 StatefulSet 的 控制器 就会通过各种更新来达到那种你想要的状态。 StatefulSets 对于需要满足以下一个或多个需求的应用程序很有价值： 稳定的、唯一的网络标识符。 稳定的、持久的存储。 有序的、优雅的部署和缩放。 有序的、自动的滚动更新。创建12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# web.yamlapiVersion: v1kind: Servicemetadata: name: nginx labels: app: nginxspec: ports: - port: 80 name: web clusterIP: None selector: app: nginx---apiVersion: apps/v1kind: StatefulSetmetadata: name: webspec: serviceName: "nginx" replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: registry.cn-hangzhou.aliyuncs.com/larswang/nginx:1.0 ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html imagePullSecrets: - name: registry-secret-aliyun volumeClaimTemplates: - metadata: name: www spec: accessModes: [ "ReadWriteOnce" ] resources: requests: storage: 1Gi 创建1kubectl apply -f web.yaml 镜像权限问题直接执行命令创建，可能会因为仓库权限的原因，拉不下来镜像。可以通过下面的方式解决。 12345kubectl create secret docker-registry registry-secret-aliyun --docker-server=registry.cn-hangzhou.aliyuncs.com --docker-username=xxx@xx.com --docker-password=xxxx//换成你自己的阿里云镜像仓库账号密码。kubectl describe secret registry-secret-aliyun//查看secret 在yaml文件中，containers同级位置添加12imagePullSecrets:- name: registry-secret-aliyun 顺序启动再次运行创建命令，在另一个终端使用下面命令查看运行情况。12345//查看statefulsetkubectl get statefulset web//查看podkubectl get pods -l app=nginx 每个pod 会加顺序标识，且按顺序启动的，web-0启动完，web-1再启动。 网络标识每个 Pod 都拥有一个基于其顺序索引的稳定的主机名。使用kubectl exec在每个 Pod 中执行hostname。1for i in 0 1; do kubectl exec web-$i -- sh -c 'hostname'; done 使用 kubectl run 运行一个提供 nslookup 命令的容器，该命令来自于 dnsutils 包。通过对 Pod 的主机名执行 nslookup，可以检查他们在集群内部的 DNS 地址。1kubectl run -i --tty --image busybox:1.28 dns-test --restart=Never --rm PersistentVolumeClaimsPersistentVolumes是持久化卷，属于集群中的资源。PVC 是对这些资源的请求，也作为对资源的请求的检查这是有状态应用在pod被删除后，能保留数据的主要原因。 查看pod使用的pvc1kubectl get pvc -l app=nginx NGINX web 服务器默认会加载位于 /usr/share/nginx/html/index.html 的 index 文件。StatefulSets spec 中的 volumeMounts 字段保证了 /usr/share/nginx/html 文件夹由一个 PersistentVolume 支持。 写入文件1for i in 0 1; do kubectl exec web-$i -- sh -c 'echo $(hostname) &gt; /usr/share/nginx/html/index.html'; done 查看文件1for i in 0 1; do kubectl exec -it web-$i -- cat /usr/share/nginx/html/index.html; done 删除pod1kubectl delete pod -l app=nginx pod删除后，statefulset 会自动拉起可以另起一个终端 观察pod情况 再次查看1for i in 0 1; do kubectl exec -it web-$i -- cat /usr/share/nginx/html/index.html; done 可以看到，pod被删除后，再次创建新的pod，之前写入的文件没有丢失。 扩缩容扩容1kubectl scale sts web --replicas=5 缩容1kubectl scale sts web --replicas=2 可以看到 扩容是按pod顺序，从小到大 依次扩容。缩容是按照pod顺序，从大到小，依次缩容。 查看pvc缩容后查看StatefulSet的pvc 发现pvc还依然保留 不会被删除。 清理现场清理statefulset1kubectl delete statefulset web 清理service1kubectl delete service nginx 清理pvc123k delete pvc www-web-0k delete pv pvc-d52ff063-937c-4246-864a-c6fab808e2ff 总结本文我们学习了如何创建一个有状态应用，主要使用了StatefulSet的pod顺序索引和PersistentVolumeClaims的持久性存储的特性。画重点： 每个pod都有一个顺序标识 启动&amp;扩容时，按标识从小到大依次启动 停止&amp;缩容时，按标识从大到小依次停止 pod节点挂掉后，存储资源不会被删除，新创建的同名pod可以继续使用相关资源 本文中用到的yaml文件见我的GitHub仓库AloofJr]]></content>
      <tags>
        <tag>Kubernetes</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【从入门到放弃-Kubernetes】Kubernetes入门-对外暴露服务]]></title>
    <url>%2F7a09b302.html</url>
    <content type="text"><![CDATA[前言上文【从入门到放弃-Kubernetes】Kubernetes入门-无状态应用扩缩容中介绍了如何通过yaml配置文件部署无状态的应用，及如何对其扩缩容。部署后的服务可以通过dashboard在k8s集群中看到。但还没有对外暴露 提供服务。本文，我们学习下k8s对外暴露服务的几种方式。 服务类型（type）可以在service的yaml文件中设置type配置，type的取值及含义如下： ClusterIP通过集群的内部 IP 暴露服务，选择该值，服务只能够在集群内部可以访问，这也是默认的 ServiceType。12345678910apiVersion: v1kind: Servicemetadata: name: clusterip-servicespec: type: ClusterIP selector: app: node ports: - port: 8080 注意：直接访问 http://10.104.145.1:8080 是不行的，10.104.145.1是一个内部IP。 NodePort通过每个 Node 上的 IP 和静态端口（NodePort）暴露服务。NodePort 服务会路由到 ClusterIP 服务，这个 ClusterIP 服务会自动创建。通过请求 NodeIP:NodePort，可以从集群的外部访问一个 NodePort 服务。1234567891011apiVersion: v1kind: Servicemetadata: name: nodeport-servicespec: type: NodePort selector: app: node ports: - port: 8080 nodePort: 30001 如果没有指定.spec.ports[].nodePort，则会在–service-node-port-range 标志指定的范围内分配端口（默认值：30000-32767） 官方文档显示，此时Service 就能够通过 NodeIP:spec.ports[].nodePort 和 spec.clusterIp:spec.ports[].port 而对外可见了。 但我本地测试，两种方式均无法访问，查阅资料应该和这个issue有关，具体原因还没深追，issue中提到minikube tunnel的方式，测试可用。 LoadBalancer使用外部负载均衡器方式访问，来自外部负载均衡器的流量将直接打到 backend Pod 上，不过实际它们是如何工作的，这要依赖于云提供商。1234567891011apiVersion: v1kind: Servicemetadata: name: loadbalancer-servicespec: type: LoadBalancer selector: app: node ports: - port: 8080 nodePort: 30003 因为我们没有使用外部服务器，所以EXTERNAL-IP是空，无法直接访问，因此需要使用1minikube tunnel 可以看到EXTERNAL-IP已经有了，这时候访问： http://127.0.0.1:8080/ ，可以看到我们service.js中的输出。 ExternalNameExternalName可以将集群外的服务映射为集群内的资源1234567apiVersion: v1kind: Servicemetadata: name: externalname-servicespec: type: ExternalName externalName: nc2era.com 映射成功后，我们进入到pod中。 可以看到 ping nc2era.com和ping externalname-service是一样的。 expose基于deployment对一个deployment创建service1kubectl expose deployment node-deployment --name=my-service --type=LoadBalancer 访问 http://127.0.0.1:8080 可以看到service.js的输出。 基于service在一个service的基础上创建另一个service12345//先创建一个nodeport类型的service，此时无法访问 http://127.0.0.1:8080kubectl expose deployment node-deployment --name=node-service --type=NodePort//基于node-service，创建一个LoadBalancer service， 此时可以访问 http://127.0.0.1:8080 了kubectl expose service node-service --name=my-service --type=LoadBalancer port-forwardkubectl port-forward 通过端口转发映射本地端口到指定的应用端口，从而访问集群中的应用程序(Pod). 转发pod端口123kubectl get podskubectl port-forward pod/node-deployment-8cd5587f7-xw9nb 8088:8080 查看pods列表，选择一个进行端口转发 此时可以访问 http://127.0.0.1:8088 转发Deployment端口123kubectl get deploymentskubectl port-forward deployment/node-deployment 8088:8080 查看deployment并进行端口转发 此时可以访问 http://127.0.0.1:8088 转发service端口123456//创建一个nodeport类型的service，此时 http://127.0.0.1:8088 无法访问kubectl apply -f nodeport-service.yamlkubectl get serviceskubectl port-forward service/nodeport-service 8088:8080 创建一个nodeport类型service，并转发映射本地8088端口至service的8080端口。 此时可以访问 http://127.0.0.1:8088 总结以上就是几种可以在本地访问k8s集群服务的方式。具体细节我还没有深入了解，有不对的地方欢迎交流指出。在学习完基础入门知识后，后续文章中会逐个进行剖析。 本文中所有yaml文件都在我的git仓库AloofJr，欢迎大家使用学习。]]></content>
      <tags>
        <tag>Kubernetes</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【从入门到放弃-Kubernetes】Kubernetes入门-无状态应用扩缩容]]></title>
    <url>%2F10e50416.html</url>
    <content type="text"><![CDATA[前言上文【从入门到放弃-Kubernetes】Kubernetes入门-应用部署中，我们学习了如何通过命令行部署应用，本文我们学习如果通过yaml配置文件进行应用部署，并进行应用的扩缩容。 Kubernetes 对象本段是参考kubernetes官方手册 的学习笔记，建议初步了解下，如已了解相关概念，可跳过本段，直接看下面的操作。 Kubernetes 对象 是持久化的实体。Kubernetes 使用这些实体去表示整个集群的状态。描述了如下信息： 哪些容器化应用在运行（以及在哪个 Node 上） 可以被应用使用的资源 关于应用运行时表现的策略，比如重启策略、升级策略，以及容错策略 一但对象被创建，k8s集群就会开始持续工作以保证对象符合期望状态。 对象规约（Spec）与状态（Status）每个 Kubernetes 对象包含两个嵌套的对象字段，它们负责管理对象的配置：对象 spec 和 对象 status 。 spec 是必需的，它描述了对象的 期望状态（Desired State） —— 希望对象所具有的特征。 status 描述了对象的 实际状态（Actual State） ，它是由 Kubernetes 系统提供和更新的。在任何时刻，Kubernetes 控制面一直努力地管理着对象的实际状态以与期望状态相匹配。 描述 Kubernetes 对象一般使用yaml来描述一个k8s对象，yaml是专门用来写配置的语言。 基本语法规则如下： 大小写敏感 使用缩进表示层级关系 缩进不允许使用tab，只允许空格 缩进的空格数不重要，只要相同层级的元素左对齐即可 ‘#’表示注释 使用yaml描述k8s对象，需要以下必需字段： apiVersion - 创建该对象所使用的 Kubernetes API 的版本 kind - 想要创建的对象的类型 metadata - 帮助识别对象唯一性的数据，包括一个 name 字符串、UID 和可选的 namespace spec - 描述了对象的 期望状态（Desired State），k8s集群会持续保证对象符合描述的状态。 管理 Kubernetes 对象管理k8s对象有三种方式 命令式命令（Imperative commands）就是我们上文中使用的方式，在命令行直接操作。如：1kubectl create deployment nginx --image nginx 与对象配置相比的优点： 命令简单，易学且易于记忆。 命令仅需一步即可对集群进行更改。 与对象配置相比的缺点： 命令不与变更审查流程集成。 命令不提供与更改关联的审核跟踪。 除了实时内容外，命令不提供记录源。 命令不提供用于创建新对象的模板。 命令式对象配置（Imperative object configuration）需要操作指令和配置文件配合使用。如：1kubectl create -f nginx.yaml 与命令式命令相比的优点： 对象配置可以存储在源控制系统中，比如 Git。 对象配置可以与流程集成，例如在推送和审计之前检查更新。 对象配置提供了用于创建新对象的模板。 与命令式命令相比的缺点： 对象配置需要对对象架构有基本的了解。 对象配置需要额外的步骤来编写 YAML 文件。 与声明式对象配置相比的优点： 命令式对象配置行为更加简单易懂。 从 Kubernetes 1.5 版本开始，命令式对象配置更加成熟。 与声明式对象配置相比的缺点： 命令式对象配置更适合文件，而非目录。 对活动对象的更新必须反映在配置文件中，否则会在下一次替换时丢失。 声明式对象配置（Declarative object configuration）使用声明式对象配置，不需要在命令中显示的指定操作，这样可以将配置文件放在目录中，对目录中的文件进行不同的操作。 比如：1kubectl apply -f configs/ 与命令式对象配置相比的优点： 对活动对象所做的更改即使未合并到配置文件中，也会被保留下来。 声明性对象配置更好地支持对目录进行操作并自动检测每个文件的操作类型（创建，修补，删除）。 与命令式对象配置相比的缺点： 声明式对象配置难于调试并且出现异常时结果难以理解。 使用 diff 产生的部分更新会创建复杂的合并和补丁操作。 实践操作创建Deployment创建node-deployment.yaml文件123456789101112131415161718192021apiVersion: apps/v1kind: Deploymentmetadata: name: node-deployment labels: app: nodespec: replicas: 3 selector: matchLabels: app: node template: metadata: labels: app: node spec: containers: - name: node image: registry.cn-hangzhou.aliyuncs.com/larswang/hello-node:1.0 ports: - containerPort: 80 创建1kubectl apply -f node-deployment.yaml 查看deployments1kubectl get deployments 查看pods1kubectl get pods --show-labels 缩放Deployment扩容到十个副本1kubectl scale deployment.v1.apps/node-deployment --replicas=10 扩展完成查看deployments和pods情况123kubectl get deploymentskubectl get pods --show-labels 缩容到三个副本1kubectl scale deployment.v1.apps/node-deployment --replicas=3 此时可以看到 其中7个pod处于Terminating状态。一段时间后，再次查看，只剩下了3个正在running的pod。 自动恢复先查看pods列表1kubectl get pods --show-labels 选中其中一个pod并删除1kubectl delete pod node-deployment-57df45c5bf-d8xst 等删除成功后，再次查看pods列表1kubectl get pods --show-labels 会发现被删除的pod已经不存在了，但是Deployment又创建了一个新的pod。 这就是k8s会始终尽量保证集群的运行状态和配置描述的状态保持一致的特性。 获取Deployment描述信息1kubectl describe deployment node-deployment 可以看到Deployment的当前描述，及pod历史变化情况。NewReplicaSet： 当第一次创建 Deployment 时，它创建了一个 ReplicaSet （node-deployment-57df45c5bf） 并创建了3个副本。 Events： 先扩容到了10个pods 缩容到了3个pods 缩容到了1个pods 扩容到了3个pods 查看Deployment状态1kubectl get deployment node-deployment -o yaml 以yaml格式展示Deployment的配置情况及当前状态 总结本文我们介绍了什么是k8s对象，以及使用yaml配置，创建了一个Deployment。通过scale对Deployment进行缩放，并演示了手动删除一个pod后，k8s根据描述保持运行态与描述一致的特性。 在操作完一遍之后，回过头再看概念，会有种茅塞顿开的感觉。 文中的配置文件可以在AloofJr中找到。]]></content>
      <tags>
        <tag>Kubernetes</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【从入门到放弃-Java】工具-词频分析]]></title>
    <url>%2F8c6bcfb9.html</url>
    <content type="text"><![CDATA[前言最近有根据文件内容进行词频分析的需求，如果是纯英文的，写个程序处理比较容易，但涉及到中文词频分析，最关键的一步就是中文分词。 搜了不少文章，最后找到一篇比较好用的 Java实现中文词频统计。主要利用了ansj_seg进行中文分词，分词后再进行词频统计。 针对文章中提供的代码示例，做了稍许改动，贴在下面 做个记录。 依赖添加最新版ansj_seg依赖12345&lt;dependency&gt; &lt;groupId&gt;org.ansj&lt;/groupId&gt; &lt;artifactId&gt;ansj_seg&lt;/artifactId&gt; &lt;version&gt;5.1.6&lt;/version&gt;&lt;/dependency&gt; 代码实现代码可见 AloofJr 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121package com.my.tools.ansj;import com.alibaba.common.lang.StringUtil;import org.ansj.splitWord.analysis.ToAnalysis;import java.io.*;import java.util.*;/** * 对文件中文分词后，根据词频排序输出 * @author wq * @date 2020/4/8 */public class Analysis &#123; public static void main(String[] args) throws IOException &#123; wordFrequency(""); &#125; public static void wordFrequency(String path) throws IOException &#123; List&lt;Map.Entry&lt;String, Integer&gt;&gt; wordList = getWordList(path); wordList.forEach(entry -&gt; &#123; System.out.println(entry.getKey() + "\t" + entry.getValue()); &#125;); &#125; /** * 获取 分词-词频 列表 * */ private static List&lt;Map.Entry&lt;String, Integer&gt;&gt; getWordList(String path) throws IOException &#123; Map&lt;String, Integer&gt; map = new HashMap&lt;&gt;(16); String result = ToAnalysis.parse(getString(path)).toStringWithOutNature(); //分词后的内容，分词间使用英文逗号分隔。 String[] words = result.split(","); for (String word : words) &#123; String str = word.trim(); // 过滤空白字符 if (StringUtil.isBlank(str)) &#123; continue; &#125; // 过滤一些高频率的符号 else if (str.matches("[）|（|.|，|。|+|-|“|”|：|？|\\s]")) &#123; continue; &#125; // 此处过滤长度为1的str else if (str.length() &lt; 2) &#123; continue; &#125; if (!map.containsKey(word)) &#123; map.put(word, 1); &#125; else &#123; int n = map.get(word); map.put(word, ++n); &#125; &#125; return sortByValue(map); &#125; /** * 根据词频从高到低排序 * */ private static List&lt;Map.Entry&lt;String, Integer&gt;&gt; sortByValue(Map&lt;String, Integer&gt; map) &#123; if (map == null) &#123; return null; &#125; List&lt;Map.Entry&lt;String, Integer&gt;&gt; list = new ArrayList&lt;&gt;(); list.addAll(map.entrySet()); Collections.sort(list, new Comparator&lt;Map.Entry&lt;String, Integer&gt;&gt;() &#123; @Override public int compare(Map.Entry&lt;String, Integer&gt; o1, Map.Entry&lt;String, Integer&gt; o2) &#123; return o2.getValue().compareTo(o1.getValue()); &#125; &#125;); return list; &#125; /** * 获取文件内容 * */ private static String getString(String path) throws IOException &#123; FileInputStream inputStream = new FileInputStream(new File(path)); BufferedReader reader = new BufferedReader(new InputStreamReader(inputStream)); StringBuilder strBuilder = new StringBuilder(); String line; while ((line = reader.readLine()) != null) &#123; strBuilder.append(line); &#125; reader.close(); inputStream.close(); return strBuilder.toString(); &#125;&#125; 参考作者：Asche 出处：https://www.cnblogs.com/asche/p/9673611.html]]></content>
      <tags>
        <tag>Java</tag>
        <tag>ansj</tag>
        <tag>分词</tag>
        <tag>词频</tag>
        <tag>中文分词</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【从入门到放弃-Kubernetes】Kubernetes入门-应用部署]]></title>
    <url>%2Fd7df9610.html</url>
    <content type="text"><![CDATA[前言上文【从入门到放弃-Kubernetes】Kubernetes入门中我们了解到如何在本地使用Minikube搭建一个k8s集群，本文中，我们一起学习如何在集群中部署一个简单的应用。 我们可以通过设置k8s的部署配置，来控制k8s如何创建和更新你的应用实例，一但部署设置创建好，集群主节点就会根据你的配置来调度应用实例，部署在集群的各个节点上。 部署完成后，k8s会继续监控应用实例在各个节点的运行状态，一但出现宕机、被删除等情况，k8s会在集群其他节点上创建新的应用实例，来满足你的部署配置设置。这也是不同于以往部署方式的地方，利用k8s部署应用，可以帮你提高系统的高可用性。 准备镜像首先我们需要准备一个应用镜像，因为k8s上部署的应用需要是容器化的。 可以直接使用我构建好的registry.cn-hangzhou.aliyuncs.com/larswang/hello-node:1.0镜像，跳过本段落，直接看 创建Deployment 。 也可以参考下面的方式 构建自己的镜像。 下面我们就用docker创建一个简单的NodeJS服务镜像。（如果还没安装docker的朋友可以下载安装，相关配置教程网上很多，这里不做展开）。 代码准备可以直接使用hello-node代码仓库的内容 server.js123456789var http = require('http');var handleRequest = function(request, response) &#123; console.log('Received request for URL: ' + request.url); response.writeHead(200); response.end('Hello World!');&#125;;var www = http.createServer(handleRequest);www.listen(8080); Dockerfile1234FROM node:6.14.2EXPOSE 8080COPY server.js .CMD [ "node", "server.js" ] 构建镜像我使用的是阿里云的镜像仓库：立即访问 创建镜像仓库 关联源代码 立即构建 构建成功 本地拉取123docker pull registry.cn-hangzhou.aliyuncs.com/larswang/hello-node:1.0docker run --name node-test -p 8080:8080 -d registry.cn-hangzhou.aliyuncs.com/larswang/hello-node:1.0 本地访问：http://localhost:8080/，可以看到Hello World!输出。说明镜像构建OK。 DeploymentDeployment是用来部署无状态应用的，即描述无状态应用集群状态，比如设置部署5个副本，当其中某个副本宕机时，会自动扩容至5个副本。 1kubectl create deployment hello-node --image=registry.cn-hangzhou.aliyuncs.com/larswang/hello-node:1.0 查看deployment1kubectl get deployments 查看pod1kubectl get pods Service默认Pod 只能通过 Kubernetes 集群中的内部 IP 地址访问， 为了能在Kubernetes 虚拟网络的外部访问，需要将 Pod 暴露为 Kubernetes Service。1kubectl expose deployment hello-node --type=LoadBalancer --port=8080 通过LoadBalancer类型的方式将集群服务的8080端口暴露出去。 查看Services1kubectl get services 访问Service1minikube service hello-node 浏览器会自动打开http://127.0.0.1:51284，显示Hello World! 清理12kubectl delete service hello-nodekubectl delete deployment hello-node 清理集群中创建的资源。 总结本文我们介绍了如何创建一个远程镜像，并通过命令行的方式使用镜像部署服务。 实际上我们通常使用yaml配置文件的方式去部署服务。下文我们一起学习下如果根据yaml文件部署一个无状态应用，并对其进行扩缩容。]]></content>
      <tags>
        <tag>Kubernetes</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【从入门到放弃-Kubernetes】Kubernetes入门]]></title>
    <url>%2F67853354.html</url>
    <content type="text"><![CDATA[前言云原生势不可挡，了解云原生肯定绕不开Kubernetes，这里不对云原生、Kubernetes概念做过多描述，直接开干，在本地搭建Kubernetes环境并了解基础操作。 参考文档： kubernetes官方文档 Kubernetes Handbook——Kubernetes中文指南/云原生应用架构实践手册 操作环境: macOS 10.15.3 约定： 后续文章中统一使用Kubernetes的简称：k8s kubectlkubectl是k8s集群的命令行管理工具，可以使用kubectl对k8s集群进行管理、进行容器化应用的安装部署等。 安装1brew install kubectl mac下直接使用brew进行安装，其它环境或安装方式可见官方安装文档 使用1kubectl -h 使用-h参数列出支持命令，有以下输出说明已安装成功 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677kubectl controls the Kubernetes cluster manager. Find more information at:https://kubernetes.io/docs/reference/kubectl/overview/Basic Commands (Beginner): create Create a resource from a file or from stdin. expose 使用 replication controller, service, deployment 或者 pod并暴露它作为一个 新的 Kubernetes Service run 在集群中运行一个指定的镜像 set 为 objects 设置一个指定的特征Basic Commands (Intermediate): explain 查看资源的文档 get 显示一个或更多 resources edit 在服务器上编辑一个资源 delete Delete resources by filenames, stdin, resources and names, or byresources and label selectorDeploy Commands: rollout Manage the rollout of a resource scale Set a new size for a Deployment, ReplicaSet or ReplicationController autoscale 自动调整一个 Deployment, ReplicaSet, 或者ReplicationController 的副本数量Cluster Management Commands: certificate 修改 certificate 资源. cluster-info 显示集群信息 top Display Resource (CPU/Memory/Storage) usage. cordon 标记 node 为 unschedulable uncordon 标记 node 为 schedulable drain Drain node in preparation for maintenance taint 更新一个或者多个 node 上的 taintsTroubleshooting and Debugging Commands: describe 显示一个指定 resource 或者 group 的 resources 详情 logs 输出容器在 pod 中的日志 attach Attach 到一个运行中的 container exec 在一个 container 中执行一个命令 port-forward Forward one or more local ports to a pod proxy 运行一个 proxy 到 Kubernetes API server cp 复制 files 和 directories 到 containers和从容器中复制 files 和 directories. auth Inspect authorizationAdvanced Commands: diff Diff live version against would-be applied version apply 通过文件名或标准输入流(stdin)对资源进行配置 patch 使用 strategic merge patch 更新一个资源的 field(s) replace 通过 filename 或者 stdin替换一个资源 wait Experimental: Wait for a specific condition on one or manyresources. convert 在不同的 API versions 转换配置文件 kustomize Build a kustomization target from a directory or a remote url.Settings Commands: label 更新在这个资源上的 labels annotate 更新一个资源的注解 completion Output shell completion code for the specified shell (bash orzsh)Other Commands: alpha Commands for features in alpha api-resources Print the supported API resources on the server api-versions Print the supported API versions on the server, in the form of"group/version" config 修改 kubeconfig 文件 plugin Provides utilities for interacting with plugins. version 输出 client 和 server 的版本信息Usage: kubectl [flags] [options]Use "kubectl &lt;command&gt; --help" for more information about a given command.Use "kubectl options" for a list of global command-line options (applies to allcommands). 命令的使用方式，在后续使用时会具体介绍。 Minikube准备先执行以下命令检查你的系统是否支持虚拟化。1sysctl -a | grep -E --color 'machdep.cpu.features|VMX' 如果看到输出中有 VMX 则说明支持。 安装1brew install minikube 直接使用brew安装，其它环境或安装方式可见官方安装文档 使用启动1minikube start --driver=docker –driver 参数可以指定minikube的启动方式，有：virtualbox、hyperkit、docker、vmware等。参考文档 这里我们选择docker的方式 然后就是漫长的等待。。。 如果启动报错，提示kicbase镜像下载失败，可能因为GFW无法访问gcr.io，可以参考minikube环境搭建，亲测可用。 启动完成 查看状态使用status检查集群状态 1minikube status 出现下列输出，说明集群启动正常 12345m01host: Runningkubelet: Runningapiserver: Runningkubeconfig: Configured 关闭1minikube stop 关闭后再运行1minikube status 输出12345m01host: Stoppedkubelet: Stoppedapiserver: Stoppedkubeconfig: Stopped dashboarddashboard是k8s提供的一个web操作控制台，可以查看集群运行情况并进行管理。 1minikube dashboard 执行后，会自动在默认浏览器中打开 http://127.0.0.1:50281/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/#/overview?namespace=default 总结至此，一个使用Minikube搭建的k8s集群就已经ready了。下文中，我们将继续学习如何在集群中部署一个简单的应用。 其实可以看到，最基础的入门操作是比较简单的，官方文档也十分详细，只要跟着操作一遍，基本就能了解。 之前尝试过几次k8s的入门学习，都止步于基础概念，看了忘、忘了看，却没有真正的操作一遍。从实践入手，是亘古不变的学习法门，一定不能手懒。]]></content>
      <tags>
        <tag>Kubernetes</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【从入门到放弃-ZooKeeper】ZooKeeper实战-分布式竞选]]></title>
    <url>%2Ff5899a14.html</url>
    <content type="text"><![CDATA[前言上文【从入门到放弃-ZooKeeper】ZooKeeper实战-分布式锁-升级版中，我们通过利用ZooKeeper的临时节点和Watcher特性，实现了一个分布式锁。本文我们结合实际场景，完成一个分布式竞争选举。 设计这里我们实现一个公平的选举方式，即先参加选举的优先被选为leader。具体的实现思路 参考了ZooKeeper提供的官方示例：zookeeper-recipes-election START：服务器开始竞选 OFFER：创建临时顺序结点 DETERMINE：开始决策，将临时节点按末尾序号从小到大排序，如果当前节点的序号最小，则竞选成功，否则，则Watch前一个节点，当前一个节点被删除时，再次进行决策 ELECTED：当前节点是序号最小的节点，竞选成功 READY：当前节点不是序号最小的节点，竞选不成功，Watch前一个节点，进入READY态 FAILED：当出现异常情况时，为失败状态 STOP：结束竞选 LeaderElectionSupport123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public class LeaderElectionSupport implements LeaderElection&#123; private static Logger logger = LoggerFactory.getLogger(LeaderElectionSupport.class); //ZooKeeper客户端，进行ZooKeeper操作 private ZooKeeper zooKeeper; //根节点名称 private String dir; //节点前缀 private String node; //ZooKeeper鉴权信息 private List&lt;ACL&gt; acls; //要加锁节点 private String fullPath; //选举状态 private State state; //监听器 private Set&lt;LeaderElectionListener&gt; listeners; //存当前节点的信息 private volatile LeaderNode leaderNode; //监察器 private Watcher watcher; /** * Constructor. * * @param zooKeeper the zoo keeper * @param dir the dir * @param node the node * @param acls the acls */ public LeaderElectionSupport(ZooKeeper zooKeeper, String dir, String node, List&lt;ACL&gt; acls) &#123; this.zooKeeper = zooKeeper; this.dir = dir; this.node = node; this.acls = acls; this.fullPath = dir.concat("/").concat(this.node); init(); state = State.STOP; listeners = Collections.synchronizedSet(new HashSet&lt;&gt;()); &#125; /** * 初始化根节点、检查器等 * */ private void init() &#123; try &#123; watcher = new LeaderWatcher(); Stat stat = zooKeeper.exists(dir, false); if (stat == null) &#123; zooKeeper.create(dir, null, acls, CreateMode.PERSISTENT); &#125; &#125; catch (Exception e) &#123; logger.error("[LeaderElectionSupport#init] error : " + e.toString(), e); &#125; &#125;&#125; start1234567891011121314/** * Start. * 开始竞选 */@Overridepublic void start() &#123; synchronized (this) &#123; state = State.START; dispatchEvent(EventType.START); offerElection(); determineElection(); &#125;&#125; offerElection123456789101112131415161718192021222324252627/** * 创建临时节点，参加竞选，并将主机信息保存在node中 * */private void offerElection() &#123; dispatchEvent(EventType.OFFER_START); state = State.OFFER; if (leaderNode == null) &#123; synchronized (this) &#123; try &#123; if (leaderNode == null) &#123; InetAddress ia = InetAddress.getLocalHost(); LeaderNode tmpNode = new LeaderNode(); tmpNode.setHostName(ia.getHostName()); String path = zooKeeper.create(fullPath, ConversionUtil.objectToBytes(ia.getHostName()), acls, CreateMode.EPHEMERAL_SEQUENTIAL); tmpNode.setNodePath(path); tmpNode.setId(NodeUtil.getNodeId(path)); leaderNode = tmpNode; &#125; &#125; catch (Exception e) &#123; becomeFailed(e); &#125; &#125; &#125; dispatchEvent(EventType.OFFER_COMPLETE);&#125; determineElection1234567891011121314151617181920212223/** * 决定竞选结果 * 1、竞选节点序号最低的赢取选举 * 2、未赢得选举的节点，监听上一个节点，直到上一个节点被删除，则尝试重新竞选 * */private void determineElection() &#123; dispatchEvent(EventType.DETERMINE_START); state = State.DETERMINE; synchronized (this) &#123; TreeSet&lt;String&gt; nodePathSet = getNodePathSet(); if (nodePathSet.isEmpty()) &#123; becomeFailed(new Exception("no node")); return; &#125; String leaderPath = nodePathSet.first(); if (leaderNode.getNodePath().equalsIgnoreCase(leaderPath)) &#123; becomeLeader(); &#125; else &#123; becomeReady(nodePathSet.headSet(leaderNode.getNodePath()).last()); &#125; &#125; dispatchEvent(EventType.DETERMINE_COMPLETE);&#125; becomeLeader12345678/** * 竞选成功 * */private void becomeLeader() &#123; dispatchEvent(EventType.ELECTED_START); state = State.ELECTED; dispatchEvent(EventType.ELECTED_COMPLETE);&#125; becomeReady1234567891011121314151617181920/** * 竞选失败进入就绪态 * */private void becomeReady(String path) &#123; try &#123; Stat stat = zooKeeper.exists(path, watcher); if (stat == null) &#123; determineElection(); &#125; else &#123; dispatchEvent(EventType.READY_START); state = State.READY; dispatchEvent(EventType.READY_COMPLETE); &#125; &#125; catch (KeeperException e) &#123; becomeFailed(e); &#125; catch (InterruptedException e) &#123; becomeFailed(e); &#125;&#125; becomeFailed12345678/** * 当发生异常时，更新为FAILED状态 * */private void becomeFailed(Exception e) &#123; state = State.FAILED; dispatchEvent(EventType.FAILED); logger.error("[LeaderElectionSupport#becomeFailed] error : " + e.toString(), e);&#125; getNodePathSet1234567891011121314151617181920/** * 获取参加竞选的节点信息 * */private TreeSet&lt;String&gt; getNodePathSet() &#123; TreeSet&lt;String&gt; nodeSet = new TreeSet&lt;&gt;(); try &#123; List&lt;String&gt; nodes = zooKeeper.getChildren(dir, false); for (String node : nodes) &#123; nodeSet.add(dir.concat("/").concat(node)); &#125; &#125; catch (KeeperException e) &#123; becomeFailed(e); &#125; catch (InterruptedException e) &#123; becomeFailed(e); &#125; return nodeSet;&#125; stop12345678910111213/** * Stop. * 停止竞选 */@Overridepublic void stop() &#123; synchronized (this) &#123; dispatchEvent(EventType.STOP_START); deleteNode(); state = State.STOP; dispatchEvent(EventType.STOP_COMPLETE); &#125;&#125; deleteNode1234567891011121314151617/** * 停止时，删除节点，退出竞选 * */private void deleteNode() &#123; try &#123; if (leaderNode != null) &#123; synchronized (this) &#123; zooKeeper.delete(leaderNode.getNodePath(), -1); leaderNode = null; &#125; &#125; &#125; catch (InterruptedException e) &#123; becomeFailed(e); &#125; catch (KeeperException e) &#123; becomeFailed(e); &#125;&#125; getLeaderHostName12345678910111213141516171819202122232425262728/** * Gets get leader host name. * * @return the get leader host name */@Overridepublic String getLeaderHostName() &#123; synchronized (this) &#123; TreeSet&lt;String&gt; nodePathSet = getNodePathSet(); if (!nodePathSet.isEmpty()) &#123; try &#123; String leaderPath = nodePathSet.first(); return (String) ConversionUtil.bytesToObject(zooKeeper.getData(leaderPath, false, null)); &#125; catch (KeeperException e) &#123; logger.error("[LeaderWatcher#getLeaderHostName] error : " + e.toString(), e); &#125; catch (InterruptedException e) &#123; logger.error("[LeaderWatcher#getLeaderHostName] error : " + e.toString(), e); &#125; catch (IOException e) &#123; logger.error("[LeaderWatcher#getLeaderHostName] error : " + e.toString(), e); &#125; catch (ClassNotFoundException e) &#123; logger.error("[LeaderWatcher#getLeaderHostName] error : " + e.toString(), e); &#125; &#125; return null; &#125;&#125; getLeaderNodePath1234567891011121314/** * Gets get leader node path. * * @return the get leader node path */@Overridepublic String getLeaderNodePath() &#123; synchronized (this) &#123; TreeSet&lt;String&gt; nodePathSet = getNodePathSet(); return nodePathSet.isEmpty() ? null : nodePathSet.first(); &#125;&#125; LeaderWatcher123456789101112131415161718192021/** * 内部watcher类，当竞选失败时，watch前一个节点，当前一个节点别移除时，再次发起决策 * */private class LeaderWatcher implements Watcher &#123; /** * Process. * * @param watchedEvent the watched event */ @Override public void process(WatchedEvent watchedEvent) &#123; try &#123; if (Event.EventType.NodeDeleted.equals(watchedEvent.getType()) &amp;&amp; !State.STOP.equals(state)) &#123; determineElection(); &#125; &#125; catch (Exception e) &#123; logger.error("[LeaderWatcher#process] error : " + e.toString(), e); &#125; &#125;&#125; 总结以上就是我们利用ZooKeeper的临时节点和Watcher特性实现的公平模式分布式竞选。 可以进行简单的选主操作，适用于如执行单机定时任务、心跳检测等场景。实际上是实现的Master-Slave模型。 源代码可见：aloofJr 而对高可用要求较多的复杂选举场景，如分布式存储、同步等，则需要考虑集群一致性、脑裂等各种情况，则需要实现如Paxos、raft、Zab等一致性算法协议。如ZooKeeper集群的选举模式就是使用的Zab算法。我们后续会进行深入的探讨。]]></content>
      <tags>
        <tag>Java</tag>
        <tag>ZooKeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【从入门到放弃-ZooKeeper】ZooKeeper实战-分布式锁-升级版]]></title>
    <url>%2F9445b788.html</url>
    <content type="text"><![CDATA[前言上文【从入门到放弃-ZooKeeper】ZooKeeper实战-分布式锁中，我们通过利用ZooKeeper的临时节点特性，实现了一个分布式锁。但是是通过轮询的方式去判断不断尝试获取锁，空转对于CPU还是有一定消耗的，同时，对于多个线程竞争锁激烈的时候，很容易出现羊群效应。 为了解决上面两个问题。本文来看一下如何实现一个升级版的分布式锁。 设计我们依然实现java.util.concurrent.locks.Lock接口。和上一文中实现方式不同的是，我们使用ZooKeeper的EPHEMERAL_SEQUENTIAL临时顺序节点。当首次获取锁时，会创建一个临时节点，如果这个临时节点末尾数字是当前父节点下同名节点中最小的，则获取锁成功。否则，则监听上一个数字较大的节点，直到上一个节点被释放，则再次尝试获取锁成功。这样可以避免多个线程同时获取一把锁造成的竞争。同时使用了ZooKeeper提供的watch功能，避免了轮询带来的CPU空转。获取锁后使用一个volatile int类型的state进行计数，来实现锁的可重入机制。 DistributedFairLock12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public class DistributedFairLock implements Lock &#123; private static Logger logger = LoggerFactory.getLogger(DistributedFairLock.class); //ZooKeeper客户端，进行ZooKeeper操作 private ZooKeeper zooKeeper; //根节点名称 private String dir; //加锁节点 private String node; //ZooKeeper鉴权信息 private List&lt;ACL&gt; acls; //要加锁节点 private String fullPath; //加锁标识，为0时表示未获取到锁，每获取一次锁则加一，释放锁时减一。减到0时断开连接，删除临时节点。 private volatile int state; //当前锁创建的节点id private String id; //通过CountDownLatch阻塞，直到监听上一节点被取消，再进行后续操作 private CountDownLatch countDownLatch; /** * Constructor. * * @param zooKeeper the zoo keeper * @param dir the dir * @param node the node * @param acls the acls */ public DistributedFairLock(ZooKeeper zooKeeper, String dir, String node, List&lt;ACL&gt; acls) &#123; this.zooKeeper = zooKeeper; this.dir = dir; this.node = node; this.acls = acls; this.fullPath = dir.concat("/").concat(this.node); init(); &#125; private void init() &#123; try &#123; Stat stat = zooKeeper.exists(dir, false); if (stat == null) &#123; zooKeeper.create(dir, null, acls, CreateMode.PERSISTENT); &#125; &#125; catch (Exception e) &#123; logger.error("[DistributedFairLock#init] error : " + e.toString(), e); &#125; &#125;&#125; lock1234567891011121314151617181920212223242526272829303132333435363738394041424344public void lock() &#123; try &#123; //加锁 synchronized (this) &#123; //如果当前未持有锁 if (state &lt;= 0) &#123; //创建节点 if (id == null) &#123; id = zooKeeper.create(fullPath, null, acls, CreateMode.EPHEMERAL_SEQUENTIAL); &#125; //获取当前路径下所有的节点 List&lt;String&gt; nodes = zooKeeper.getChildren(dir, false); SortedSet&lt;String&gt; sortedSet = new TreeSet&lt;&gt;(); for (String node : nodes) &#123; sortedSet.add(dir.concat("/").concat(node)); &#125; //获取所有id小于当前节点顺序的节点 SortedSet&lt;String&gt; lessSet = ((TreeSet&lt;String&gt;) sortedSet).headSet(id); if (!lessSet.isEmpty()) &#123; //监听上一个节点，就是通过这里避免多锁竞争和CPU空转，实现公平锁的 Stat stat = zooKeeper.exists(lessSet.last(), new LockWatcher()); if (stat != null) &#123; countDownLatch = new CountDownLatch(1); countDownLatch.await(); &#125; &#125; &#125; state++; &#125; &#125; catch (InterruptedException e) &#123; logger.error("[DistributedFairLock#lock] error : " + e.toString(), e); Thread.currentThread().interrupt(); &#125; catch (KeeperException ke) &#123; logger.error("[DistributedFairLock#lock] error : " + ke.toString(), ke); if (!KeeperException.Code.NODEEXISTS.equals(ke.code())) &#123; Thread.currentThread().interrupt(); &#125; &#125;&#125; tryLock12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576public boolean tryLock() &#123; try &#123; synchronized (this) &#123; if (state &lt;= 0) &#123; if (id == null) &#123; id = zooKeeper.create(fullPath, null, acls, CreateMode.EPHEMERAL_SEQUENTIAL); &#125; List&lt;String&gt; nodes = zooKeeper.getChildren(dir, false); SortedSet&lt;String&gt; sortedSet = new TreeSet&lt;&gt;(); for (String node : nodes) &#123; sortedSet.add(dir.concat("/").concat(node)); &#125; SortedSet&lt;String&gt; lessSet = ((TreeSet&lt;String&gt;) sortedSet).headSet(id); if (!lessSet.isEmpty()) &#123; return false; &#125; &#125; state++; &#125; &#125; catch (InterruptedException e) &#123; logger.error("[DistributedFairLock#tryLock] error : " + e.toString(), e); return false; &#125; catch (KeeperException ke) &#123; logger.error("[DistributedFairLock#tryLock] error : " + ke.toString(), ke); if (!KeeperException.Code.NODEEXISTS.equals(ke.code())) &#123; return false; &#125; &#125; return true;&#125;@Overridepublic boolean tryLock(long time, TimeUnit unit) throws InterruptedException &#123; try &#123; synchronized (this) &#123; if (state &lt;= 0) &#123; if (id == null) &#123; id = zooKeeper.create(fullPath, null, acls, CreateMode.EPHEMERAL_SEQUENTIAL); &#125; List&lt;String&gt; nodes = zooKeeper.getChildren(dir, false); SortedSet&lt;String&gt; sortedSet = new TreeSet&lt;&gt;(); for (String node : nodes) &#123; sortedSet.add(dir.concat("/").concat(node)); &#125; SortedSet&lt;String&gt; lessSet = ((TreeSet&lt;String&gt;) sortedSet).headSet(id); if (!lessSet.isEmpty()) &#123; Stat stat = zooKeeper.exists(lessSet.last(), new LockWatcher()); if (stat != null) &#123; countDownLatch = new CountDownLatch(1); countDownLatch.await(time, unit); &#125; &#125; &#125; state++; &#125; &#125; catch (InterruptedException e) &#123; logger.error("[DistributedFairLock#tryLock] error : " + e.toString(), e); return false; &#125; catch (KeeperException ke) &#123; logger.error("[DistributedFairLock#tryLock] error : " + ke.toString(), ke); if (!KeeperException.Code.NODEEXISTS.equals(ke.code())) &#123; return false; &#125; &#125; return true;&#125; unlock12345678910111213141516public void unlock() &#123; synchronized (this) &#123; if (state &gt; 0) &#123; state--; &#125; //当不再持有锁时，删除创建的临时节点 if (state == 0 &amp;&amp; zooKeeper != null) &#123; try &#123; zooKeeper.delete(id, -1); id = null; &#125; catch (Exception e) &#123; logger.error("[DistributedFairLock#unlock] error : " + e.toString(), e); &#125; &#125; &#125;&#125; LockWatcher12345678910private class LockWatcher implements Watcher &#123; @Override public void process(WatchedEvent event) &#123; synchronized (this) &#123; if (countDownLatch != null) &#123; countDownLatch.countDown(); &#125; &#125; &#125;&#125; 总结上面就是我们改良后，通过临时顺序节点和watch机制实现的公平可重入分布式锁。源代码可见：aloofJr通过watch机制避免轮询带来的CPU空转。通过顺序临时节点避免了羊群效应。 如果对以上方式有更好的优化方案，欢迎一起讨论。]]></content>
      <tags>
        <tag>Java</tag>
        <tag>ZooKeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【从入门到放弃-ZooKeeper】ZooKeeper实战-分布式锁]]></title>
    <url>%2F1a9e764b.html</url>
    <content type="text"><![CDATA[前言上文【从入门到放弃-ZooKeeper】ZooKeeper实战-分布式队列中，我们一起写了下如何通过ZooKeeper的持久性顺序节点实现一个分布式队列。本文我们来一起写一个ZooKeeper的实现的分布式锁。 设计参考之前学习的【从入门到放弃-Java】并发编程-JUC-locks-ReentrantLock，实现java.util.concurrent.locks.Lock接口。我们通过重写接口中的方法实现一个可重入锁。 lock：请求锁，如果成功则直接返回，不成功则阻塞 直到获取锁。 lockInterruptibly：请求锁，如果失败则一直阻塞等待 直到获取锁或线程中断 tryLock：1、尝试获取锁，获取失败的话 直接返回false，不会再等待。2、尝试获取锁，获取成功返回true，否则一直请求，直到超时返回false unlock：释放锁 我们使用ZooKeeper的EPHEMERAL临时节点机制，如果能创建成功的话，则获取锁成功，释放锁或客户端断开连接后，临时节点自动删除，这样可以避免误删除或漏删除的情况。 获取锁失败后，这里我们使用轮询的方式来不断尝试创建。其实应该使用Watcher机制来实现，这样能避免大量的无用请求。在下一节更优雅的分布式锁实现机制中我们会用到。 DistributedLock12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class DistributedLock implements Lock &#123; private static Logger logger = LoggerFactory.getLogger(DistributedQueue.class); //ZooKeeper客户端，进行ZooKeeper操作 private ZooKeeper zooKeeper; //根节点名称 private String dir; //加锁节点 private String node; //ZooKeeper鉴权信息 private List&lt;ACL&gt; acls; //要加锁节点 private String fullPath; //加锁标识，为0时表示未获取到锁，每获取一次锁则加一，释放锁时减一。减到0时断开连接，删除临时节点。 private volatile int state; /** * Constructor. * * @param zooKeeper the zoo keeper * @param dir the dir * @param node the node * @param acls the acls */ public DistributedLock(ZooKeeper zooKeeper, String dir, String node, List&lt;ACL&gt; acls) &#123; this.zooKeeper = zooKeeper; this.dir = dir; this.node = node; this.acls = acls; this.fullPath = dir.concat("/").concat(node); init(); &#125; private void init() &#123; try &#123; Stat stat = zooKeeper.exists(dir, false); if (stat == null) &#123; zooKeeper.create(dir, null, acls, CreateMode.PERSISTENT); &#125; &#125; catch (Exception e) &#123; logger.error("[DistributedLock#init] error : " + e.toString(), e); &#125; &#125;&#125; lock1234567891011121314151617181920212223242526public void lock() &#123; //通过state实现重入机制，如果已经获取锁，则将state++即可。 if (addLockCount()) &#123; return; &#125; //一直尝试获取锁，知道获取成功 for (;;) &#123; try &#123; //创建临时节点 zooKeeper.create(fullPath, null, acls, CreateMode.EPHEMERAL); //第一次获取锁，state++，这里不需要使用加锁机制保证原子性，因为同一时间，最多只有一个线程能create节点成功。 state++; break; &#125; catch (InterruptedException ie) &#123; //如果捕获中断异常，则设置当前线程为中断状态 logger.error("[DistributedLock#lock] error : " + ie.toString(), ie); Thread.currentThread().interrupt(); &#125; catch (KeeperException ke) &#123; //如果捕获到的异常是 节点已存在 外的其他异常，则设置当前线程为中断状态 logger.error("[DistributedLock#lock] error : " + ke.toString(), ke); if (!KeeperException.Code.NODEEXISTS.equals(ke.code())) &#123; Thread.currentThread().interrupt(); &#125; &#125; &#125;&#125; lockInterruptibly123456789101112131415161718192021222324252627public void lockInterruptibly() throws InterruptedException &#123; //通过state实现重入机制，如果已经获取锁，则将state++即可。 if (addLockCount()) &#123; return; &#125; for (;;) &#123; //如果当前线程为中断状态，则抛出中断异常 if (Thread.interrupted()) &#123; throw new InterruptedException(); &#125; try &#123; zooKeeper.create(fullPath, null, acls, CreateMode.EPHEMERAL); state++; break; &#125; catch (InterruptedException ie) &#123; //如果捕获中断异常，则设置当前线程为中断状态 logger.error("[DistributedLock#lockInterruptibly] error : " + ie.toString(), ie); Thread.currentThread().interrupt(); &#125; catch (KeeperException ke) &#123; //如果捕获到的异常是 节点已存在 外的其他异常，则设置当前线程为中断状态 logger.error("[DistributedLock#lockInterruptibly] error : " + ke.toString(), ke); if (!KeeperException.Code.NODEEXISTS.equals(ke.code())) &#123; Thread.currentThread().interrupt(); &#125; &#125; &#125;&#125; tryLock1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public boolean tryLock() &#123; //通过state实现重入机制，如果已经获取锁，则将state++即可。 if (addLockCount()) &#123; return true; &#125; //如果获取成功则返回true，失败则返回false try &#123; zooKeeper.create(fullPath, null, acls, CreateMode.EPHEMERAL); state++; return true; &#125; catch (Exception e) &#123; logger.error("[DistributedLock#tryLock] error : " + e.toString(), e); &#125; return false;&#125;public boolean tryLock(long time, TimeUnit unit) throws InterruptedException &#123; //通过state实现重入机制，如果已经获取锁，则将state++即可。 if (addLockCount()) &#123; return true; &#125; //如果尝试获取超时，则返回false long nanosTimeout = unit.toNanos(time); if (nanosTimeout &lt;= 0L) &#123; return false; &#125; final long deadline = System.nanoTime() + nanosTimeout; for (;;) &#123; //如果当前线程为中断状态，则抛出中断异常 if (Thread.interrupted()) &#123; throw new InterruptedException(); &#125; //如果尝试获取超时，则返回false nanosTimeout = deadline - System.nanoTime(); if (nanosTimeout &lt;= 0L) &#123; return false; &#125; try &#123; zooKeeper.create(fullPath, null, acls, CreateMode.EPHEMERAL); state++; return true; &#125; catch (InterruptedException ie) &#123; //如果捕获中断异常，则返回false logger.error("[DistributedLock#tryLock] error : " + ie.toString(), ie); return false; &#125; catch (KeeperException ke) &#123; //如果捕获到的异常是 节点已存在 外的其他异常，则返回false logger.error("[DistributedLock#tryLock] error : " + ke.toString(), ke); if (!KeeperException.Code.NODEEXISTS.equals(ke.code())) &#123; return false; &#125; &#125; &#125;&#125; unlock12345678910111213public void unlock() &#123; //通过state实现重入机制，如果已经获取锁，释放锁时，需要将state--。 delLockCount(); //如果state为0时，说明不再持有锁，需要将连接关闭，自动删除临时节点 if (state == 0 &amp;&amp; zooKeeper != null) &#123; try &#123; zooKeeper.close(); &#125; catch (InterruptedException e) &#123; logger.error("[DistributedLock#unlock] error : " + e.toString(), e); &#125; &#125;&#125; addLockCount123456789101112private boolean addLockCount() &#123; //如果state大于0，即已持有锁，将state数量加一 if (state &gt; 0) &#123; synchronized (this) &#123; if (state &gt; 0) &#123; state++; return true; &#125; &#125; &#125; return false;&#125; delLockCount123456789101112private boolean delLockCount() &#123; //如果state大于0，即还持有锁，将state数量减一 if (state &gt; 0) &#123; synchronized (this) &#123; if (state &gt; 0) &#123; state--; return true; &#125; &#125; &#125; return false;&#125; 总结上面就是一个通过ZooKeeper实现的分布式可重入锁，利用了临时节点的特性。源代码可见：aloofJr其中有几个可以优化的点。 轮询的方式换成Watcher机制 可重入锁实现方式的优化 所有线程竞争一个节点的创建，容易出现羊群效应，且是一种不公平的锁竞争模式 下节我们使用新的方式实现分布式锁来解决上面的几个问题，如果大家好的优化建议，欢迎一起讨论。]]></content>
      <tags>
        <tag>Java</tag>
        <tag>ZooKeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【从入门到放弃-ZooKeeper】ZooKeeper实战-分布式队列]]></title>
    <url>%2Faa721058.html</url>
    <content type="text"><![CDATA[前言上文【从入门到放弃-ZooKeeper】ZooKeeper入门中，我们学习了ZooKeeper的简单安装和cli使用。接下来我们开始基于java API的实战编程。本文先来写一个分布式队列的代码实现。 设计我们来写一个先进先出的分布式无界公平队列。参考我们之前介绍的【从入门到放弃-Java】并发编程-JUC-ConcurrentLinkedQueue和【从入门到放弃-Java】并发编程-JUC-LinkedBlockingQueue。我们直接继承AbstractQueue类，并实现Queue接口。主要重写offer、poll、peek、size方法。我们使用ZooKeeper的持久化顺序节点来实现分布式队列。 offer：入队，入队时新创建一个持久化顺序节点，节点后缀会根据ZooKeeper的特性自动累加。 poll：出队，获取根节点下的所有节点，根据后缀数字排序，数组最小的是最先入队的，因此要最先出队。 peek：获取到最下入队的数据，和poll的区别是，peek只获取数据，不出队，不删除已经消费的节点。 size：获取队列长度，实现方式是，获取根节点下的节点数量即可。这个方法在并发时可能会有问题。慎用。 DistributedQueue123456789101112131415161718192021222324252627282930313233343536373839404142434445//继承AbstractQueue类并实现Queue接口public class DistributedQueue&lt;E&gt; extends AbstractQueue&lt;E&gt; implements Queue&lt;E&gt; &#123; private static Logger logger = LoggerFactory.getLogger(DistributedQueue.class); //ZooKeeper客户端，进行ZooKeeper操作 private ZooKeeper zooKeeper; //根节点名称 private String dir; //数据节点名称，顺序节点在插入口会变为 node&#123;00000000xx&#125; 格式 private String node; //ZooKeeper鉴权信息 private List&lt;ACL&gt; acls; /** * Constructor. * * @param zooKeeper the zoo keeper * @param dir the dir * @param node the node * @param acls the acls */ public DistributedQueue (ZooKeeper zooKeeper, String dir, String node, List&lt;ACL&gt; acls) &#123; this.zooKeeper = zooKeeper; this.dir = dir; this.node = node; this.acls = acls; init(); &#125; private void init() &#123; //需要先判断根节点是否存在，不存在的话，创建子节点时会出错。 try &#123; Stat stat = zooKeeper.exists(dir, false); if (stat == null) &#123; zooKeeper.create(dir, null, acls, CreateMode.PERSISTENT); &#125; &#125; catch (Exception e) &#123; logger.error("[DistributedQueue#init] error : " + e.toString(), e); &#125; &#125;&#125; offer12345678910111213141516171819/** * Offer boolean. * * @param o the o * @return the boolean */@Overridepublic boolean offer(E o) &#123; //构建要插入的节点名称 String fullPath = dir.concat("/").concat(node); try &#123; //创建子节点成功则返回入队成功 zooKeeper.create(fullPath, objectToBytes(o), acls, CreateMode.PERSISTENT_SEQUENTIAL); return true; &#125; catch (Exception e) &#123; logger.error("[DistributedQueue#offer] error : " + e.toString(), e); &#125; return false;&#125; poll12345678910111213141516171819202122232425262728293031323334353637/** * Poll e. * * @return the e */@Overridepublic E poll() &#123; try &#123; //获取根节点所有子节点信息。 List&lt;String&gt; children = zooKeeper.getChildren(dir, null); //如果队列是空的则返回null if (children == null || children.isEmpty()) &#123; return null; &#125; //将子节点名称排序 Collections.sort(children); for (String child : children) &#123; //拼接子节点的具体名称 String fullPath = dir.concat("/").concat(child); try &#123; //如果获取数据成功，则类型转换后，返回，并删除改队列中该节点 byte[] bytes = zooKeeper.getData(fullPath, false, null); E data = (E) bytesToObject(bytes); zooKeeper.delete(fullPath, -1); return data; &#125; catch (Exception e) &#123; logger.warn("[DistributedQueue#poll] warn : " + e.toString(), e); &#125; &#125; &#125; catch (Exception e) &#123; logger.error("[DistributedQueue#peek] poll : " + e.toString(), e); &#125; return null;&#125; peek1234567891011121314151617181920212223242526272829303132333435363738/** * Peek e. * * @return the e */@Overridepublic E peek() &#123; try &#123; //获取根节点所有子节点信息。 List&lt;String&gt; children = zooKeeper.getChildren(dir, null); //如果队列是空的则返回null if (children == null || children.isEmpty()) &#123; return null; &#125; //将子节点名称排序 Collections.sort(children); for (String child : children) &#123; //拼接子节点的具体名称 String fullPath = dir.concat("/").concat(child); try &#123; //如果获取数据成功，则类型转换后，返回，不会删除改队列中该节点 byte[] bytes = zooKeeper.getData(fullPath, false, null); E data = (E) bytesToObject(bytes); return data; &#125; catch (Exception e) &#123; logger.warn("[DistributedQueue#peek] warn : " + e.toString(), e); &#125; &#125; &#125; catch (Exception e) &#123; logger.error("[DistributedQueue#peek] warn : " + e.toString(), e); &#125; return null;&#125; size123456789101112131415161718/** * Size int. * * @return the int */@Overridepublic int size() &#123; try &#123; //获取根节点的子节点名称 List&lt;String&gt; children = zooKeeper.getChildren(dir, null); //返回子结点信息数量 return children.size(); &#125; catch (Exception e) &#123; logger.error("[DistributedQueue#offer] size : " + e.toString(), e); &#125; return 0;&#125; 总结上面我们一起学习了如何利用持久性顺序节点，创建一个分布式先进先出队列。源代码可见：aloofJr。如果有好的优化建议，欢迎一起讨论。]]></content>
      <tags>
        <tag>Java</tag>
        <tag>ZooKeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【从入门到放弃-ZooKeeper】ZooKeeper入门]]></title>
    <url>%2Feda11ef4.html</url>
    <content type="text"><![CDATA[前言ZooKeeper是一个分布式服务协调框架，可以用来维护分布式配置信息、服务注册中心、实现分布式锁等。在Hbase、Hadoop、kafka等项目中都有广泛的应用。随着分布式、微服务的普及，ZooKeeper已经成为我们日常开发工作中无法绕过的一环，本文将从安装配置到最基础的使用入手，对其进行介绍。 安装部署下载在 http://mirror.bit.edu.cn/apache/zookeeper/stable/ 下载推荐的stable版本。可以直接下载编译好的bin文件。目前最新的stable版本是3.5.5 解压将下载的文件解压至工作目录，我的工作目录是/var/workspace/zookeeper1tar -xzvf apache-zookeeper-3.5.5-bin.tar.gz -C /var/workspace/zookeeper 配置 此时如果直接启动zookeeper是会失败的，提示找不到zoo.cfg文件。需要我们把conf目录中提供的zoo_sample.cfg示例配置文件，复制为一份zoo.cfg，zoo.cfg是默认的启动配置文件1cp ../conf/zoo_sample.cfg ../conf/zoo.cfg 启动 默认启动，使用config/zoo.cfg配置文件在后台启动 前台启动，会将启动日志打印在终端。终端关闭后服务也关闭。 访问 使用zcCli.sh -server host:port 访问ZooKeeper服务器。不加-server参数时，默认使用127.0.0.1:2181 使用ZooKeeper使用类似资源文件目录的方式来管理节点，每个节点可以存储数据。ZooKeeper有四种不同类型的节点： PERSISTENT：持久化节点，除非手动删除，否则会永久保存 PERSISTENT_SEQUENTIAL：持久化顺序节点，除非手动删除，否则会永久保存。默认会在用户设置的节点名称后，顺序的增加十位的数字字符串。如 test_0000000001 EPHEMERAL：临时节点，在session结束后，临时节点会被自动删除。 EPHEMERAL_SEQUENTIAL：临时顺序节点，在session结束后，临时节点会被自动删除。默认会在用户设置的节点名称后，顺序的增加十位的数字字符串。如 test_0000000001 help使用zkCli.sh连接到服务端后，可以使用help展示常见命令的使用方式。 ls 列出某个路径下的节点 create 可以使用create {path} 创建节点。使用 create -s {path} 可以创建有序节点，后面添加十位递增的数字后缀。 使用 create -e 创建临时节点，当断开连接后，临时节点会被删除。 set/get ZooKeeper的节点可以存储数据，使用set 方法将数据存在节点中。使用get 方法，从节点中获取数据。 set -v {currentVersion} {path} {data} 节点的数据每次更新时，都会递增，如果要更新节点，请使用上面的命令修改，如果currentVersion不是最新版本时，则更新失败，类似乐观锁CAS。 delete 可以通过delete删除节点。 总结通过上面的学习，我们已经学会了ZooKeeper最基本的安装部署及使用方式。需要牢记： ZooKeeper的节点是类似文件系统的管理方式 Zookeeper有四种节点类型，临时节点会在会话断开后自动删除，顺序节点后缀序号会自动递增 set数据时，可以指定版本号，版本号与当前版本一致时才更新。这种乐观锁的更新方式，可以避免并发时数据被覆盖]]></content>
      <tags>
        <tag>Java ZooKeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【从入门到放弃-Java】并发编程-JUC-locks-ReentrantReadWriteLock]]></title>
    <url>%2Fd624101f.html</url>
    <content type="text"><![CDATA[前言上文【从入门到放弃-Java】并发编程-JUC-locks-ReentrantLock我们了解到，ReentrantLock是一个互斥排他的重入锁，读和读、读和写、写和写不能同时进行。但在很多场景下，读多写少，我们希望能并发读，这时候ReentrantReadWriteLock就派上用场了，是专门针对这种场景设计的。接下来我们一起来学习下ReentrantReadWriteLock。 ReentrantReadWriteLock 12345678910111213141516171819/** * Creates a new &#123;@code ReentrantReadWriteLock&#125; with * default (nonfair) ordering properties. */ public ReentrantReadWriteLock() &#123; this(false); &#125; /** * Creates a new &#123;@code ReentrantReadWriteLock&#125; with * the given fairness policy. * * @param fair &#123;@code true&#125; if this lock should use a fair ordering policy */ public ReentrantReadWriteLock(boolean fair) &#123; sync = fair ? new FairSync() : new NonfairSync(); readerLock = new ReadLock(this); writerLock = new WriteLock(this); &#125; 我们可以看到和ReentrantLock一样，ReentrantReadWriteLock也使用了通过AQS实现的FairSync和NonfairSync模式有两个成员变量锁ReadLock和WriteLock ReadLock::lock获取读锁，不死不休123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156public void lock() &#123; sync.acquireShared(1);&#125;public final void acquireShared(int arg) &#123; if (tryAcquireShared(arg) &lt; 0) doAcquireShared(arg);&#125;@ReservedStackAccessprotected final int tryAcquireShared(int unused) &#123; /* * Walkthrough: * 1. If write lock held by another thread, fail. * 2. Otherwise, this thread is eligible for * lock wrt state, so ask if it should block * because of queue policy. If not, try * to grant by CASing state and updating count. * Note that step does not check for reentrant * acquires, which is postponed to full version * to avoid having to check hold count in * the more typical non-reentrant case. * 3. If step 2 fails either because thread * apparently not eligible or CAS fails or count * saturated, chain to version with full retry loop. */ Thread current = Thread.currentThread(); int c = getState(); //如果已经有写锁，且不是当前线程持有的，则加读锁失败 //如果当前线程已经持有写锁，则可以获取读锁，这就是锁降级 if (exclusiveCount(c) != 0 &amp;&amp; getExclusiveOwnerThread() != current) return -1; int r = sharedCount(c); /** * 判断读线程是否阻塞，取决于队列的策略 * 公平锁策略：如果当前同步队列不为空且当前线程不是队列的第一个节点，则阻塞。 * 非公平锁策略：如果当前队列的第一个节点时写锁，则需要阻塞。这样是为了防止写锁饥饿。 * 如果不需要阻塞，且读锁数未达到最大值 则尝试通过cas的方式获取锁 */ if (!readerShouldBlock() &amp;&amp; r &lt; MAX_COUNT &amp;&amp; compareAndSetState(c, c + SHARED_UNIT)) &#123; //如果当前读锁为0，则当前线程获取锁 if (r == 0) &#123; firstReader = current; firstReaderHoldCount = 1; //如过第一个读锁的持有者是当前线程，则firstReaderHoldCount数量加一 &#125; else if (firstReader == current) &#123; firstReaderHoldCount++; &#125; else &#123; HoldCounter rh = cachedHoldCounter; //如果最后一个获取锁的线程不是当前线程 if (rh == null || rh.tid != LockSupport.getThreadId(current)) //获取当前线程的锁 cachedHoldCounter = rh = readHolds.get(); //如果当前最后一个线程获取锁数量为0，则将其设置为当前线程的holdcounter else if (rh.count == 0) readHolds.set(rh); //读锁数+1 rh.count++; &#125; return 1; &#125; //尝试无限循环获取读锁 return fullTryAcquireShared(current);&#125;final int fullTryAcquireShared(Thread current) &#123; /* * This code is in part redundant with that in * tryAcquireShared but is simpler overall by not * complicating tryAcquireShared with interactions between * retries and lazily reading hold counts. */ HoldCounter rh = null; for (;;) &#123; int c = getState(); //如果已经有写锁，且不是当前线程持有的，返回-1 if (exclusiveCount(c) != 0) &#123; if (getExclusiveOwnerThread() != current) return -1; // else we hold the exclusive lock; blocking here // would cause deadlock. //如果需要阻塞 &#125; else if (readerShouldBlock()) &#123; // Make sure we're not acquiring read lock reentrantly if (firstReader == current) &#123; // assert firstReaderHoldCount &gt; 0; &#125; else &#123; if (rh == null) &#123; rh = cachedHoldCounter; if (rh == null || rh.tid != LockSupport.getThreadId(current)) &#123; //如果当前线程持有的锁数为0，则移除 rh = readHolds.get(); if (rh.count == 0) readHolds.remove(); &#125; &#125; if (rh.count == 0) return -1; &#125; &#125; if (sharedCount(c) == MAX_COUNT) throw new Error("Maximum lock count exceeded"); if (compareAndSetState(c, c + SHARED_UNIT)) &#123; if (sharedCount(c) == 0) &#123; firstReader = current; firstReaderHoldCount = 1; &#125; else if (firstReader == current) &#123; firstReaderHoldCount++; &#125; else &#123; if (rh == null) rh = cachedHoldCounter; if (rh == null || rh.tid != LockSupport.getThreadId(current)) rh = readHolds.get(); else if (rh.count == 0) readHolds.set(rh); rh.count++; cachedHoldCounter = rh; // cache for release &#125; return 1; &#125; &#125;&#125;private void doAcquireShared(int arg) &#123; final Node node = addWaiter(Node.SHARED); boolean interrupted = false; try &#123; for (;;) &#123; final Node p = node.predecessor(); //无限循环，直到当前线程是队列的头结点，则尝试获取读锁 if (p == head) &#123; int r = tryAcquireShared(arg); if (r &gt;= 0) &#123; //获取锁成功后，将当前线程从队列头结点移除 setHeadAndPropagate(node, r); p.next = null; // help GC return; &#125; &#125; if (shouldParkAfterFailedAcquire(p, node)) interrupted |= parkAndCheckInterrupt(); &#125; &#125; catch (Throwable t) &#123; cancelAcquire(node); throw t; &#125; finally &#123; if (interrupted) selfInterrupt(); &#125;&#125; ReadLock::lockInterruptibly获取读锁，直到成功或被中断123456789101112131415161718192021222324252627282930313233343536373839public void lockInterruptibly() throws InterruptedException &#123; sync.acquireSharedInterruptibly(1);&#125;public final void acquireSharedInterruptibly(int arg) throws InterruptedException &#123; //如果收到中断信号，则抛出中断异常 if (Thread.interrupted()) throw new InterruptedException(); //如果尝试获取锁失败，则循环等待获取锁 if (tryAcquireShared(arg) &lt; 0) doAcquireSharedInterruptibly(arg);&#125;private void doAcquireSharedInterruptibly(int arg) throws InterruptedException &#123; final Node node = addWaiter(Node.SHARED); try &#123; for (;;) &#123; //无限循环，直到当前线程是队列的头结点，则尝试获取读锁 final Node p = node.predecessor(); if (p == head) &#123; int r = tryAcquireShared(arg); if (r &gt;= 0) &#123; setHeadAndPropagate(node, r); p.next = null; // help GC return; &#125; &#125; //获取锁失败的话则需要进行中断检测，检测到中断信号则抛出异常 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) throw new InterruptedException(); &#125; &#125; catch (Throwable t) &#123; cancelAcquire(node); throw t; &#125;&#125; ReadLock::tryLock1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980//尝试获取读锁，如果有写锁获取失败，则直接返回失败public boolean tryLock() &#123; return sync.tryReadLock();&#125;@ReservedStackAccessfinal boolean tryReadLock() &#123; Thread current = Thread.currentThread(); for (;;) &#123; int c = getState(); if (exclusiveCount(c) != 0 &amp;&amp; getExclusiveOwnerThread() != current) return false; int r = sharedCount(c); if (r == MAX_COUNT) throw new Error("Maximum lock count exceeded"); if (compareAndSetState(c, c + SHARED_UNIT)) &#123; if (r == 0) &#123; firstReader = current; firstReaderHoldCount = 1; &#125; else if (firstReader == current) &#123; firstReaderHoldCount++; &#125; else &#123; HoldCounter rh = cachedHoldCounter; if (rh == null || rh.tid != LockSupport.getThreadId(current)) cachedHoldCounter = rh = readHolds.get(); else if (rh.count == 0) readHolds.set(rh); rh.count++; &#125; return true; &#125; &#125;&#125;//尝试获取读锁，获取失败或者超时未获取到的话，则返回失败public boolean tryLock(long timeout, TimeUnit unit) throws InterruptedException &#123; return sync.tryAcquireSharedNanos(1, unit.toNanos(timeout));&#125;private boolean doAcquireSharedNanos(int arg, long nanosTimeout) throws InterruptedException &#123; if (nanosTimeout &lt;= 0L) return false; final long deadline = System.nanoTime() + nanosTimeout; final Node node = addWaiter(Node.SHARED); try &#123; for (;;) &#123; //排到当前线程的话则尝试获取锁 final Node p = node.predecessor(); if (p == head) &#123; int r = tryAcquireShared(arg); if (r &gt;= 0) &#123; setHeadAndPropagate(node, r); p.next = null; // help GC return true; &#125; &#125; //超时返回false nanosTimeout = deadline - System.nanoTime(); if (nanosTimeout &lt;= 0L) &#123; cancelAcquire(node); return false; &#125; //阻塞当前线程 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; nanosTimeout &gt; SPIN_FOR_TIMEOUT_THRESHOLD) LockSupport.parkNanos(this, nanosTimeout); //如果被中断 if (Thread.interrupted()) throw new InterruptedException(); &#125; &#125; catch (Throwable t) &#123; cancelAcquire(node); throw t; &#125;&#125; ReadLock::unlock释放锁123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384public void unlock() &#123; sync.releaseShared(1);&#125;public final boolean releaseShared(int arg) &#123; if (tryReleaseShared(arg)) &#123; doReleaseShared(); return true; &#125; return false;&#125;protected final boolean tryReleaseShared(int unused) &#123; Thread current = Thread.currentThread(); //如果当前线程是第一个持有读锁的 if (firstReader == current) &#123; // assert firstReaderHoldCount &gt; 0; //如果是唯一一个持有读锁的，则firstReader设置为null if (firstReaderHoldCount == 1) firstReader = null; //firstReaderHoldCount减一， else firstReaderHoldCount--; &#125; else &#123; HoldCounter rh = cachedHoldCounter; //如果不是最后一个持有读锁的线程 if (rh == null || rh.tid != LockSupport.getThreadId(current)) //从ThreadLocal获取readHolds rh = readHolds.get(); int count = rh.count; //如果小于等于1，则移除readHolds if (count &lt;= 1) &#123; readHolds.remove(); if (count &lt;= 0) throw unmatchedUnlockException(); &#125; //持有锁的数量减一 --rh.count; &#125; for (;;) &#123; //将state设置为0，原因是在写锁降级为读锁后，释放读锁时，需要将state设为0，方便后续的写锁竞争。 int c = getState(); int nextc = c - SHARED_UNIT; if (compareAndSetState(c, nextc)) // Releasing the read lock has no effect on readers, // but it may allow waiting writers to proceed if // both read and write locks are now free. return nextc == 0; &#125;&#125;private void doReleaseShared() &#123; /* * Ensure that a release propagates, even if there are other * in-progress acquires/releases. This proceeds in the usual * way of trying to unparkSuccessor of head if it needs * signal. But if it does not, status is set to PROPAGATE to * ensure that upon release, propagation continues. * Additionally, we must loop in case a new node is added * while we are doing this. Also, unlike other uses of * unparkSuccessor, we need to know if CAS to reset status * fails, if so rechecking. */ for (;;) &#123; Node h = head; //如果头结点不是null，并且队列不为空 if (h != null &amp;&amp; h != tail) &#123; int ws = h.waitStatus; //如果当前结点是SIGNAL信号 if (ws == Node.SIGNAL) &#123; //唤醒头结点 if (!h.compareAndSetWaitStatus(Node.SIGNAL, 0)) continue; // loop to recheck cases unparkSuccessor(h); &#125; else if (ws == 0 &amp;&amp; !h.compareAndSetWaitStatus(0, Node.PROPAGATE)) continue; // loop on failed CAS &#125; if (h == head) // loop if head changed break; &#125;&#125; WriteLock::lock获取写锁，如果获取失败，则加入等待队列具体方法和ReentrantLock调用的方法相同，可参考【从入门到放弃-Java】并发编程-JUC-locks-ReentrantLock123public void lock() &#123; sync.acquire(1);&#125; WriteLock::lockInterruptibly获取写锁，如果获取失败，则加入等待队列，直到获取到或被中断具体方法和ReentrantLock调用的方法相同，可参考【从入门到放弃-Java】并发编程-JUC-locks-ReentrantLock WriteLock::tryLock1234567891011121314151617181920212223242526272829public boolean tryLock() &#123; return sync.tryWriteLock();&#125;@ReservedStackAccessfinal boolean tryWriteLock() &#123; Thread current = Thread.currentThread(); int c = getState(); //如果存在写锁，且写锁不是当前线程持有的，则返回false if (c != 0) &#123; int w = exclusiveCount(c); if (w == 0 || current != getExclusiveOwnerThread()) return false; if (w == MAX_COUNT) throw new Error("Maximum lock count exceeded"); &#125; //如果不存在写锁或是当前线程获取的写锁，则尝试将state加一 if (!compareAndSetState(c, c + 1)) return false; //设置持有写锁的线程为当前线程 setExclusiveOwnerThread(current); return true;&#125;public boolean tryLock(long timeout, TimeUnit unit) throws InterruptedException &#123; //和ReentrantLock的调用方法一样，不再赘述 return sync.tryAcquireNanos(1, unit.toNanos(timeout));&#125; WriteLock::unlock1234567891011121314151617181920212223242526272829public void unlock() &#123; sync.release(1);&#125;public final boolean release(int arg) &#123; if (tryRelease(arg)) &#123; Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; &#125; return false;&#125;@ReservedStackAccessprotected final boolean tryRelease(int releases) &#123; //如果不是当前线程持有的写锁，抛出异常 if (!isHeldExclusively()) throw new IllegalMonitorStateException(); int nextc = getState() - releases; //判断持有的写锁是否释放完毕 boolean free = exclusiveCount(nextc) == 0; //如果释放完毕，则将当前持有锁的线程设置为null if (free) setExclusiveOwnerThread(null); //设置持有的锁数量减一 setState(nextc); return free;&#125; 总结通过源码分析，我们了解到，可以通过ReentrantReadWriteLock可以获取读锁和写锁。 写锁是互斥锁，只能一个线程持有，写锁和ReentrantLock类似 读锁是共享锁，可以多个线程同时持有。 读锁通过firstReader和cachedHoldCounter优化获取、释放锁的性能。使用ThreadLocal readHolds存放所有持有锁线程的tid和持有锁数量。 线程可以将自己持有的写锁降级为读锁，在释放读锁时，一起释放。]]></content>
      <tags>
        <tag>Java</tag>
        <tag>线程安全</tag>
        <tag>并发编程</tag>
        <tag>锁机制</tag>
        <tag>同步</tag>
        <tag>synchronized</tag>
        <tag>锁</tag>
        <tag>ReentrantReadWriteLock</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【从入门到放弃-Java】并发编程-JUC-locks-ReentrantLock]]></title>
    <url>%2F1b8f3dad.html</url>
    <content type="text"><![CDATA[前言ReentrantLock是非常常用的锁，在前面【从入门到放弃-Java】并发编程-JUC-LinkedBlockingQueue在我们了解到，LinkedBlockingQueue入队、出队都是依赖ReentrantLock进行锁同步和线程唤醒、等待的。本文来学习下ReentrantLock。 ReentrantLock1234567891011121314151617/** * Creates an instance of &#123;@code ReentrantLock&#125;. * This is equivalent to using &#123;@code ReentrantLock(false)&#125;. */public ReentrantLock() &#123; sync = new NonfairSync();&#125;/** * Creates an instance of &#123;@code ReentrantLock&#125; with the * given fairness policy. * * @param fair &#123;@code true&#125; if this lock should use a fair ordering policy */public ReentrantLock(boolean fair) &#123; sync = fair ? new FairSync() : new NonfairSync();&#125; 通过构造函数，我们可以看到可以根据参数fair，生成公平的同步和不公平的同步模式。接下来需要看下FairSync和NonfairSync到底是何方神圣 Sync Sync是一个抽象类，FairSync和NonfairSync都继承自Sync并实现了tryAcquire方法，tryAcquire是在AbstractQueuedSynchronizer（AQS）中声明的。AbstractQueuedSynchronizer中的方法非常多，我们通过ReentrantLock中各方法的调用来逐步熟悉它。 ReentrantLock::lock123public void lock() &#123; sync.acquire(1);&#125; 请求锁，如果加锁失败则一直等待。ReentrantLock中加锁的方法非常简洁，直接调用sync的acquire方法下面我们看下acquire的具体实现。 AbstractQueuedSynchronizer::acquire123456789101112131415161718192021222324252627282930313233343536373839/** * Acquires in exclusive mode, ignoring interrupts. Implemented * by invoking at least once &#123;@link #tryAcquire&#125;, * returning on success. Otherwise the thread is queued, possibly * repeatedly blocking and unblocking, invoking &#123;@link * #tryAcquire&#125; until success. This method can be used * to implement method &#123;@link Lock#lock&#125;. * * @param arg the acquire argument. This value is conveyed to * &#123;@link #tryAcquire&#125; but is otherwise uninterpreted and * can represent anything you like. */public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();&#125;final boolean acquireQueued(final Node node, int arg) &#123; boolean interrupted = false; try &#123; for (;;) &#123; //会一直等待，直到获取到锁为止 final Node p = node.predecessor(); if (p == head &amp;&amp; tryAcquire(arg)) &#123; setHead(node); p.next = null; // help GC return interrupted; &#125; if (shouldParkAfterFailedAcquire(p, node)) interrupted |= parkAndCheckInterrupt(); &#125; &#125; catch (Throwable t) &#123; cancelAcquire(node); if (interrupted) selfInterrupt(); throw t; &#125;&#125; 首先尝试获取锁（具体实现下面分析），获取成功函数结束 获取失败，则加入等待队列一直自旋尝试获取锁直到获取成功或超时。 如果获取失败，则抛出中断异常 NonfairSync::tryAcquire1234567891011121314151617181920212223242526272829303132/** * Sync object for non-fair locks */protected final boolean tryAcquire(int acquires) &#123; return nonfairTryAcquire(acquires);&#125;/** * Performs non-fair tryLock. tryAcquire is implemented in * subclasses, but both need nonfair try for trylock method. */@ReservedStackAccessfinal boolean nonfairTryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); //如果state = 0 即当前没加锁，则尝试通过CAS的方式加锁，加锁后将持有锁的线程设置为当前线程 if (c == 0) &#123; if (compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; //如果当前有锁，则判断是否是当前线程的锁，是的话state加一，即重入锁，不是的话 返回加锁失败 else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error("Maximum lock count exceeded"); setState(nextc); return true; &#125; return false;&#125; 如果没锁，则尝试获取锁 如果有锁，判断是否是当前线程持有的 是当前线程持有，则state值加1 返回加锁成功。即重入锁 不是当前线程持有，则加锁失败 FairSync::tryAcquire1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889/** * Fair version of tryAcquire. Don't grant access unless * recursive call or no waiters or is first. */@ReservedStackAccessprotected final boolean tryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); //如果没锁 if (c == 0) &#123; //如果队列中没有比当前线程等待更久的线程，则尝试通过CAS的方式获取锁 if (!hasQueuedPredecessors() &amp;&amp; compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; //如果当前有锁，则判断是否是当前线程的锁，是的话state加一，即重入锁，不是的话 返回加锁失败 else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) throw new Error("Maximum lock count exceeded"); setState(nextc); return true; &#125; return false;&#125;/** * Queries whether any threads have been waiting to acquire longer * than the current thread. * * &lt;p&gt;An invocation of this method is equivalent to (but may be * more efficient than): * &lt;pre&gt; &#123;@code * getFirstQueuedThread() != Thread.currentThread() * &amp;&amp; hasQueuedThreads()&#125;&lt;/pre&gt; * * &lt;p&gt;Note that because cancellations due to interrupts and * timeouts may occur at any time, a &#123;@code true&#125; return does not * guarantee that some other thread will acquire before the current * thread. Likewise, it is possible for another thread to win a * race to enqueue after this method has returned &#123;@code false&#125;, * due to the queue being empty. * * &lt;p&gt;This method is designed to be used by a fair synchronizer to * avoid &lt;a href="AbstractQueuedSynchronizer.html#barging"&gt;barging&lt;/a&gt;. * Such a synchronizer's &#123;@link #tryAcquire&#125; method should return * &#123;@code false&#125;, and its &#123;@link #tryAcquireShared&#125; method should * return a negative value, if this method returns &#123;@code true&#125; * (unless this is a reentrant acquire). For example, the &#123;@code * tryAcquire&#125; method for a fair, reentrant, exclusive mode * synchronizer might look like this: * * &lt;pre&gt; &#123;@code * protected boolean tryAcquire(int arg) &#123; * if (isHeldExclusively()) &#123; * // A reentrant acquire; increment hold count * return true; * &#125; else if (hasQueuedPredecessors()) &#123; * return false; * &#125; else &#123; * // try to acquire normally * &#125; * &#125;&#125;&lt;/pre&gt; * * @return &#123;@code true&#125; if there is a queued thread preceding the * current thread, and &#123;@code false&#125; if the current thread * is at the head of the queue or the queue is empty * @since 1.7 */public final boolean hasQueuedPredecessors() &#123; Node h, s; //如果队列不为空 if ((h = head) != null) &#123; //看当前线程是不是在队列的队首，即排队时间最长的队列 if ((s = h.next) == null || s.waitStatus &gt; 0) &#123; s = null; // traverse in case of concurrent cancellation //这里为啥从队列尾部开始向前遍历？？？可能是因为队列头部可能会有大量超时的节点，从后往前遍历更快？ for (Node p = tail; p != h &amp;&amp; p != null; p = p.prev) &#123; if (p.waitStatus &lt;= 0) s = p; &#125; &#125; if (s != null &amp;&amp; s.thread != Thread.currentThread()) return true; &#125; return false;&#125; ReentrantLock::lockInterruptibly123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public void lockInterruptibly() throws InterruptedException &#123; sync.acquireInterruptibly(1);&#125;/** * Acquires in exclusive mode, aborting if interrupted. * Implemented by first checking interrupt status, then invoking * at least once &#123;@link #tryAcquire&#125;, returning on * success. Otherwise the thread is queued, possibly repeatedly * blocking and unblocking, invoking &#123;@link #tryAcquire&#125; * until success or the thread is interrupted. This method can be * used to implement method &#123;@link Lock#lockInterruptibly&#125;. * * @param arg the acquire argument. This value is conveyed to * &#123;@link #tryAcquire&#125; but is otherwise uninterpreted and * can represent anything you like. * @throws InterruptedException if the current thread is interrupted */public final void acquireInterruptibly(int arg) throws InterruptedException &#123; //如果线程被重点，抛出异常 if (Thread.interrupted()) throw new InterruptedException(); //如果获取锁失败 if (!tryAcquire(arg)) doAcquireInterruptibly(arg);&#125;/** * Acquires in exclusive interruptible mode. * @param arg the acquire argument */private void doAcquireInterruptibly(int arg) throws InterruptedException &#123; //new一个新节点 final Node node = addWaiter(Node.EXCLUSIVE); try &#123; //轮询直到获取到锁或者线程被中断 for (;;) &#123; final Node p = node.predecessor(); if (p == head &amp;&amp; tryAcquire(arg)) &#123; setHead(node); p.next = null; // help GC return; &#125; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) throw new InterruptedException(); &#125; &#125; catch (Throwable t) &#123; cancelAcquire(node); throw t; &#125;&#125; 请求锁，如果失败则一直阻塞等待 直到获取锁或线程中断 ReentrantLock::tryLock123456789101112131415161718public boolean tryLock() &#123; //尝试获取锁，获取失败的话 直接返回false，不会再等待 return sync.nonfairTryAcquire(1);&#125;public boolean tryLock(long timeout, TimeUnit unit) throws InterruptedException &#123; //尝试获取锁，如果失败的话，等待timeout时间后返回false，如果被中断则抛出异常 return sync.tryAcquireNanos(1, unit.toNanos(timeout));&#125;public final boolean tryAcquireNanos(int arg, long nanosTimeout) throws InterruptedException &#123; if (Thread.interrupted()) throw new InterruptedException(); return tryAcquire(arg) || doAcquireNanos(arg, nanosTimeout);&#125; 请求锁，如果请求失败，则返回false ReentrantLock::unlock123456789101112131415161718192021222324252627282930public void unlock() &#123; sync.release(1);&#125;public final boolean release(int arg) &#123; if (tryRelease(arg)) &#123; Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; &#125; return false;&#125;@ReservedStackAccessprotected final boolean tryRelease(int releases) &#123; //释放锁 state数减releases int c = getState() - releases; //如果当前线程没有持有锁，则抛出异常 if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; //当c=0时，锁完全释放，ownerThread设为null。 if (c == 0) &#123; free = true; setExclusiveOwnerThread(null); &#125; setState(c); return free;&#125; 释放锁，直到state=0完全释放时，线程owner设置为null ReentrantLock::newCondition1234567public Condition newCondition() &#123; return sync.newCondition();&#125;final ConditionObject newCondition() &#123; return new ConditionObject();&#125; ConditionObject::await线程释放锁，阻塞挂起，直到被signal唤醒，则继续尝试获取锁123456789101112131415161718192021222324public final void await() throws InterruptedException &#123; //如果当前线程被中断、则抛出中断异常 if (Thread.interrupted()) throw new InterruptedException(); //新创建个节点，将当前线程加入等待队列 Node node = addConditionWaiter(); //完全释放锁 int savedState = fullyRelease(node); int interruptMode = 0; //阻塞直到node被唤醒 while (!isOnSyncQueue(node)) &#123; LockSupport.park(this); //如果被中断，则直接break if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break; &#125; //尝试获取锁 if (acquireQueued(node, savedState) &amp;&amp; interruptMode != THROW_IE) interruptMode = REINTERRUPT; if (node.nextWaiter != null) // clean up if cancelled unlinkCancelledWaiters(); if (interruptMode != 0) reportInterruptAfterWait(interruptMode);&#125; ConditionObject::awaitNanos线程释放锁，阻塞挂起一段时间，直到被signal唤醒或超时，则继续尝试获取锁1234567891011121314151617181920212223242526272829303132333435public final long awaitNanos(long nanosTimeout) throws InterruptedException &#123; if (Thread.interrupted()) throw new InterruptedException(); // We don't check for nanosTimeout &lt;= 0L here, to allow // awaitNanos(0) as a way to "yield the lock". final long deadline = System.nanoTime() + nanosTimeout; long initialNanos = nanosTimeout; //加入等待队列 Node node = addConditionWaiter(); //释放所有锁 int savedState = fullyRelease(node); int interruptMode = 0; while (!isOnSyncQueue(node)) &#123; //等待超时 if (nanosTimeout &lt;= 0L) &#123; transferAfterCancelledWait(node); break; &#125; if (nanosTimeout &gt; SPIN_FOR_TIMEOUT_THRESHOLD) LockSupport.parkNanos(this, nanosTimeout); if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break; nanosTimeout = deadline - System.nanoTime(); &#125; //尝试获取锁 if (acquireQueued(node, savedState) &amp;&amp; interruptMode != THROW_IE) interruptMode = REINTERRUPT; if (node.nextWaiter != null) unlinkCancelledWaiters(); if (interruptMode != 0) reportInterruptAfterWait(interruptMode); long remaining = deadline - System.nanoTime(); // avoid overflow return (remaining &lt;= initialNanos) ? remaining : Long.MIN_VALUE;&#125; ConditionObject::awaitUntil线程释放锁，阻塞挂起一段时间，直到被signal唤醒或到指定时间，则继续尝试获取锁1234567891011121314151617181920212223242526public final boolean awaitUntil(Date deadline) throws InterruptedException &#123; long abstime = deadline.getTime(); if (Thread.interrupted()) throw new InterruptedException(); Node node = addConditionWaiter(); int savedState = fullyRelease(node); boolean timedout = false; int interruptMode = 0; while (!isOnSyncQueue(node)) &#123; if (System.currentTimeMillis() &gt;= abstime) &#123; timedout = transferAfterCancelledWait(node); break; &#125; LockSupport.parkUntil(this, abstime); if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break; &#125; if (acquireQueued(node, savedState) &amp;&amp; interruptMode != THROW_IE) interruptMode = REINTERRUPT; if (node.nextWaiter != null) unlinkCancelledWaiters(); if (interruptMode != 0) reportInterruptAfterWait(interruptMode); return !timedout;&#125; ConditionObject::signal把首节点的status设置为Node.SIGNAL 则阻塞的线程循环判断发现statue状态变了，则唤醒继续执行。如果设置status失败，则在此线程中调用LockSupport.unpark唤醒阻塞的线程 12345678910111213141516171819202122232425262728293031323334353637public final void signal() &#123; if (!isHeldExclusively()) throw new IllegalMonitorStateException(); Node first = firstWaiter; if (first != null) doSignal(first);&#125;private void doSignal(Node first) &#123; do &#123; if ( (firstWaiter = first.nextWaiter) == null) lastWaiter = null; first.nextWaiter = null; &#125; while (!transferForSignal(first) &amp;&amp; (first = firstWaiter) != null);&#125;final boolean transferForSignal(Node node) &#123; /* * If cannot change waitStatus, the node has been cancelled. */ if (!node.compareAndSetWaitStatus(Node.CONDITION, 0)) return false; /* * Splice onto queue and try to set waitStatus of predecessor to * indicate that thread is (probably) waiting. If cancelled or * attempt to set waitStatus fails, wake up to resync (in which * case the waitStatus can be transiently and harmlessly wrong). */ Node p = enq(node); int ws = p.waitStatus; //唤醒一个节点，把statue设置为Node.SIGNAL。如果设置失败了，则自己调用LockSupport.unpark唤醒线程 if (ws &gt; 0 || !p.compareAndSetWaitStatus(ws, Node.SIGNAL)) LockSupport.unpark(node.thread); return true;&#125; ConditionObject::signalAll唤醒所有的节点。12345678910111213141516public final void signalAll() &#123; if (!isHeldExclusively()) throw new IllegalMonitorStateException(); Node first = firstWaiter; if (first != null) doSignalAll(first);&#125;private void doSignalAll(Node first) &#123; lastWaiter = firstWaiter = null; do &#123; Node next = first.nextWaiter; first.nextWaiter = null; transferForSignal(first); first = next; &#125; while (first != null);&#125; 总结在【从入门到放弃-Java】并发编程-锁-synchronized中，我们学习内置锁synchronized，与ReentrantLock对比 两者都是可重入的互斥锁。 synchronized是隐式的加解锁，不需要手动解锁。而ReentrantLock需要显式的lock和unlock。lock加锁多少次，对应的就需要unlock多少次。因此一般都会在finally中unlock。避免因异常等情况导致锁无法释放 ReentrantLock通过AQS（volatile state + CAS + CLH队列实现）加解锁。synchronized是通过monitor实现（存在偏向锁、轻量级锁、重量级锁等锁升级）。 ReentrantLock可以使用lockInterruptibly响应中断，synchronized只能傻等、等到死 ReentrantLock可以使用非公平锁和公平锁模式，可以通过非公平性减少CAS的竞争，提升性能。也可以通过公平锁减少线程饥饿情况发生 ReentrantLock可以创造多个Condition，来实现线程等待通知机制（阻塞、唤醒）]]></content>
      <tags>
        <tag>Java</tag>
        <tag>线程安全</tag>
        <tag>并发编程</tag>
        <tag>锁机制</tag>
        <tag>同步</tag>
        <tag>synchronized</tag>
        <tag>锁</tag>
        <tag>ReentrantLock</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【从入门到放弃-Java】并发编程-JUC-SynchronousQueue]]></title>
    <url>%2F26db81a2.html</url>
    <content type="text"><![CDATA[前言上文【从入门到放弃-Java】并发编程-JUC-LinkedBlockingQueue，我们介绍了基于链表的有界阻塞队列LinkedBlockingQueue，它是Executors.newFixedThreadPool中workQueue使用的阻塞队列。本文我们来学习ExecutorService.newCachedThreadPool中使用的阻塞队列：SynchronousQueue。 SynchronousQueue 如图和LinkedBlockingQueue一样，都是继承了AbstractQueue类，实现了BlockingQueue和Serializable接口 SynchronousQueue1234567891011121314151617/** * Creates a &#123;@code SynchronousQueue&#125; with nonfair access policy. */public SynchronousQueue() &#123; this(false);&#125;/** * Creates a &#123;@code SynchronousQueue&#125; with the specified fairness policy. * * @param fair if true, waiting threads contend in FIFO order for * access; otherwise the order is unspecified. */public SynchronousQueue(boolean fair) &#123; //公平模式下使用队列，实现先进先出，非公平模式下使用栈，先进后出 transferer = fair ? new TransferQueue&lt;E&gt;() : new TransferStack&lt;E&gt;();&#125; 因为SynchronousQueue的put、offer、take、poll方法全是调用了Transferer的transfer方法，我们一起来看下这个transfer到底是何方神圣。 Transferer12345678910111213141516171819/** * Shared internal API for dual stacks and queues. */abstract static class Transferer&lt;E&gt; &#123; /** * Performs a put or take. * * @param e if non-null, the item to be handed to a consumer; * if null, requests that transfer return an item * offered by producer. * @param timed if this operation should timeout * @param nanos the timeout, in nanoseconds * @return if non-null, the item provided or received; if null, * the operation failed due to timeout or interrupt -- * the caller can distinguish which of these occurred * by checking Thread.interrupted. */ abstract E transfer(E e, boolean timed, long nanos);&#125; Transferer是一个抽象类，只有一个抽象方法transfer。可以从注释中看到： e是元素根据e是否为null来控制是生产者还是消费者。 timed是布尔值，控制是否使用超时机制。 nanos是超时时间。 transfer的具体实现有两个，在Transferer的两个实现类：TransferQueue和TransferStack中 TransferQueue::transfer123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102E transfer(E e, boolean timed, long nanos) &#123; /* Basic algorithm is to loop trying to take either of * two actions: * * 1. If queue apparently empty or holding same-mode nodes, * try to add node to queue of waiters, wait to be * fulfilled (or cancelled) and return matching item. * * 2. If queue apparently contains waiting items, and this * call is of complementary mode, try to fulfill by CAS'ing * item field of waiting node and dequeuing it, and then * returning matching item. * * In each case, along the way, check for and try to help * advance head and tail on behalf of other stalled/slow * threads. * * The loop starts off with a null check guarding against * seeing uninitialized head or tail values. This never * happens in current SynchronousQueue, but could if * callers held non-volatile/final ref to the * transferer. The check is here anyway because it places * null checks at top of loop, which is usually faster * than having them implicitly interspersed. */ QNode s = null; // constructed/reused as needed //判断是消费者还是生产者，如果e为null则消费者，e不为null是生产者 boolean isData = (e != null); for (;;) &#123; QNode t = tail; QNode h = head; //tail和head是队列的尾部和头部，是一个item为空的QNode，如果队列被其它线程改动了，则continue重新处理 if (t == null || h == null) // saw uninitialized value continue; // spin //如果队列为空，或者处于same-mode模式 if (h == t || t.isData == isData) &#123; // empty or same-mode //如果不是最后一个节点，则继续寻找最后一个有数据的节点 QNode tn = t.next; if (t != tail) // inconsistent read continue; //如果已经tn不为null，则尝试通过CAS把tn置为尾结点，然后重新执行 if (tn != null) &#123; // lagging tail advanceTail(t, tn); continue; &#125; //如果超时，直接返回null if (timed &amp;&amp; nanos &lt;= 0L) // can't wait return null; //如果新节点还未创建，则创建一个新的QNode来承载元素e if (s == null) s = new QNode(e, isData); //尝试通过CAS将t的下一个节点设置为s，如果设置失败，则说明t的下一个节点已经被添加了元素，则需要从头开始处理 if (!t.casNext(null, s)) // failed to link in continue; //尝试将tail设置为新建的节点s advanceTail(t, s); // swing tail and wait //加入队列后阻塞，把等待线程设置为当前线程，等待唤醒处理 Object x = awaitFulfill(s, e, timed, nanos); //如果超时中断则删除这个节点并返回null if (x == s) &#123; // wait was cancelled clean(t, s); return null; &#125; //如果不是tail节点，则判断是否是head节点， if (!s.isOffList()) &#123; // not already unlinked advanceHead(t, s); // unlink if head //如果返回的x不为null，则设置item为自身 if (x != null) // and forget fields s.item = s; //把等待线程设置为null s.waiter = null; &#125; return (x != null) ? (E)x : e; &#125; else &#123; // complementary-mode QNode m = h.next; // node to fulfill if (t != tail || m == null || h != head) continue; // inconsistent read Object x = m.item; //x为null说明已经被消费 if (isData == (x != null) || // m already fulfilled x == m || // m cancelled //通过cas将首节点设置为e（null） !m.casItem(x, e)) &#123; // lost CAS advanceHead(h, m); // dequeue and retry continue; &#125; advanceHead(h, m); // successfully fulfilled //唤醒节点设置的线程 LockSupport.unpark(m.waiter); //返回获取到的item return (x != null) ? (E)x : e; &#125; &#125;&#125; 先判断队列是否为空，或者尾结点与当前节点模式相同，则将节点加入队列尾部 等待线程被唤醒（put被take唤醒，take被put唤醒）处理 如果队列不为空，或者尾结点与当前节点模式不相同，则唤醒头部节点，取出数据，并把头部节点移除 TransferStack::transfer123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687E transfer(E e, boolean timed, long nanos) &#123; /* * Basic algorithm is to loop trying one of three actions: * * 1. If apparently empty or already containing nodes of same * mode, try to push node on stack and wait for a match, * returning it, or null if cancelled. * * 2. If apparently containing node of complementary mode, * try to push a fulfilling node on to stack, match * with corresponding waiting node, pop both from * stack, and return matched item. The matching or * unlinking might not actually be necessary because of * other threads performing action 3: * * 3. If top of stack already holds another fulfilling node, * help it out by doing its match and/or pop * operations, and then continue. The code for helping * is essentially the same as for fulfilling, except * that it doesn't return the item. */ //如果e是null，则是REQUEST模式，不为null则是DATA模式 SNode s = null; // constructed/reused as needed int mode = (e == null) ? REQUEST : DATA; for (;;) &#123; SNode h = head; //如果是队列不为空 if (h == null || h.mode == mode) &#123; // empty or same-mode //如果超时 if (timed &amp;&amp; nanos &lt;= 0L) &#123; // can't wait if (h != null &amp;&amp; h.isCancelled()) //cas方式移除头部节点 casHead(h, h.next); // pop cancelled node else return null; //从头部插入数据 &#125; else if (casHead(h, s = snode(s, e, h, mode))) &#123; //等待节点被内的元素被处理完毕或等待超时 SNode m = awaitFulfill(s, timed, nanos); //如果是中断，则清除节点s并返回null if (m == s) &#123; // wait was cancelled clean(s); return null; &#125; //从头部取出数据并移除节点 if ((h = head) != null &amp;&amp; h.next == s) casHead(h, s.next); // help s's fulfiller return (E) ((mode == REQUEST) ? m.item : s.item); &#125; //如果节点h没有被处理完 &#125; else if (!isFulfilling(h.mode)) &#123; // try to fulfill if (h.isCancelled()) // already cancelled casHead(h, h.next); // pop and retry else if (casHead(h, s=snode(s, e, h, FULFILLING|mode))) &#123; for (;;) &#123; // loop until matched or waiters disappear SNode m = s.next; // m is s's match if (m == null) &#123; // all waiters are gone casHead(s, null); // pop fulfill node s = null; // use new node next time break; // restart main loop &#125; SNode mn = m.next; //尝试唤醒节点中保存的线程 if (m.tryMatch(s)) &#123; casHead(s, mn); // pop both s and m return (E) ((mode == REQUEST) ? m.item : s.item); &#125; else // lost match s.casNext(m, mn); // help unlink &#125; &#125; &#125; else &#123; // help a fulfiller SNode m = h.next; // m is h's match if (m == null) // waiter is gone casHead(h, null); // pop fulfilling node else &#123; SNode mn = m.next; //尝试唤醒节点中保存的线程 if (m.tryMatch(h)) // help match casHead(h, mn); // pop both h and m else // lost match h.casNext(m, mn); // help unlink &#125; &#125; &#125;&#125; 先判断队列是否为空，或者尾结点与当前节点模式相同，则将节点加入队列头部 等待线程被唤醒（put被take唤醒，take被put唤醒）处理 如果队列不为空，或者尾结点与当前节点模式不相同，则唤醒头部节点，取出数据，并把头部节点移除 总结SynchronousQueue是一个无空间的队列即不可以通过peek来获取数据或者contain判断数据是否在队列中。当队列为空时，队列执行take或put操作都会调用transfer，使线程进入阻塞，等待一个与tail节点模式互补（即put等take、take等put）的请求。如果新请求与队列tail节点的模式相同，则将请求加入队列，模式不同，则可进行消费从队列中移除节点。TransferStack：非公平的栈模式，先进后出（头进头出）TransferQueue：公平的队列模式，先进先出（尾进头出） 我的理解： SynchronousQueue不存储数据，只存储请求 当生产或消费请求到达时，如果队列中没有互补的请求，则将会此请求加入队列中，线程进入阻塞 等待互补的请求到达。 若是互补的请求到达时，则唤醒队列中的线程，消费请求使用生产请求中的数据内容。]]></content>
      <tags>
        <tag>Java</tag>
        <tag>线程安全</tag>
        <tag>并发编程</tag>
        <tag>锁机制</tag>
        <tag>同步</tag>
        <tag>synchronized</tag>
        <tag>锁</tag>
        <tag>LinkedBlockingQueue</tag>
        <tag>SynchronousQueue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【从入门到放弃-Java】并发编程-JUC-LinkedBlockingQueue]]></title>
    <url>%2F671bd3f8.html</url>
    <content type="text"><![CDATA[简介上一篇【从入门到放弃-Java】并发编程-JUC-ConcurrentLinkedQueue学习了并发队列ConcurrentLinkedQueue，它是一个非阻塞无界队列。本文来学习下JUC中的一个阻塞有界队列-LinkedBlockingQueue。 LinkedBlockingQueue 如图继承了AbstractQueue类，实现了BlockingQueue和Serializable接口 LinkedBlockingQueue1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/** * Creates a &#123;@code LinkedBlockingQueue&#125; with a capacity of * &#123;@link Integer#MAX_VALUE&#125;. */// 如果没传capacity 则默认使用Integer.MAX_VALUE作为队列大小public LinkedBlockingQueue() &#123; this(Integer.MAX_VALUE);&#125;/** * Creates a &#123;@code LinkedBlockingQueue&#125; with the given (fixed) capacity. * * @param capacity the capacity of this queue * @throws IllegalArgumentException if &#123;@code capacity&#125; is not greater * than zero */ //设置大小为capacity，并设置item为null的head和last辅助节点public LinkedBlockingQueue(int capacity) &#123; if (capacity &lt;= 0) throw new IllegalArgumentException(); this.capacity = capacity; last = head = new Node&lt;E&gt;(null);&#125;/** * Creates a &#123;@code LinkedBlockingQueue&#125; with a capacity of * &#123;@link Integer#MAX_VALUE&#125;, initially containing the elements of the * given collection, * added in traversal order of the collection's iterator. * * @param c the collection of elements to initially contain * @throws NullPointerException if the specified collection or any * of its elements are null */public LinkedBlockingQueue(Collection&lt;? extends E&gt; c) &#123; this(Integer.MAX_VALUE); //加入队锁 final ReentrantLock putLock = this.putLock; putLock.lock(); // Never contended, but necessary for visibility try &#123; int n = 0; //依次将Collection中的元素加入队列 for (E e : c) &#123; if (e == null) throw new NullPointerException(); if (n == capacity) throw new IllegalStateException("Queue full"); enqueue(new Node&lt;E&gt;(e)); ++n; &#125; //设置队列大小n count.set(n); &#125; finally &#123; //解锁 putLock.unlock(); &#125;&#125; 可以从构造函数看出，LinkedBlockingQueue是一个有界的队列，队列最大值为capacity，如果初始化时不设置队列大小，则默认大小为Integer.MAX_VALUE put将元素加入队列，如果队列满，则一直等待，直到线程被中断或被唤醒1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * Inserts the specified element at the tail of this queue, waiting if * necessary for space to become available. * * @throws InterruptedException &#123;@inheritDoc&#125; * @throws NullPointerException &#123;@inheritDoc&#125; */public void put(E e) throws InterruptedException &#123; if (e == null) throw new NullPointerException(); final int c; final Node&lt;E&gt; node = new Node&lt;E&gt;(e); //入队锁，如果收到中断信号，则抛出异常 final ReentrantLock putLock = this.putLock; final AtomicInteger count = this.count; putLock.lockInterruptibly(); try &#123; /* * Note that count is used in wait guard even though it is * not protected by lock. This works because count can * only decrease at this point (all other puts are shut * out by lock), and we (or some other waiting put) are * signalled if it ever changes from capacity. Similarly * for all other uses of count in other wait guards. */ //如果队列满了，则通知线程进入await状态。 while (count.get() == capacity) &#123; notFull.await(); &#125; //将node加入队列 enqueue(node); //队列元素数加一 c = count.getAndIncrement(); //如果队列没满，则唤醒await的线程进行入队操作 if (c + 1 &lt; capacity) notFull.signal(); &#125; finally &#123; //释放锁 putLock.unlock(); &#125; //如果是第一次添加元素，则通知等待的读线程可以开始读数据了 if (c == 0) signalNotEmpty();&#125; offer12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485/** * Inserts the specified element at the tail of this queue if it is * possible to do so immediately without exceeding the queue's capacity, * returning &#123;@code true&#125; upon success and &#123;@code false&#125; if this queue * is full. * When using a capacity-restricted queue, this method is generally * preferable to method &#123;@link BlockingQueue#add add&#125;, which can fail to * insert an element only by throwing an exception. * * @throws NullPointerException if the specified element is null */ //将元素加入队列，如果队列满，则直接返回falsepublic boolean offer(E e) &#123; if (e == null) throw new NullPointerException(); final AtomicInteger count = this.count; //如果队列满了，则直接返回false。 if (count.get() == capacity) return false; final int c; final Node&lt;E&gt; node = new Node&lt;E&gt;(e); //加入队锁 final ReentrantLock putLock = this.putLock; putLock.lock(); try &#123; //再次判断， 队列是否满了，避免在第一次判断后和加锁前，队列被加满 if (count.get() == capacity) return false; //将node添加到队列中 enqueue(node); c = count.getAndIncrement(); //如果队列没满，则唤醒await的线程进行入队操作 if (c + 1 &lt; capacity) notFull.signal(); &#125; finally &#123; //释放锁 putLock.unlock(); &#125; //如果是第一次添加元素，则通知等待的读线程可以开始读数据了 if (c == 0) signalNotEmpty(); return true;&#125;/** * Inserts the specified element at the tail of this queue, waiting if * necessary up to the specified wait time for space to become available. * * @return &#123;@code true&#125; if successful, or &#123;@code false&#125; if * the specified waiting time elapses before space is available * @throws InterruptedException &#123;@inheritDoc&#125; * @throws NullPointerException &#123;@inheritDoc&#125; */ //将元素加入队列，可以设置等待超时时间，如果队列满，则等待timeout毫秒，超时返回falsepublic boolean offer(E e, long timeout, TimeUnit unit) throws InterruptedException &#123; if (e == null) throw new NullPointerException(); long nanos = unit.toNanos(timeout); final int c; //加锁 final ReentrantLock putLock = this.putLock; final AtomicInteger count = this.count; putLock.lockInterruptibly(); try &#123; //如果队列满了，则等待timeout毫秒，超时则返回false while (count.get() == capacity) &#123; if (nanos &lt;= 0L) return false; nanos = notFull.awaitNanos(nanos); &#125; //入队 enqueue(new Node&lt;E&gt;(e)); c = count.getAndIncrement(); //如果队列没满，则唤醒await的线程进行入队操作 if (c + 1 &lt; capacity) notFull.signal(); &#125; finally &#123; //释放锁 putLock.unlock(); &#125; //如果是第一次添加元素，则通知等待的读线程可以开始读数据了 if (c == 0) signalNotEmpty(); return true;&#125; take从队列中取出元素，如果队列为空，则一直等待，直到线程被中断或被唤醒12345678910111213141516171819202122232425262728public E take() throws InterruptedException &#123; final E x; final int c; final AtomicInteger count = this.count; //加出队锁 final ReentrantLock takeLock = this.takeLock; takeLock.lockInterruptibly(); try &#123; //如果队列为空，则通知线程进入await状态。 while (count.get() == 0) &#123; notEmpty.await(); &#125; //从队列头部取出元素 x = dequeue(); //count减一 c = count.getAndDecrement(); //如果队列不为空，则唤醒出队等待线程 if (c &gt; 1) notEmpty.signal(); &#125; finally &#123; //释放锁 takeLock.unlock(); &#125; //如果从满的队列中出列，则唤醒入队线程，队列已经不满了，可以添加元素了 if (c == capacity) signalNotFull(); return x;&#125; poll123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public E poll() &#123; final AtomicInteger count = this.count; //如果队列为空，直接返回null if (count.get() == 0) return null; final E x; final int c; //加出队锁 final ReentrantLock takeLock = this.takeLock; takeLock.lock(); try &#123; //如果队列为空，直接返回null if (count.get() == 0) return null; //出队，移除第一个数据节点 x = dequeue(); c = count.getAndDecrement(); //如果队列不为空，则唤醒出队等待线程 if (c &gt; 1) notEmpty.signal(); &#125; finally &#123; takeLock.unlock(); &#125; //如果从满的队列中出列，则唤醒入队线程，队列已经不满了，可以添加元素了 if (c == capacity) signalNotFull(); return x;&#125;public E poll(long timeout, TimeUnit unit) throws InterruptedException &#123; final E x; final int c; long nanos = unit.toNanos(timeout); final AtomicInteger count = this.count; final ReentrantLock takeLock = this.takeLock; takeLock.lockInterruptibly(); try &#123; //如果队列为空，则等待timeout时间， 超时返回null while (count.get() == 0) &#123; if (nanos &lt;= 0L) return null; nanos = notEmpty.awaitNanos(nanos); &#125; //出队列 x = dequeue(); c = count.getAndDecrement(); //如果队列不为空，则唤醒出队等待线程 if (c &gt; 1) notEmpty.signal(); &#125; finally &#123; //释放锁 takeLock.unlock(); &#125; //如果从满的队列中出列，则唤醒入队线程，队列已经不满了，可以添加元素了 if (c == capacity) signalNotFull(); return x; &#125; peek1234567891011121314public E peek() &#123; final AtomicInteger count = this.count; //如果队列为空，返回null if (count.get() == 0) return null; final ReentrantLock takeLock = this.takeLock; takeLock.lock(); try &#123; //队列不为空返回第一个数据节点的元素，不移除节点，为空则返回null return (count.get() &gt; 0) ? head.next.item : null; &#125; finally &#123; takeLock.unlock(); &#125;&#125; remove1234567891011121314151617181920212223242526272829303132/** * Removes a single instance of the specified element from this queue, * if it is present. More formally, removes an element &#123;@code e&#125; such * that &#123;@code o.equals(e)&#125;, if this queue contains one or more such * elements. * Returns &#123;@code true&#125; if this queue contained the specified element * (or equivalently, if this queue changed as a result of the call). * * @param o element to be removed from this queue, if present * @return &#123;@code true&#125; if this queue changed as a result of the call */public boolean remove(Object o) &#123; //如果移除的元素为null，在返回false if (o == null) return false; //加入队和出队锁 fullyLock(); try &#123; //遍历队列，存在元素o则移除，返回true，否则返回false for (Node&lt;E&gt; pred = head, p = pred.next; p != null; pred = p, p = p.next) &#123; if (o.equals(p.item)) &#123; unlink(p, pred); return true; &#125; &#125; return false; &#125; finally &#123; //释放入队锁和出队锁 fullyUnlock(); &#125;&#125; add、element都在AbstractQueue中实现，上文【从入门到放弃-Java】并发编程-JUC-ConcurrentLinkedQueue中已分析，不再赘述。 总结通过源码分析，我们了解到了入队和出队的机制。初始化时，需要设置队列大小， 在队列满时，入队操作会等待，队列为空时，出队操作会等待。即LinkedBlockingQueue是一个有界的阻塞队列。和ConcurrentLinkedQueue对比，LinkedBlockingQueue采用锁分离，比较适合生产和消费频率差不多的场景，并且锁同步更适合单消费者的任务队列，而ConcurrentLinkedQueue使用CAS，并发性能较高更适合消费者多的消息队列。在常用线程池中，Executors.newFixedThreadPool也是采用的LinkedBlockingQueue作为workQueue，在线程数超过corePoolSize后，会将任务加入到workQueue中等待处理。关于线程池的使用，后面会详细展开。]]></content>
      <tags>
        <tag>Java</tag>
        <tag>线程安全</tag>
        <tag>并发编程</tag>
        <tag>锁机制</tag>
        <tag>同步</tag>
        <tag>synchronized</tag>
        <tag>锁</tag>
        <tag>LinkedBlockingQueue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【从入门到放弃-Java】并发编程-JUC-ConcurrentLinkedQueue]]></title>
    <url>%2F6f9eb5ee.html</url>
    <content type="text"><![CDATA[简介队列是一种先进先出的数据结构，在排队、削峰、缓存等多种场景下都会用到。今天学习下JUC中提供的并发队列-ConcurrentLinkedQueue可以看他的继承和实现接口非常简单，继承了AbstractQueue类，实现了Queue接口。 ConcurrentLinkedQueueadd在队列尾部新添加一个元素123456789101112/** * Inserts the specified element at the tail of this queue. * As the queue is unbounded, this method will never throw * &#123;@link IllegalStateException&#125; or return &#123;@code false&#125;. * * @return &#123;@code true&#125; (as specified by &#123;@link Collection#add&#125;) * @throws NullPointerException if the specified element is null */ //在队列尾部加入新的元素，元素不能为null。这里直接调用看offer方法。public boolean add(E e) &#123; return offer(e);&#125; offer在队列尾部新添加一个元素123456789101112131415161718192021222324252627282930313233343536373839404142/** * Inserts the specified element at the tail of this queue. * As the queue is unbounded, this method will never return &#123;@code false&#125;. * * @return &#123;@code true&#125; (as specified by &#123;@link Queue#offer&#125;) * @throws NullPointerException if the specified element is null */public boolean offer(E e) &#123; //创建一个新的节点，新的节点元素不能为null。 final Node&lt;E&gt; newNode = new Node&lt;E&gt;(Objects.requireNonNull(e)); //从tail开始遍历队列，直到队尾 for (Node&lt;E&gt; t = tail, p = t;;) &#123; Node&lt;E&gt; q = p.next; //如果p是最后一个节点 if (q == null) &#123; // p is last node //通过CAS的方式把newNode加到下一个节点 if (NEXT.compareAndSet(p, null, newNode)) &#123; // Successful CAS is the linearization point // for e to become an element of this queue, // and for newNode to become "live". //插入成功后，再次判断，tail是否已经改变，如果p != tail，则尝试将newNode设置为Tail，设置失败也没关系，因为有其它线程设置了 if (p != t) // hop two nodes at a time; failure is OK TAIL.weakCompareAndSet(this, t, newNode); return true; &#125; // Lost CAS race to another thread; re-read next &#125; else if (p == q) // We have fallen off list. If tail is unchanged, it // will also be off-list, in which case we need to // jump to head, from which all live nodes are always // reachable. Else the new tail is a better bet. //当p == q时如并发时节点被删除等。需要重新设置p p = (t != (t = tail)) ? t : head; else // Check for tail updates after two hops. //如果p在上次赋值后，if处理前，节点有新增，则走到这一步 p = (p != t &amp;&amp; t != (t = tail)) ? t : q; &#125;&#125; poll取出队列头部的一个元素删除1234567891011121314151617181920212223public E poll() &#123; restartFromHead: for (;;) &#123; for (Node&lt;E&gt; h = head, p = h, q;; p = q) &#123; final E item; //从队列首部head取出元素，并使用cas将头部设为null。 if ((item = p.item) != null &amp;&amp; p.casItem(item, null)) &#123; // Successful CAS is the linearization point // for item to be removed from this queue. if (p != h) // hop two nodes at a time updateHead(h, ((q = p.next) != null) ? q : p); return item; &#125; //如果队列空了，返回null else if ((q = p.next) == null) &#123; updateHead(h, p); return null; &#125; //如果p已经被删除 返回头部重新开始获取 else if (p == q) continue restartFromHead; &#125; &#125;&#125; peek取出队列头的元素，但不删除12345678910111213141516public E peek() &#123; restartFromHead: for (;;) &#123; for (Node&lt;E&gt; h = head, p = h, q;; p = q) &#123; final E item; //返回head的元素，如果队列为空，返回null if ((item = p.item) != null || (q = p.next) == null) &#123; updateHead(h, p); return item; &#125; //如果head被删除掉，则返回头部继续获取 else if (p == q) continue restartFromHead; &#125; &#125;&#125; remove123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960//取出队列头的元素并在队列中删除，和poll的区别是，如果队列为空，remove会抛出异常。而poll会返回null/** * Retrieves and removes the head of this queue. This method differs * from &#123;@link #poll poll&#125; only in that it throws an exception if this * queue is empty. * * &lt;p&gt;This implementation returns the result of &#123;@code poll&#125; * unless the queue is empty. * * @return the head of this queue * @throws NoSuchElementException if this queue is empty */public E remove() &#123; E x = poll(); //调用poll方法，获取队列头部的元素并删除，如果队列为空，抛出异常 if (x != null) return x; else throw new NoSuchElementException();&#125;//移除队列中的某个元素/** * Removes a single instance of the specified element from this queue, * if it is present. More formally, removes an element &#123;@code e&#125; such * that &#123;@code o.equals(e)&#125;, if this queue contains one or more such * elements. * Returns &#123;@code true&#125; if this queue contained the specified element * (or equivalently, if this queue changed as a result of the call). * * @param o element to be removed from this queue, if present * @return &#123;@code true&#125; if this queue changed as a result of the call */public boolean remove(Object o) &#123; if (o == null) return false; restartFromHead: for (;;) &#123; //从队列头部开始查找 for (Node&lt;E&gt; p = head, pred = null; p != null; ) &#123; Node&lt;E&gt; q = p.next; final E item; //如果找到等于o的元素，则移除并返回true if ((item = p.item) != null) &#123; if (o.equals(item) &amp;&amp; p.casItem(item, null)) &#123; skipDeadNodes(pred, p, p, q); return true; &#125; pred = p; p = q; continue; &#125; //如果是队列尾部，判断是否新插入了数据。如果插入了，则返回头部重新查找 for (Node&lt;E&gt; c = p;; q = p.next) &#123; if (q == null || q.item != null) &#123; pred = skipDeadNodes(pred, c, p, q); p = q; break; &#125; if (p == (p = q)) continue restartFromHead; &#125; &#125; //如果没找到 返回false return false; &#125;&#125; element获取队列头的元素，但不会从队列中删除，和peek的区别是，如果队列为空，则element会抛出异常，peek是返回null12345678910111213141516171819/** * Retrieves, but does not remove, the head of this queue. This method * differs from &#123;@link #peek peek&#125; only in that it throws an exception if * this queue is empty. * * &lt;p&gt;This implementation returns the result of &#123;@code peek&#125; * unless the queue is empty. * * @return the head of this queue * @throws NoSuchElementException if this queue is empty */public E element() &#123; E x = peek(); //调用peek方法返回队列头部数据但不删除，如果队列为空返回null，则抛出异常 if (x != null) return x; else throw new NoSuchElementException();&#125; isEmpty判断队列是否为空1234567891011121314151617181920212223242526272829303132/** * Returns &#123;@code true&#125; if this queue contains no elements. * * @return &#123;@code true&#125; if this queue contains no elements */public boolean isEmpty() &#123; //如果第一个节点时null 则是空的 return first() == null;&#125;/** * Returns the first live (non-deleted) node on list, or null if none. * This is yet another variant of poll/peek; here returning the * first node, not element. We could make peek() a wrapper around * first(), but that would cost an extra volatile read of item, * and the need to add a retry loop to deal with the possibility * of losing a race to a concurrent poll(). */ //会返回第一个Node，和peek的区别是，peek是返回的第一个Node中的元素Node&lt;E&gt; first() &#123; restartFromHead: for (;;) &#123; for (Node&lt;E&gt; h = head, p = h, q;; p = q) &#123; boolean hasItem = (p.item != null); if (hasItem || (q = p.next) == null) &#123; updateHead(h, p); return hasItem ? p : null; &#125; else if (p == q) continue restartFromHead; &#125; &#125;&#125; size获取队列的大小，会遍历队列计算。但是因为此方法无锁，获取的数据可能会不准确。12345678910111213141516171819202122232425262728293031/** * Returns the number of elements in this queue. If this queue * contains more than &#123;@code Integer.MAX_VALUE&#125; elements, returns * &#123;@code Integer.MAX_VALUE&#125;. * * &lt;p&gt;Beware that, unlike in most collections, this method is * &lt;em&gt;NOT&lt;/em&gt; a constant-time operation. Because of the * asynchronous nature of these queues, determining the current * number of elements requires an O(n) traversal. * Additionally, if elements are added or removed during execution * of this method, the returned result may be inaccurate. Thus, * this method is typically not very useful in concurrent * applications. * * @return the number of elements in this queue */public int size() &#123; restartFromHead: for (;;) &#123; int count = 0; //会遍历队列中的节点。count值每次加一。因此，size方法是比较耗时的，且在队列中的节点删除或添加时，这个值可能不准。因此建议还是用isEmpty来判断队列是否为空。尽量别使用此方法。 for (Node&lt;E&gt; p = first(); p != null;) &#123; if (p.item != null) if (++count == Integer.MAX_VALUE) break; // @see Collection.size() if (p == (p = p.next)) continue restartFromHead; &#125; return count; &#125;&#125; 总结通过对源码的学习，我们了解到以下几个重点： ConcurrentLinkedQueue是基于CAS来进行并发控制的，因此同步的代价较小，并发性能比较好。是一个非阻塞队列 ConcurrentLinkedQueue是无界的，队列中的元素无个数限制 size方法需要遍历队列的节点时间复杂度为O(n)，性能会随着队列长度降低，且因为无锁，队列可能被增删，不一定能获取到正确的结果。]]></content>
      <tags>
        <tag>Java</tag>
        <tag>线程安全</tag>
        <tag>并发编程</tag>
        <tag>锁机制</tag>
        <tag>同步</tag>
        <tag>synchronized</tag>
        <tag>锁</tag>
        <tag>ConcurrentLinkedQueue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【从入门到放弃-Java】并发编程-JUC-CopyOnWriteArraySet]]></title>
    <url>%2Fbb660a13.html</url>
    <content type="text"><![CDATA[前言CopyOnWriteArraySet也是JUC下常用容器，其底层实现是基于CopyOnWriteArrayList的，关于CopyOnWriteArrayList的详情可以查看【从入门到放弃-Java】并发编程-JUC-CopyOnWriteArrayList，接下来我们看下源码。 CopyOnWriteArraySetCopyOnWriteArraySet1234567891011121314151617181920212223242526/** * Creates an empty set. */ //简单粗暴，只有一个成员变量al，是CopyOnWriteArrayList类型的，初始化时，new一个CopyOnWriteArrayList赋值给al。public CopyOnWriteArraySet() &#123; al = new CopyOnWriteArrayList&lt;E&gt;();&#125;/** * Creates a set containing all of the elements of the specified * collection. * * @param c the collection of elements to initially contain * @throws NullPointerException if the specified collection is null */public CopyOnWriteArraySet(Collection&lt;? extends E&gt; c) &#123; if (c.getClass() == CopyOnWriteArraySet.class) &#123; @SuppressWarnings("unchecked") CopyOnWriteArraySet&lt;E&gt; cc = (CopyOnWriteArraySet&lt;E&gt;)c; al = new CopyOnWriteArrayList&lt;E&gt;(cc.al); &#125; else &#123; al = new CopyOnWriteArrayList&lt;E&gt;(); al.addAllAbsent(c); &#125;&#125; add123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/** * Adds the specified element to this set if it is not already present. * More formally, adds the specified element &#123;@code e&#125; to this set if * the set contains no element &#123;@code e2&#125; such that * &#123;@code Objects.equals(e, e2)&#125;. * If this set already contains the element, the call leaves the set * unchanged and returns &#123;@code false&#125;. * * @param e element to be added to this set * @return &#123;@code true&#125; if this set did not already contain the specified * element */public boolean add(E e) &#123; //调用CopyOnWriteArrayList的addIfAbsent方法。 return al.addIfAbsent(e);&#125;//看下CopyOnWriteArrayList的addIfAbsent方法如何实现。/** * Appends the element, if not present. * * @param e element to be added to this list, if absent * @return &#123;@code true&#125; if the element was added */public boolean addIfAbsent(E e) &#123; Object[] snapshot = getArray(); //先查找e是否在存在，不存在的话 调用addIfAbsent(E e, Object[] snapshot)方法 return indexOfRange(e, snapshot, 0, snapshot.length) &lt; 0 &amp;&amp; addIfAbsent(e, snapshot);&#125;/** * A version of addIfAbsent using the strong hint that given * recent snapshot does not contain e. */private boolean addIfAbsent(E e, Object[] snapshot) &#123; //加锁 synchronized (lock) &#123; Object[] current = getArray(); int len = current.length; //加锁后check下快照是否被改动 if (snapshot != current) &#123; //如果改动过，则判断改动后的数组是否包含要添加的元素，如果有的话，则返回失败 // Optimize for lost race to another addXXX operation int common = Math.min(snapshot.length, len); for (int i = 0; i &lt; common; i++) if (current[i] != snapshot[i] &amp;&amp; Objects.equals(e, current[i])) return false; if (indexOfRange(e, current, common, len) &gt;= 0) return false; &#125; //验证数组中没有此元素，则在末尾添加。 Object[] newElements = Arrays.copyOf(current, len + 1); newElements[len] = e; //替换原数组。 setArray(newElements); return true; &#125;&#125; addAll123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657/** * Adds all of the elements in the specified collection to this set if * they're not already present. If the specified collection is also a * set, the &#123;@code addAll&#125; operation effectively modifies this set so * that its value is the &lt;i&gt;union&lt;/i&gt; of the two sets. The behavior of * this operation is undefined if the specified collection is modified * while the operation is in progress. * * @param c collection containing elements to be added to this set * @return &#123;@code true&#125; if this set changed as a result of the call * @throws NullPointerException if the specified collection is null * @see #add(Object) */public boolean addAll(Collection&lt;? extends E&gt; c) &#123; //直接调用CopyOnWriteArrayList的addAllAbsent方法。 return al.addAllAbsent(c) &gt; 0;&#125;/** * Appends all of the elements in the specified collection that * are not already contained in this list, to the end of * this list, in the order that they are returned by the * specified collection's iterator. * * @param c collection containing elements to be added to this list * @return the number of elements added * @throws NullPointerException if the specified collection is null * @see #addIfAbsent(Object) */public int addAllAbsent(Collection&lt;? extends E&gt; c) &#123; //将要添加的元素变为数组 Object[] cs = c.toArray(); if (cs.length == 0) return 0; //加锁 synchronized (lock) &#123; Object[] es = getArray(); int len = es.length; int added = 0; // uniquify and compact elements in cs //遍历要添加的数组，如果在List的数组中不存在此元素，则添加到cs数组中 for (int i = 0; i &lt; cs.length; ++i) &#123; Object e = cs[i]; if (indexOfRange(e, es, 0, len) &lt; 0 &amp;&amp; indexOfRange(e, cs, 0, added) &lt; 0) cs[added++] = e; &#125; if (added &gt; 0) &#123; Object[] newElements = Arrays.copyOf(es, len + added); //将cs数组的前added个元素添加在List数组后面 System.arraycopy(cs, 0, newElements, len, added); //替换原数组 setArray(newElements); &#125; return added; &#125;&#125; remove123456789101112131415/** * Removes the specified element from this set if it is present. * More formally, removes an element &#123;@code e&#125; such that * &#123;@code Objects.equals(o, e)&#125;, if this set contains such an element. * Returns &#123;@code true&#125; if this set contained the element (or * equivalently, if this set changed as a result of the call). * (This set will not contain the element once the call returns.) * * @param o object to be removed from this set, if present * @return &#123;@code true&#125; if this set contained the specified element */public boolean remove(Object o) &#123; //直接调用CopyOnWriteArrayList的remove return al.remove(o);&#125; 总结通过源码分析，我们了解到，CopyOnWriteArraySet主要是依赖CopyOnWriteArrayList来实现各方法的。因此与CopyOnWriteArrayList一样，更适用于读多写少的并发操作中。详细原因在【从入门到放弃-Java】并发编程-JUC-CopyOnWriteArrayList中以及解释过了，这里不再赘述。 值得一提的是，常用的非并发容器HashSet，是基于HashMap实现的，利用HashMap中Entry的key做到唯一值，Entry的value是一个不可变静态对象Object。但是JUC中并发Set是却不是基于Map的，学习完这三章并发容器，你能回答这是为什么吗？可以在评论中留言，我们一起探讨下哦~]]></content>
      <tags>
        <tag>Java</tag>
        <tag>线程安全</tag>
        <tag>并发编程</tag>
        <tag>锁机制</tag>
        <tag>同步</tag>
        <tag>synchronized</tag>
        <tag>锁</tag>
        <tag>CopyOnWriteArrayList</tag>
        <tag>CopyOnWriteArraySet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【从入门到放弃-Java】并发编程-JUC-CopyOnWriteArrayList]]></title>
    <url>%2F4f4e5516.html</url>
    <content type="text"><![CDATA[前言上文【从入门到放弃-Java】并发编程-JUC-ConcurrentHashMap中，我们学习了常用的并发容器CurrentHashMap，本文我们来了解下List的并发容器：CopyOnWriteArrayList直接来看源码。 CopyOnWriteArrayListadd1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * Appends the specified element to the end of this list. * * @param e element to be appended to this list * @return &#123;@code true&#125; (as specified by &#123;@link Collection#add&#125;) */public boolean add(E e) &#123; //使用synchronized加锁 synchronized (lock) &#123; Object[] es = getArray(); int len = es.length; //拷贝一份array，数组大小加一 es = Arrays.copyOf(es, len + 1); //设置最后一位为需要添加的值e es[len] = e; //将新的array设置为当前List的中的值，array是volatile类型的，因此写入后其它线程能立即看到 setArray(es); return true; &#125;&#125;/** * Inserts the specified element at the specified position in this * list. Shifts the element currently at that position (if any) and * any subsequent elements to the right (adds one to their indices). * * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; */public void add(int index, E element) &#123; //使用synchronized加锁 synchronized (lock) &#123; Object[] es = getArray(); int len = es.length; if (index &gt; len || index &lt; 0) throw new IndexOutOfBoundsException(outOfBounds(index, len)); Object[] newElements; int numMoved = len - index; if (numMoved == 0) //如果是在尾部插入，则将List中的数组直接copy并将长度加一 newElements = Arrays.copyOf(es, len + 1); else &#123; //如果不是在尾部插入，则以index处将原array分割成两部分copy到新数组中，空出index的位置 newElements = new Object[len + 1]; System.arraycopy(es, 0, newElements, 0, index); System.arraycopy(es, index, newElements, index + 1, numMoved); &#125; //在index出设置element的值 newElements[index] = element; setArray(newElements); &#125;&#125; addAll12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182/** * Appends all of the elements in the specified collection to the end * of this list, in the order that they are returned by the specified * collection's iterator. * * @param c collection containing elements to be added to this list * @return &#123;@code true&#125; if this list changed as a result of the call * @throws NullPointerException if the specified collection is null * @see #add(Object) */public boolean addAll(Collection&lt;? extends E&gt; c) &#123; //获取数组 Object[] cs = (c.getClass() == CopyOnWriteArrayList.class) ? ((CopyOnWriteArrayList&lt;?&gt;)c).getArray() : c.toArray(); if (cs.length == 0) return false; synchronized (lock) &#123; Object[] es = getArray(); int len = es.length; Object[] newElements; if (len == 0 &amp;&amp; cs.getClass() == Object[].class) //如果当前List长度为0，则直接设置array为新数组 newElements = cs; else &#123; //将原数组copy为新的数组，新的数组长度为len+cs.lenth newElements = Arrays.copyOf(es, len + cs.length); //从len处开始，设置为需要添加的数组 System.arraycopy(cs, 0, newElements, len, cs.length); &#125; //将值写入List的成员变量array，array是volatile类型的，因此写入后其它线程能立即看到 setArray(newElements); return true; &#125;&#125;/** * Inserts all of the elements in the specified collection into this * list, starting at the specified position. Shifts the element * currently at that position (if any) and any subsequent elements to * the right (increases their indices). The new elements will appear * in this list in the order that they are returned by the * specified collection's iterator. * * @param index index at which to insert the first element * from the specified collection * @param c collection containing elements to be added to this list * @return &#123;@code true&#125; if this list changed as a result of the call * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; * @throws NullPointerException if the specified collection is null * @see #add(int,Object) */public boolean addAll(int index, Collection&lt;? extends E&gt; c) &#123; //获取数组 Object[] cs = c.toArray(); synchronized (lock) &#123; Object[] es = getArray(); int len = es.length; if (index &gt; len || index &lt; 0) throw new IndexOutOfBoundsException(outOfBounds(index, len)); if (cs.length == 0) //如果当前List长度为0，直接返回false插入失败 return false; int numMoved = len - index; Object[] newElements; if (numMoved == 0) //如果List尾部插入，则直接在尾部copy要插入的数组 newElements = Arrays.copyOf(es, len + cs.length); else &#123; //如果不是在尾部插入，则以index处将原array分割成两部分copy到新数组中，空出index到index+cs.length长度的位置 newElements = new Object[len + cs.length]; System.arraycopy(es, 0, newElements, 0, index); System.arraycopy(es, index, newElements, index + cs.length, numMoved); &#125; //将index到index+cs.length填充需要插入的数组 System.arraycopy(cs, 0, newElements, index, cs.length); setArray(newElements); return true; &#125;&#125; get1234567//get方式就比较简单了 因为array是volatile类型的，不需要任何同步操作就可取到值，保证了并发public E get(int index) &#123; return elementAt(getArray(), index);&#125;static &lt;E&gt; E elementAt(Object[] a, int index) &#123; return (E) a[index];&#125; remove12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485/** * Removes the element at the specified position in this list. * Shifts any subsequent elements to the left (subtracts one from their * indices). Returns the element that was removed from the list. * * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; */public E remove(int index) &#123; //同步锁 synchronized (lock) &#123; Object[] es = getArray(); int len = es.length; E oldValue = elementAt(es, index); int numMoved = len - index - 1; Object[] newElements; if (numMoved == 0) //如果是移除最后一个元素，则直接copy 0到len-2位置的元素即可 newElements = Arrays.copyOf(es, len - 1); else &#123; //如果不是最后一个元素，则copy需要删除数组[0,index)和[index,length]的元素到新数组。 newElements = new Object[len - 1]; System.arraycopy(es, 0, newElements, 0, index); System.arraycopy(es, index + 1, newElements, index, numMoved); &#125; setArray(newElements); return oldValue; &#125;&#125;/** * Removes the first occurrence of the specified element from this list, * if it is present. If this list does not contain the element, it is * unchanged. More formally, removes the element with the lowest index * &#123;@code i&#125; such that &#123;@code Objects.equals(o, get(i))&#125; * (if such an element exists). Returns &#123;@code true&#125; if this list * contained the specified element (or equivalently, if this list * changed as a result of the call). * * @param o element to be removed from this list, if present * @return &#123;@code true&#125; if this list contained the specified element */public boolean remove(Object o) &#123; Object[] snapshot = getArray(); //找到需要删除元素的索引 int index = indexOfRange(o, snapshot, 0, snapshot.length); return index &gt;= 0 &amp;&amp; remove(o, snapshot, index);&#125;/** * A version of remove(Object) using the strong hint that given * recent snapshot contains o at the given index. */private boolean remove(Object o, Object[] snapshot, int index) &#123; synchronized (lock) &#123; Object[] current = getArray(); int len = current.length; //加锁后验证数组是否在找到要删除的索引后改动过，如果改动的话，删除改动后的第一个o元素 if (snapshot != current) findIndex: &#123; int prefix = Math.min(index, len); for (int i = 0; i &lt; prefix; i++) &#123; if (current[i] != snapshot[i] &amp;&amp; Objects.equals(o, current[i])) &#123; index = i; break findIndex; &#125; &#125; if (index &gt;= len) return false; if (current[index] == o) break findIndex; index = indexOfRange(o, current, index, len); if (index &lt; 0) return false; &#125; //删除index位置的元素 Object[] newElements = new Object[len - 1]; System.arraycopy(current, 0, newElements, 0, index); System.arraycopy(current, index + 1, newElements, index, len - index - 1); setArray(newElements); return true; &#125;&#125; iterator1234567891011121314/** * Returns an iterator over the elements in this list in proper sequence. * * &lt;p&gt;The returned iterator provides a snapshot of the state of the list * when the iterator was constructed. No synchronization is needed while * traversing the iterator. The iterator does &lt;em&gt;NOT&lt;/em&gt; support the * &#123;@code remove&#125; method. * * @return an iterator over the elements in this list in proper sequence */public Iterator&lt;E&gt; iterator() &#123; //和ArrayList不同，CopyOnWriteArrayList在iterator遍历时，是对当前的array做了一个快照，在遍历期间，array可能会被别的线程更改，但快照不会改变，不受影响，因此在迭代中，数组元素不能改。 return new COWIterator&lt;E&gt;(getArray(), 0);&#125; 总结通过源码分析我们了解到，CopyOnWriteArrayList写操作时，都是以加synchronized锁并copy一份数组进行修改的方式进行的，如果List比较大时，会非常占用资源。 读操作时不用加锁，因为array是volatile的，不需要额外同步，因此读性能非常高。 因此CopyOnWriteArrayList更适用于读多写少的并发操作中。 在遍历时，将当前的array做一个快照，不受其他线程更改的影响。因此，在iterator中，list中的元素是不能更改的（因为更改的是快照，不会写到原数组中，更改一定是无效的）。 但在for循环时，CopyOnWriteArrayList中的元素可以被更改，而ArrayList不行，因为ArrayList的元素被遍历到时总会先检查是否被更改，更改会抛出ConcurrentModificationException]]></content>
      <tags>
        <tag>Java</tag>
        <tag>线程安全</tag>
        <tag>并发编程</tag>
        <tag>锁机制</tag>
        <tag>同步</tag>
        <tag>synchronized</tag>
        <tag>锁</tag>
        <tag>CopyOnWriteArrayList</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【从入门到放弃-Java】并发编程-JUC-ConcurrentHashMap]]></title>
    <url>%2F304643e5.html</url>
    <content type="text"><![CDATA[前言上文【从入门到放弃-Java】并发编程-锁-synchronized中，我们介绍了可以使用内置锁synchronized同步类或代码块儿，到达线程安全的目的。 jdk帮我们把常用的一些模块封装成同步容器，如Vector、Hashtable、Collections.synchronizedXxx等。实现方式主要是将常用的容器类加了Synchronized同步。但我们知道，synchronized的频繁使用及竞争较为激烈时，对性能的影响比较大。 jdk1.5之后为我们提供了多种并发容器类，来提升同步容器的性能，这些类主要在java.util.concurrent包（简称juc，包内还有很多其它的并发工具类）中。我们本文先来学习下最常用的并发容器-ConcurrentHashMap。 ConcurrentHashMapput123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127/** * Maps the specified key to the specified value in this table. * Neither the key nor the value can be null. * * &lt;p&gt;The value can be retrieved by calling the &#123;@code get&#125; method * with a key that is equal to the original key. * * @param key key with which the specified value is to be associated * @param value value to be associated with the specified key * @return the previous value associated with &#123;@code key&#125;, or * &#123;@code null&#125; if there was no mapping for &#123;@code key&#125; * @throws NullPointerException if the specified key or value is null */ // key和value都不能是nullpublic V put(K key, V value) &#123; return putVal(key, value, false);&#125;/** Implementation for put and putIfAbsent */final V putVal(K key, V value, boolean onlyIfAbsent) &#123; //如果key或者value是null则立即抛出空指针异常 if (key == null || value == null) throw new NullPointerException(); //求hash值，将哈希的高位扩展到低位，并将高位强制为0。主要是为了减少hash冲突。 int hash = spread(key.hashCode()); int binCount = 0; //Node是Map.Entry的实现类，存放key、value。但key、value都不能是null。table的个数是2的n次方 for (Node&lt;K,V&gt;[] tab = table;;) &#123; Node&lt;K,V&gt; f; int n, i, fh; K fk; V fv; //Node会延迟初始化、即在第一次插入数据的时候进行初始化 if (tab == null || (n = tab.length) == 0) tab = initTable(); //以原子的方式获取Node数组n-1位置的node，如果未null，尝试插入新值 else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; //使用cas的方式设置新node的key、value值 if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value))) break; // no lock when adding to empty bin &#125; //如果Node是一个ForwardingNode，即有其它线程在扩容，则一起进行扩容操作 else if ((fh = f.hash) == MOVED) //如果当前正在扩容，则当前线程加入一起帮助扩容。 tab = helpTransfer(tab, f); //当使用putIfAbsent时，如果map中存在key，则返回对应的value else if (onlyIfAbsent // check first node without acquiring lock &amp;&amp; fh == hash &amp;&amp; ((fk = f.key) == key || (fk != null &amp;&amp; key.equals(fk))) &amp;&amp; (fv = f.val) != null) return fv; else &#123; V oldVal = null; /** * currentHashMap在JDK1.8中使用synchronized对需要修改的Node加锁同步，替代了JDK1.7及之前版本采用分段锁的方式。两种方式对比： * 1、1.7采用数组+Segment+分段锁的方式实现，分段锁及将几个map分为多个类似hashmap的结构，内部是多个Entry链表数组。加锁时，使用ReentrantLock对访问的Segment加锁，其它Segment可以正常操作。缺点是寻找节点需要两次hash，一次找到Segment，一次找到Entry链表的头部。 * 2、1.8采用数组+链表或红黑树的方式实现。使用Node替代了Segment，采用了CAS及synchronized进行同步。当Node链表的长度大于阙值（默认为8）时，会将链表转化为红黑树，提升查找性能。 * */ //通过synchronized的方式，对当前Node进行加锁操作。 synchronized (f) &#123; //判断f节点是否已被其它线程修改 if (tabAt(tab, i) == f) &#123; //如果当前Node还是链表结构时 if (fh &gt;= 0) &#123; binCount = 1; //遍历Node链表，设置value for (Node&lt;K,V&gt; e = f;; ++binCount) &#123; K ek; //如果当前节点的key与我们要设置的key相等时，则将值设置为value。 if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; &#125; Node&lt;K,V&gt; pred = e; //设置e为Node链表中的下一个元素，继续判断key是否相等，直到找到相等的key设置值。但如果链表中没有相等的key时，则在链表尾部新增一个元素，并设置值。 if ((e = e.next) == null) &#123; pred.next = new Node&lt;K,V&gt;(hash, key, value); break; &#125; &#125; &#125; //如果当前Node为红黑树结构时 else if (f instanceof TreeBin) &#123; Node&lt;K,V&gt; p; binCount = 2; //设置值 if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) &#123; oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; else if (f instanceof ReservationNode) throw new IllegalStateException("Recursive update"); &#125; &#125; //如果Node链表的长度大于8时，判断是链表结构扩容，或者转为红黑树结构 if (binCount != 0) &#123; if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break; &#125; &#125; &#125; addCount(1L, binCount); return null;&#125;/** * Copies all of the mappings from the specified map to this one. * These mappings replace any mappings that this map had for any of the * keys currently in the specified map. * * @param m mappings to be stored in this map */public void putAll(Map&lt;? extends K, ? extends V&gt; m) &#123; tryPresize(m.size()); for (Map.Entry&lt;? extends K, ? extends V&gt; e : m.entrySet()) putVal(e.getKey(), e.getValue(), false);&#125; get1234567891011121314151617181920212223242526272829303132333435/** * Returns the value to which the specified key is mapped, * or &#123;@code null&#125; if this map contains no mapping for the key. * * &lt;p&gt;More formally, if this map contains a mapping from a key * &#123;@code k&#125; to a value &#123;@code v&#125; such that &#123;@code key.equals(k)&#125;, * then this method returns &#123;@code v&#125;; otherwise it returns * &#123;@code null&#125;. (There can be at most one such mapping.) * * @throws NullPointerException if the specified key is null */public V get(Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek; //获取hash值 int h = spread(key.hashCode()); if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) &#123; //如果获取到的Node的hash值和key的相等，则说明是链表。 if ((eh = e.hash) == h) &#123; if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) return e.val; &#125; //如果获取Node的hash值小于0则说明是非链式结构 else if (eh &lt; 0) //不断查找Node的下一个节点，知道找到为止 return (p = e.find(h, key)) != null ? p.val : null; //不断查找Node的下一个节点，直到找到为止（感觉和find重复了。最外层的if中只需要一个Node::find方法就能搞定。知道原因的大神请指正） while ((e = e.next) != null) &#123; if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) return e.val; &#125; &#125; return null;&#125; treeifyBin扩容或将结构转为红黑树1234567891011121314151617181920212223242526272829private final void treeifyBin(Node&lt;K,V&gt;[] tab, int index) &#123; Node&lt;K,V&gt; b; int n; if (tab != null) &#123; //如果当前Node数组小于64则扩容，大于64时则转换为红黑树结构 if ((n = tab.length) &lt; MIN_TREEIFY_CAPACITY) //rehash：resize tryPresize(n &lt;&lt; 1); //如果是链表结构则转换为红黑树结构 else if ((b = tabAt(tab, index)) != null &amp;&amp; b.hash &gt;= 0) &#123; synchronized (b) &#123; if (tabAt(tab, index) == b) &#123; TreeNode&lt;K,V&gt; hd = null, tl = null; for (Node&lt;K,V&gt; e = b; e != null; e = e.next) &#123; //创建树节点，加入红黑树中 TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt;(e.hash, e.key, e.val, null, null); if ((p.prev = tl) == null) hd = p; else tl.next = p; tl = p; &#125; setTabAt(tab, index, new TreeBin&lt;K,V&gt;(hd)); &#125; &#125; &#125; &#125;&#125; tryPresize扩容操作123456789101112131415161718192021222324252627282930313233343536373839404142/** * Tries to presize table to accommodate the given number of elements. * * @param size number of elements (doesn't need to be perfectly accurate) */private final void tryPresize(int size) &#123; //size在传入前已经翻倍，这里会再次调整，变为为：大于(1.5 * oldSize + 1)的2的幂，且小于MAXIMUM_CAPACITY的大小 int c = (size &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; 1)) ? MAXIMUM_CAPACITY : tableSizeFor(size + (size &gt;&gt;&gt; 1) + 1); int sc; //当sizeCtl小于等于0时。说明已有线程在初始化或者rehash了 while ((sc = sizeCtl) &gt;= 0) &#123; Node&lt;K,V&gt;[] tab = table; int n; 如果table是空，即未初始化的话，进行初始化。 if (tab == null || (n = tab.length) == 0) &#123; n = (sc &gt; c) ? sc : c; if (U.compareAndSetInt(this, SIZECTL, sc, -1)) &#123; try &#123; if (table == tab) &#123; @SuppressWarnings("unchecked") Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; table = nt; //sc = n - n / 4 = 0.75，在final中，将sizeCtl设置为当前大小的0.75倍。大于这个阙值时，会再次进行扩容。 sc = n - (n &gt;&gt;&gt; 2); &#125; &#125; finally &#123; sizeCtl = sc; &#125; &#125; &#125; else if (c &lt;= sc || n &gt;= MAXIMUM_CAPACITY) break; //如果还未开始迁移 else if (tab == table) &#123; int rs = resizeStamp(n); if (U.compareAndSetInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) // 开始迁移 transfer(tab, null); &#125; &#125;&#125; transfer将Node迁移至新的table中123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161/** * Moves and/or copies the nodes in each bin to new table. See * above for explanation. */private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) &#123; int n = tab.length, stride; //设置线程迁移数据的步长，单核步长为n，多核为(n &gt;&gt;&gt; 3) / NCPU， 最小为16 if ((stride = (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n) &lt; MIN_TRANSFER_STRIDE) stride = MIN_TRANSFER_STRIDE; // subdivide range //如果要迁移的table还未初始化，则进行初始化动作 if (nextTab == null) &#123; // initiating try &#123; @SuppressWarnings("unchecked") Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n &lt;&lt; 1]; nextTab = nt; &#125; catch (Throwable ex) &#123; // try to cope with OOME sizeCtl = Integer.MAX_VALUE; return; &#125; nextTable = nextTab; transferIndex = n; &#125; int nextn = nextTab.length; //开始迁移，为要迁移的Node创建一个ForwardingNode节点。key和value都是null，hashcode为MOVED，nextTable指向新的table ForwardingNode&lt;K,V&gt; fwd = new ForwardingNode&lt;K,V&gt;(nextTab); //表示一个节点已被迁移完毕，可以迁移下一个了。 boolean advance = true; //迁移过程是否完毕。 boolean finishing = false; // to ensure sweep before committing nextTab //i是迁移的起始位置，bound是迁移的末尾。 for (int i = 0, bound = 0;;) &#123; Node&lt;K,V&gt; f; int fh; while (advance) &#123; int nextIndex, nextBound; //其实位置大于结束位置，说明已经迁移完毕 if (--i &gt;= bound || finishing) advance = false; //如果transferIndex小于等于0，则说明节点都已有线程在迁移了 else if ((nextIndex = transferIndex) &lt;= 0) &#123; i = -1; advance = false; &#125; else if (U.compareAndSetInt (this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex &gt; stride ? nextIndex - stride : 0))) &#123; bound = nextBound; i = nextIndex - 1; advance = false; &#125; &#125; //迁移结束 if (i &lt; 0 || i &gt;= n || i + n &gt;= nextn) &#123; int sc; //迁移完毕后，将新的table赋值给table成员变量，修改sizeCtl完成迁移 if (finishing) &#123; nextTable = null; table = nextTab; sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1); return; &#125; if (U.compareAndSetInt(this, SIZECTL, sc = sizeCtl, sc - 1)) &#123; //不相等说明还有线程没迁移完毕 if ((sc - 2) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) return; //所有线程迁移完毕后，设置finishing为完成。 finishing = advance = true; i = n; // recheck before commit &#125; &#125; //如果tab[i] = null，设置为fwd else if ((f = tabAt(tab, i)) == null) advance = casTabAt(tab, i, null, fwd); //如果当前节点已经迁移，则处理下一个节点 else if ((fh = f.hash) == MOVED) advance = true; // already processed else &#123; //加锁同步处理 synchronized (f) &#123; //验证下是否已经被其它线程处理 if (tabAt(tab, i) == f) &#123; Node&lt;K,V&gt; ln, hn; //如果是链表结构 if (fh &gt;= 0) &#123; //按照Node中元素hash值的第log(2)(n)位，记为runBit，是0或1将Node链表分为两个新的链表。 int runBit = fh &amp; n; Node&lt;K,V&gt; lastRun = f; for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) &#123; int b = p.hash &amp; n; if (b != runBit) &#123; runBit = b; lastRun = p; &#125; &#125; //将runBit位为0的链表记为ln，为1的设为hn。这里是标记最后一个不一致的节点，lastRun后节点的runBit都一样，因此不用新修改节点，减少消耗 if (runBit == 0) &#123; ln = lastRun; hn = null; &#125; else &#123; hn = lastRun; ln = null; &#125; for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) &#123; int ph = p.hash; K pk = p.key; V pv = p.val; //将runBit位为0的链表记为ln，为1的设为hn。 if ((ph &amp; n) == 0) ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn); &#125; //将分开的两个节点设置为table的i和i+n位。 setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; &#125; //如果是红黑树结构 else if (f instanceof TreeBin) &#123; TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; //按照Node中元素hash值的第log(2)(n)位，记为runBit，是0或1将Node红黑树分为两颗树。 TreeNode&lt;K,V&gt; lo = null, loTail = null; TreeNode&lt;K,V&gt; hi = null, hiTail = null; int lc = 0, hc = 0; for (Node&lt;K,V&gt; e = t.first; e != null; e = e.next) &#123; int h = e.hash; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt; (h, e.key, e.val, null, null); if ((h &amp; n) == 0) &#123; if ((p.prev = loTail) == null) lo = p; else loTail.next = p; loTail = p; ++lc; &#125; else &#123; if ((p.prev = hiTail) == null) hi = p; else hiTail.next = p; hiTail = p; ++hc; &#125; &#125; ln = (lc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(lo) : (hc != 0) ? new TreeBin&lt;K,V&gt;(lo) : t; hn = (hc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(hi) : (lc != 0) ? new TreeBin&lt;K,V&gt;(hi) : t; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; &#125; else if (f instanceof ReservationNode) throw new IllegalStateException("Recursive update"); &#125; &#125; &#125; &#125;&#125; 总结本文分析了currentHashMap是如何实现线程安全并提升性能的、如何扩容、JDK1.7和1.8实现方式的区别等 分Node加synchronize锁，不影响其它node的读写 Node节点hash冲突的元素数量少于8时，使用链表结构，大于等于8时，转换为红黑树结构提升查找性能 扩容时，会将table的长度扩大为大于(1.5 * oldSize + 1)的2的幂大小，并将每个Node根据log(2)(n)位是0或1，分为两个Node，放在新table的i和i+n的位置 JDK1.8将原currentHashMap使用数组+segment+ReentrantLock的方式改为数组+Node+CAS+synchronized的方式。减少了hash次数并采用cas和红黑树等多种优化提升性能]]></content>
      <tags>
        <tag>Java</tag>
        <tag>线程安全</tag>
        <tag>并发编程</tag>
        <tag>锁机制</tag>
        <tag>同步</tag>
        <tag>synchronized</tag>
        <tag>锁</tag>
        <tag>ConcurrentHashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【从入门到放弃-Java】并发编程-锁-synchronized]]></title>
    <url>%2F3b701acb.html</url>
    <content type="text"><![CDATA[简介上篇【从入门到放弃-Java】并发编程-线程安全中，我们了解到，可以通过加锁机制来保护共享对象，来实现线程安全。 synchronized是java提供的一种内置的锁机制。通过synchronized关键字同步代码块。线程在进入同步代码块之前会自动获得锁，并在退出同步代码块时自动释放锁。内置锁是一种互斥锁。 本文来深入学习下synchronized。 使用同步方法同步非静态方法1234567891011121314151617181920212223242526272829public class Synchronized &#123; private static int count; private synchronized void add1() &#123; count++; System.out.println(count); &#125; public static void main(String[] args) throws InterruptedException &#123; Synchronized sync = new Synchronized(); Thread thread1 = new Thread(() -&gt; &#123; for (int i = 0; i&lt; 10000; i++) &#123; sync.add1(); &#125; &#125;); Thread thread2 = new Thread(() -&gt; &#123; for (int i = 0; i&lt; 10000; i++) &#123; sync.add1(); &#125; &#125;); thread1.start(); thread2.start(); Thread.sleep(1000); System.out.println(count); &#125;&#125; 结果符合预期：synchronized作用于非静态方法，锁定的是实例对象，如上所示锁的是sync对象，因此线程能够正确的运行，count的结果总会是20000。 1234567891011121314151617181920212223242526272829public class Synchronized &#123; private static int count; private synchronized void add1() &#123; count++; System.out.println(count); &#125; public static void main(String[] args) throws InterruptedException &#123; Synchronized sync = new Synchronized(); Synchronized sync1 = new Synchronized(); Thread thread1 = new Thread(() -&gt; &#123; for (int i = 0; i&lt; 10000; i++) &#123; sync.add1(); &#125; &#125;); Thread thread2 = new Thread(() -&gt; &#123; for (int i = 0; i&lt; 10000; i++) &#123; sync1.add1(); &#125; &#125;); thread1.start(); thread2.start(); Thread.sleep(1000); System.out.println(count); &#125;&#125; 结果不符合预期：如上所示，作用于非静态方法，锁的是实例化对象，因此当sync和sync1同时运行时，还是会出现线程安全问题，因为锁的是两个不同的实例化对象。 同步静态方法12345678910111213141516171819202122232425262728293031323334public class Synchronized &#123; private static int count; private static synchronized void add1() &#123; count++; System.out.println(count); &#125; private static synchronized void add11() &#123; count++; System.out.println(count); &#125; public static void main(String[] args) throws InterruptedException &#123; Synchronized sync = new Synchronized(); Synchronized sync1 = new Synchronized(); Thread thread1 = new Thread(() -&gt; &#123; for (int i = 0; i&lt; 10000; i++) &#123; Synchronized.add1(); &#125; &#125;); Thread thread2 = new Thread(() -&gt; &#123; for (int i = 0; i&lt; 10000; i++) &#123; Synchronized.add11(); &#125; &#125;); thread1.start(); thread2.start(); Thread.sleep(1000); System.out.println(count); &#125;&#125; 结果符合预期：锁静态方法时，锁的是类对象。因此在不同的线程中调用add1和add11依然会得到正确的结果。 同步代码块锁当前实例对象1234567891011121314151617181920212223242526272829303132333435public class Synchronized &#123; private static int count; private void add1() &#123; synchronized (this) &#123; count++; System.out.println(count); &#125; &#125; private static synchronized void add11() &#123; count++; System.out.println(count); &#125; public static void main(String[] args) throws InterruptedException &#123; Synchronized sync = new Synchronized(); Synchronized sync1 = new Synchronized(); Thread thread1 = new Thread(() -&gt; &#123; for (int i = 0; i&lt; 10000; i++) &#123; sync.add1(); &#125; &#125;); Thread thread2 = new Thread(() -&gt; &#123; for (int i = 0; i&lt; 10000; i++) &#123; sync1.add1(); &#125; &#125;); thread1.start(); thread2.start(); Thread.sleep(1000); System.out.println(count); &#125;&#125; 结果不符合预期：当synchronized同步方法块时，锁的是实例对象时，如上示例在不同的实例中调用此方法还是会出现线程安全问题。 锁其它实例对象1234567891011121314151617181920212223242526272829303132333435363738public class Synchronized &#123; private static int count; public String lock = new String(); private void add1() &#123; synchronized (lock) &#123; count++; System.out.println(count); &#125; &#125; private static synchronized void add11() &#123; count++; System.out.println(count); &#125; public static void main(String[] args) throws InterruptedException &#123; Synchronized sync = new Synchronized(); Synchronized sync1 = new Synchronized(); Thread thread1 = new Thread(() -&gt; &#123; for (int i = 0; i&lt; 10000; i++) &#123; sync.add1(); &#125; &#125;); Thread thread2 = new Thread(() -&gt; &#123; for (int i = 0; i&lt; 10000; i++) &#123; sync1.add1(); &#125; &#125;); thread1.start(); thread2.start(); Thread.sleep(1000); System.out.println(count); System.out.println(sync.lock == sync1.lock); &#125;&#125; 结果不符合预期：当synchronized同步方法块时，锁的是其它实例对象时，如上示例在不同的实例中调用此方法还是会出现线程安全问题。 1234567891011121314151617181920212223242526272829303132333435363738public class Synchronized &#123; private static int count; public String lock = ""; private void add1() &#123; synchronized (lock) &#123; count++; System.out.println(count); &#125; &#125; private static synchronized void add11() &#123; count++; System.out.println(count); &#125; public static void main(String[] args) throws InterruptedException &#123; Synchronized sync = new Synchronized(); Synchronized sync1 = new Synchronized(); Thread thread1 = new Thread(() -&gt; &#123; for (int i = 0; i&lt; 10000; i++) &#123; sync.add1(); &#125; &#125;); Thread thread2 = new Thread(() -&gt; &#123; for (int i = 0; i&lt; 10000; i++) &#123; sync1.add1(); &#125; &#125;); thread1.start(); thread2.start(); Thread.sleep(1000); System.out.println(count); System.out.println(sync.lock == sync1.lock); &#125;&#125; 结果符合预期：当synchronized同步方法块时，锁的虽然是其它实例对象时，但已上实例中，因为String = “” 是存放在常量池中的，实际上锁的还是相同的对象，因此是线程安全的 锁类对象1234567891011121314151617181920212223242526272829303132333435public class Synchronized &#123; private static int count; private void add1() &#123; synchronized (Synchronized.class) &#123; count++; System.out.println(count); &#125; &#125; private static synchronized void add11() &#123; count++; System.out.println(count); &#125; public static void main(String[] args) throws InterruptedException &#123; Synchronized sync = new Synchronized(); Synchronized sync1 = new Synchronized(); Thread thread1 = new Thread(() -&gt; &#123; for (int i = 0; i&lt; 10000; i++) &#123; sync.add1(); &#125; &#125;); Thread thread2 = new Thread(() -&gt; &#123; for (int i = 0; i&lt; 10000; i++) &#123; sync1.add1(); &#125; &#125;); thread1.start(); thread2.start(); Thread.sleep(1000); System.out.println(count); &#125;&#125; 结果符合预期：当synchronized同步方法块时，锁的是类对象时，如上示例在不同的实例中调用此方法是线程安全的。 锁机制123456789public class Synchronized &#123; private static int count; public static void main(String[] args) throws InterruptedException &#123; synchronized (Synchronized.class) &#123; count++; &#125; &#125;&#125; 使用javap -v Synchronized.class反编译class文件。 可以看到synchronized实际上是通过monitorenter和monitorexit来实现锁机制的。同一时刻，只能有一个线程进入监视区。从而保证线程的同步。 正常情况下在指令4进入监视区，指令14退出监视区然后指令15直接跳到指令23 return 但是在异常情况下异常都会跳转到指令18，依次执行到指令20monitorexit释放锁，防止出现异常时未释放的情况。这其实也是synchronized的优点：无论代码执行情况如何，都不会忘记主动释放锁。 想了解Monitors更多的原理可以点击查看 锁升级因为monitor依赖操作系统的Mutex lock实现，是一个比较重的操作，需要切换系统至内核态，开销非常大。因此在jdk1.6引入了偏向锁和轻量级锁。synchronized有四种状态：无锁 -&gt; 偏向锁 -&gt; 轻量级锁 -&gt; 重量级锁。 无锁没有对资源进行锁定，所有线程都能访问和修改。但同时只有一个线程能修改成功 偏向锁在锁竞争不强烈的情况下，通常一个线程会多次获取同一个锁，为了减少获取锁的代价 引入了偏向锁，会在java对象头中记录获取锁的线程的threadID。 当线程发现对象头的threadID存在时。判断与当前线程是否是同一线程。 如果是则不需要再次加、解锁。 如果不是，则判断threadID是否存活。不存活：设置为无锁状态，其他线程竞争设置偏向锁。存活：查找threadID堆栈信息判断是否需要继续持有锁。需要持有则升级threadID线程的锁为轻量级锁。不需要持有则撤销锁，设置为无锁状态等待其它线程竞争。 因为偏向锁的撤销操作还是比较重的，导致进入安全点，因此在竞争比较激烈时，会影响性能，可以使用-XX:-UseBiasedLocking=false禁用偏向锁。 轻量级锁当偏向锁升级为轻量级锁时，其它线程尝试通过CAS方式设置对象头来获取锁。 会先在当前线程的栈帧中设置Lock Record，用于存储当前对象头中的mark word的拷贝。 复制mark word的内容到lock record，并尝试使用cas将mark word的指针指向lock record 如果替换成功，则获取偏向锁 替换不成功，则会自旋重试一定次数。 自旋一定次数或有新的线程来竞争锁时，轻量级锁膨胀为重量级锁。 CASCAS即compare and swap（比较并替换）。是一种乐观锁机制。通常有三个值 V：内存中的实际值 A：旧的预期值 B：要修改的新值即V与A相等时，则替换V为B。即内存中的实际值与我们的预期值相等时，则替换为新值。 CAS可能遇到ABA问题，即内存中的值为A，变为B后，又变为了A，此时A为新值，不应该替换。可以采取：A-1，B-2，A-3的方式来避免这个问题 重量级锁自旋是消耗CPU的，因此在自旋一段时间，或者一个线程在自旋时，又有新的线程来竞争锁，则轻量级锁会膨胀为重量级锁。重量级锁，通过monitor实现，monitor底层实际是依赖操作系统的mutex lock（互斥锁）实现。需要从用户态，切换为内核态，成本比较高 总结本文我们一起学习了 synchronized的几种用法：同步方法、同步代码块。实际上是同步类或同步实例对象。 锁升级：无锁、偏向锁、轻量级锁、重量级锁以及其膨胀过程。 synchronized作为内置锁，虽然帮我们解决了线程安全问题，但是带来了性能的损失，因此一定不能滥用。使用时请注意同步块的作用范围。通常，作用范围越小，对性能的影响也就越小（注意权衡获取、释放锁的成本，不能为了缩小作用范围，而频繁的获取、释放）。]]></content>
      <tags>
        <tag>Java</tag>
        <tag>线程安全</tag>
        <tag>并发编程</tag>
        <tag>锁机制</tag>
        <tag>同步</tag>
        <tag>synchronized</tag>
        <tag>锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【从入门到放弃-Java】并发编程-线程安全]]></title>
    <url>%2F3567c420.html</url>
    <content type="text"><![CDATA[概述并发编程，即多条线程在同一时间段内“同时”运行。 在多处理器系统已经普及的今天，多线程能发挥出其优势，如：一个8核cpu的服务器，如果只使用单线程的话，将有7个处理器被闲置，只能发挥出服务器八分之一的能力（忽略其它资源占用情况）。同时，使用多线程，可以简化我们对复杂任务的处理逻辑，降低业务模型的复杂程度。 因此并发编程对于提高服务器的资源利用率、提高系统吞吐量、降低编码难度等方面起着至关重要的作用。 以上是并发编程的优点，但是它同样引入了一个很重要的问题：线程安全。 什么是线程安全问题线程在并发执行时，因为cpu的调度等原因，线程会交替执行。如下图例子所示1234567891011121314151617181920212223public class SelfIncremental &#123; private static int count; public static void main(String[] args) &#123; Thread thread1 = new Thread(() -&gt; &#123; for (int i = 0; i&lt; 10000; i++) &#123; count++; System.out.println(count); &#125; &#125;); Thread thread2 = new Thread(() -&gt; &#123; for (int i = 0; i&lt; 10000; i++) &#123; count++; System.out.println(count); &#125; &#125;); thread1.start(); thread2.start(); &#125;&#125; 执行完毕后count的值并不是每次都能等于20000，会出现小于20000的情况，原因是thread1和thread2可能会交替执行。 如图所示： t1时刻: thread1 读取到count=100 t2时刻: thread2 读取到count=100 t3时刻: thread1 对count+1 t4时刻: thread2 对count+1 t5时刻: thread1 将101写入count t5时刻: thread2 将101写入count 因为count++ 不是一个原子操作，实际上会执行三步： 1、获取count的值 2、将count加1 3、将计算结果写入count 因此在并发执行时，两个线程同时读，可能会读取到相同的值，对相同的值加一，导致结果不符合预期，这种情况就是线程不安全。 线程安全：当多个线程访问某个类时，不管运行时环境采用何种调度方式或者这些线程将如何交替执行，并且调用时不需要采用额外的同步操作，这个类都能表现出正确的行为，那么就称这个类是线程安全的。 引发原因引发线程安全性问题的原因主要是共享内存可以被多个线程读写，因为读取和修改时机存在不确定性，导致有线程读到了过期数据，并在脏数据的基础上处理后写回共享内存，产生了错误的结果。 竟态条件在并发编程中，因为不恰当的执行时序而出现不正确的结果的情况被称为竟态条件。 常见的静态条件类型： 先检查后执行：首先观察到某个条件为真。根据这个观察结果采用相应的动作，但实际上在你观察到这个结果和采用相应动作之间，观察的结果可能发生改变变得无效，导致后续的所有操作都变得不可预期。（比如延迟初始化） 读取-修改-写入：基于对象之前的状态来定义对象状态的转换。但在读取到结果和修改之间，对象可能已被更改。这样就会基于错误的数据修改得出错误的结果并被写入。（比如递增操作） 发布与逸出发布：使对象能够在当前作用域之外的代码中使用。如将该对象的引用保存到其它代码可以访问的地方、在一个非私有的方法中返回该引用，将引用传递到其它类的方法中。如：12345public static Student student；public void init() &#123; student = new Student;&#125; 这里 student对象就被发布了。 逸出：当不该被发布的对象被发布了，就称为逸出。如12345private String name = "xxx";public String getString() &#123; return name;&#125; 这里name原为private类型但是却被getString方法发布了，就可以被视为逸出。 如何避免线程封闭线程封闭的对象只能由一个线程拥有，对象被封闭在该线程中，并且只有这个对象能修改。 线程封闭即不共享数据，仅在单线程内访问数据，这是实现线程安全最简单的方式之一。实现线程封闭可以通过： Ad-hoc线程封闭：即维护线程封闭性的职责完全由成熟实现承担。 栈封闭：通过局部变量才能访问对象，该局部变量被保存在执行线程的栈中，其他线程无法访问。 ThreadLocal类：将共享的全局变量转换为ThreadLocal对象，当线程终止后，这些值会被垃圾回收。 只读共享在没有额外同步的情况下，共享的对象可以由多个线程并发访问，但是任何线程都不能修改。共享的对象包括不可变对象和事实不可变对象。 不可变对象：如果某个对象在被创建后就不能修改，那么这个对象就是不可变对象。不可变对象一定是线程安全的。 线程安全共享线程安全的对象在其内部实现同步，因此多线程可以通过对象的公有接口来进行访问而不需要自己做同步。 保护对象被保护的对象只能通过持有特定的锁来访问。即通过加锁机制，确保对象的可见性及原子性。 内置锁：即通过synchronized关键字同步代码块。线程在进入同步代码块之前会自动获得锁，并在退出同步代码块时自动释放锁。内置锁是一种互斥锁。 重入锁：当线程视图获取一个已经持有的锁时，就会给锁的计数器加一，释放锁时计数器会减一。当计数器为0时，释放锁 volatile：访问volatile变量时，不会加锁，也不会阻塞线程执行。他只确保变量的可见性，是一种比synchronized更轻量级的同步机制。 总结本文主要是记录了学习《Java并发编程实站》前几章中，并发编程相关的一些概念。简单介绍了线程安全、锁机制等，接下来 我们会深入JUC源码，来深刻学习并发编程相关知识。 备注：本文主要源自对《Java并发编程实战》的学习笔记。]]></content>
      <tags>
        <tag>Java</tag>
        <tag>线程安全</tag>
        <tag>并发编程</tag>
        <tag>锁机制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【从入门到放弃-Java】并发编程-NIO-Selector]]></title>
    <url>%2F2cdc5430.html</url>
    <content type="text"><![CDATA[前言前两篇【从入门到放弃-Java】并发编程-NIO-Channel和【从入门到放弃-Java】并发编程-NIO-Buffer中我们学习了NIO中两个重要的概念Channel和Buffer。今天我们来看下另一个重要的内容 Selector 简介Selector是多路复用器，会不断轮询已经注册了的Channel。当有注册的channel产生连接、读、写等事件时，就会被Selector发现，从而可以进行相关后续操作。 Selector的好处是，可以通过一个线程来管理多个通道，减少了创建线程的资源占用及线程切换带来的消耗 SelectorSelectableChannel可以通过SelectionKey(记录channel和selector的注册关系)注册到Selector上。Selector维护了三个SelectionKey集合： key set：存放了Selector上已经注册了的Channel的key。可以通过keys()方法获取。 selected-key set：当之前注册感兴趣的事件到达时，set中的keys会被更新或添加，set中维护了当前至少有一个可以操作的事件的channel key的集合。是key set的子集。可以使用selectedKeys()获取。 cancelled-key：存放已经调用cancel方法取消，等待下次操作时会调用deregister取消注册的channel，调用deregister后，所有的set中都没有这个channel的key了。 open12345678910111213141516/** * Opens a selector. * * &lt;p&gt; The new selector is created by invoking the &#123;@link * java.nio.channels.spi.SelectorProvider#openSelector openSelector&#125; method * of the system-wide default &#123;@link * java.nio.channels.spi.SelectorProvider&#125; object. &lt;/p&gt; * * @return A new selector * * @throws IOException * If an I/O error occurs */public static Selector open() throws IOException &#123; return SelectorProvider.provider().openSelector();&#125; 开启selector，具体的实现会根据操作系统类型创建不同的实现类，如macOS下实际上是new了一个KQueueSelectorProvider实例，低层基于操作系统的kqueue实现。 register123456789101112131415161718192021222324252627282930protected final SelectionKey register(AbstractSelectableChannel ch, int ops, Object attachment)&#123; if (!(ch instanceof SelChImpl)) throw new IllegalSelectorException(); //新建一个SelectionKey，记录channel与selector之间的注册关系 SelectionKeyImpl k = new SelectionKeyImpl((SelChImpl)ch, this); k.attach(attachment); //前置操作，这里主要是判断下selector是否还处于open状态 // register (if needed) before adding to key set implRegister(k); // 添加selectionKey至key set // add to the selector's key set, removing it immediately if the selector // is closed. The key is not in the channel's key set at this point but // it may be observed by a thread iterating over the selector's key set. keys.add(k); try &#123; // 更新注册的事件码 k.interestOps(ops); &#125; catch (ClosedSelectorException e) &#123; assert ch.keyFor(this) == null; keys.remove(k); k.cancel(); throw e; &#125; return k;&#125; 注册selector和channel之间的事件关系。 select12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576// timeout超时@Overridepublic final int select(long timeout) throws IOException &#123; if (timeout &lt; 0) throw new IllegalArgumentException("Negative timeout"); return lockAndDoSelect(null, (timeout == 0) ? -1 : timeout);&#125;@Overridepublic final int select() throws IOException &#123; return lockAndDoSelect(null, -1);&#125;// 不阻塞@Overridepublic final int selectNow() throws IOException &#123; return lockAndDoSelect(null, 0);&#125;private int lockAndDoSelect(Consumer&lt;SelectionKey&gt; action, long timeout) throws IOException&#123; synchronized (this) &#123; ensureOpen(); if (inSelect) throw new IllegalStateException("select in progress"); inSelect = true; try &#123; synchronized (publicSelectedKeys) &#123; return doSelect(action, timeout); &#125; &#125; finally &#123; inSelect = false; &#125; &#125;&#125;protected int doSelect(Consumer&lt;SelectionKey&gt; action, long timeout) throws IOException&#123; assert Thread.holdsLock(this); // 如果timeout = 0时，不阻塞 long to = Math.min(timeout, Integer.MAX_VALUE); // max kqueue timeout boolean blocking = (to != 0); boolean timedPoll = (to &gt; 0); int numEntries; processUpdateQueue(); processDeregisterQueue(); try &#123; // 设置interrupt 可以处理中断信号 防止线程一直阻塞 begin(blocking); // 轮询的监听，直到有注册的事件发生或超时。 do &#123; long startTime = timedPoll ? System.nanoTime() : 0; numEntries = KQueue.poll(kqfd, pollArrayAddress, MAX_KEVENTS, to); if (numEntries == IOStatus.INTERRUPTED &amp;&amp; timedPoll) &#123; // timed poll interrupted so need to adjust timeout long adjust = System.nanoTime() - startTime; to -= TimeUnit.MILLISECONDS.convert(adjust, TimeUnit.NANOSECONDS); if (to &lt;= 0) &#123; // timeout expired so no retry numEntries = 0; &#125; &#125; &#125; while (numEntries == IOStatus.INTERRUPTED); assert IOStatus.check(numEntries); &#125; finally &#123; end(blocking); &#125; processDeregisterQueue(); return processEvents(numEntries, action);&#125; selectedKeys1234public final Set&lt;SelectionKey&gt; selectedKeys() &#123; ensureOpen(); return publicSelectedKeys;&#125; 获取被事件唤醒的key注意：当被遍历处理selectedKeys时，key被处理完需要手动remove掉，防止下次被重复消费，selectedKeys不会帮你删除已处理过的key。 close123456789101112131415161718192021222324252627282930public final void close() throws IOException &#123; boolean open = selectorOpen.getAndSet(false); if (!open) return; implCloseSelector();&#125;public final void implCloseSelector() throws IOException &#123; //通知处于阻塞的select方法立即返回 wakeup(); synchronized (this) &#123; implClose(); synchronized (publicSelectedKeys) &#123; // 遍历所有的SelectionKey，取消注册 // Deregister channels Iterator&lt;SelectionKey&gt; i = keys.iterator(); while (i.hasNext()) &#123; SelectionKeyImpl ski = (SelectionKeyImpl)i.next(); deregister(ski); SelectableChannel selch = ski.channel(); if (!selch.isOpen() &amp;&amp; !selch.isRegistered()) ((SelChImpl)selch).kill(); selectedKeys.remove(ski); i.remove(); &#125; assert selectedKeys.isEmpty() &amp;&amp; keys.isEmpty(); &#125; &#125;&#125; SelectionKeySelectionKey在channel register时创建。用来记录channel和selector之间的注册事件关系。事件主要有： OP_READ OP_WRITE OP_CONNECT OP_ACCEPT 每个SelectionKey有两个由整数表示的操作集合，用来标识channel支持的操作类型。 interest set：是在创建SelectionKey时定义的，当集合中的操作发生时，将会把channel置为ready状态ready set：检测到selector中已经就绪的操作类型集合 channel123public SelectableChannel channel() &#123; return (SelectableChannel)channel;&#125; 获取SelectionKey中的channel selector123public Selector selector() &#123; return selector;&#125; 获取SelectionKey中的selector isReadable123public final boolean isReadable() &#123; return (readyOps() &amp; OP_READ) != 0;&#125; 根据readyOps(readySet)判断channel是否是可读状态 isWritable123public final boolean isWritable() &#123; return (readyOps() &amp; OP_WRITE) != 0;&#125; 根据readyOps(readySet)判断channel是否是可写状态 isConnectable123public final boolean isConnectable() &#123; return (readyOps() &amp; OP_CONNECT) != 0;&#125; 根据readyOps(readySet)判断channel是否是connect状态，通常是客户端使用，判断连接是否建立 isReadable123public final boolean isAcceptable() &#123; return (readyOps() &amp; OP_ACCEPT) != 0;&#125; 根据readyOps(readySet)判断channel是否是accept状态，通常是服务端使用，判断是否有客户端请求建立连接 总结通过使用selector，可以使用一个线程来管理多个连接。需要注意的一点是，通常读、写操作都是比较耗时的，为了提高服务端的性能应该把Selector::select和read、write的具体处理逻辑在不同的线程中处理。即：使用一个线程来进行select，只做分发。在获取到就绪的SelectionKey后，通过线程池在不同的线程中处理读写操作。 通过学习完NIO相关的知识，我们可以很清楚的回答下面这个问题 问：基于BIO实现的server端，当建立100个连接时，需要多少个线程？基于NIO实现的呢？ 答：基于BIO实现的server端，通常需要由一个线程accept，并为每个新建立的连接创建一个线程去处理IO操作，因此需要 1个accept线程+100个IO线程基于NIO实现的server端，使用Selector多路复用机制，由一个线程进行select，为了提高并发可以使用线程池来处理IO操作，通常为了发挥CPU的性能会创建(cpu核数 x 2)个线程来处理IO操作。因此需要 1个select线程 + cpu核数 x 2 个IO线程]]></content>
      <tags>
        <tag>Java</tag>
        <tag>并发编程</tag>
        <tag>NIO</tag>
        <tag>buffer</tag>
        <tag>ByteBuffer</tag>
        <tag>socket</tag>
        <tag>Channel</tag>
        <tag>Selector</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【从入门到放弃-Java】并发编程-NIO-Buffer]]></title>
    <url>%2Fd5a7a7f5.html</url>
    <content type="text"><![CDATA[前言上篇【从入门到放弃-Java】并发编程-NIO-Channel中我们学习到channel是双向通道，数据通过channel在实体（文件、socket）和缓冲区（buffer）中可以双向传输。 本文我们就来学习下buffer 简介buffer即缓冲区，实际上是一块内存，可以用来写入、读取数据。是一个线性的、大小有限的、顺序承载基础数据类型的内存块。 buffer有三个重要的属性： capacity：缓冲池大小，是不可变的。当buffer写满时，需要先清空才能继续写入。 limit：是buffer中不可以被读或者写的第一个元素的位置，limit的大小永远不会超过capacity（在写模式下，limit等于capacity） position：是buffer中可以被读或者写的第一个元素的位置，position的大小永远不会超过limit 除了boolean外，每一个基础数据类型都有对应的buffer。如：ByteBuffer、CharBuffer、LongBuffer等 buffer不是线程安全的，如果要在多线程中使用 需要加锁控制 接下来以ByteBuffer为例开始学习。 ByteBufferallocateDirect1234public static ByteBuffer allocateDirect(int capacity) &#123; //会创建一个容量大小为capacity的DirectByteBuffer（ByteBuffer的子类） return new DirectByteBuffer(capacity);&#125; allocate123456public static ByteBuffer allocate(int capacity) &#123; if (capacity &lt; 0) throw createCapacityException(capacity); //会创建一个容量大小为capacity的HeapByteBuffer（ByteBuffer的子类） return new HeapByteBuffer(capacity, capacity);&#125; HeapByteBuffer和DirectByteBuffer的区别： DirectByteBuffer是直接调用native方法在本机os::malloc()创建堆外内存；HeapByteBuffer是直接在jvm的堆中分配内存。 当buffer中的数据和磁盘、网络等的交互都在操作系统的内核中发生时，使用DirectByteBuffer能避免从内核态-&gt;用户态-&gt;内核态的切换开销，所有的处理都在内核中进行，性能会比较好 当频繁创建操作数据量比较小的buffer时，使用HeapByteBuffer在jvm堆中分配内存能抵消掉使用DirectByteBuffer带来的好处。 wrap12345678910111213public static ByteBuffer wrap(byte[] array, int offset, int length)&#123; try &#123; return new HeapByteBuffer(array, offset, length); &#125; catch (IllegalArgumentException x) &#123; throw new IndexOutOfBoundsException(); &#125;&#125;public static ByteBuffer wrap(byte[] array) &#123; return wrap(array, 0, array.length); &#125; 将byte数组包装成一个ByteBuffer 读数据 使用get方法从Buffer中读取数据 从Buffer中读取数据到Channel即：Channel::write() (从buffer中读取数据写入到资源中，所以是write) 写数据 使用put方法直接设置Buffer中的数据 从Channel中读取数据到Buffer即：Channel::read() (从资源中读取数据写入到buffer中，所以是read) position12345678910111213//获取buffer中当前position的位置public final int position() &#123; return position;&#125;//设置buffer的position为newPosition，注意newPosition要大于0且小于limit，如果remark大于newPosition则设置为-1public Buffer position(int newPosition) &#123; if (newPosition &gt; limit | newPosition &lt; 0) throw createPositionException(newPosition); position = newPosition; if (mark &gt; position) mark = -1; return this;&#125; limit1234567891011121314//获取buffer中当前limit的位置public final int limit() &#123; return limit;&#125;//设置buffer的limit为newLimit，注意newLimit要大于0且小于capacity。如果position大于newLimit这设置为newLimit，如果remark大于newLimit则设置为-1public Buffer limit(int newLimit) &#123; if (newLimit &gt; capacity | newLimit &lt; 0) throw createLimitException(newLimit); limit = newLimit; if (position &gt; limit) position = limit; if (mark &gt; limit) mark = -1; return this;&#125; mark12345public Buffer mark() &#123; //标记mark为当前position mark = position; return this;&#125; 将当前位置做标记，在使用reset方法时，可以回到当前mark的位置 reset12345678public Buffer reset() &#123; int m = mark; if (m &lt; 0) throw new InvalidMarkException(); //设置position为当前mark position = m; return this;&#125; 回到之前设置mark的位置 clear123456789public Buffer clear() &#123; //设置position为0 position = 0; //limit设置为capacity大小 limit = capacity; //mark设置为-1（初始化） mark = -1; return this;&#125; 读取完数据后调用clear，即将buffer逻辑上清空了，可以从0开始写入数据 flip123456789public Buffer flip() &#123; //limit设置为当前位置 limit = position; //position设置为0 position = 0; //mark设置为-1（初始化） mark = -1; return this;&#125; 将buffer从写模式设置为读模式，limit设置为当前position的位置，即只能读取limit大小的数据 rewind12345public Buffer rewind() &#123; position = 0; mark = -1; return this;&#125; 将position设置为0，即从头开始读取 remaining123public final int remaining() &#123; return limit - position;&#125; 返回buffer中还有多少byte是未读的 hasRemaining123public final boolean hasRemaining() &#123; return position &lt; limit;&#125; 是否已读完 compact1234567public ByteBuffer compact() &#123; System.arraycopy(hb, ix(position()), hb, ix(0), remaining()); position(remaining()); limit(capacity()); discardMark(); return this;&#125; 将position和limit直接的数据copy到byteBuffer的起始处，将已读数据清空，并将新的position设置为当前未读数据的末尾。这样能避免clear方法会将未读数据也清空的问题 slice123456789101112131415161718192021public ByteBuffer slice() &#123; return new HeapByteBufferR(hb, -1, 0, this.remaining(), this.remaining(), this.position() + offset);&#125;ByteBuffer slice(int pos, int lim) &#123; assert (pos &gt;= 0); assert (pos &lt;= lim); int rem = lim - pos; return new HeapByteBufferR(hb, -1, 0, rem, rem, pos + offset);&#125; 新创建一个ByteBuffer，将缓存区分片，设置一个子缓冲区，实际上内存还是共享的，数据发生改变，两个缓冲区读取的数据都会是改变后的。 总结Buffer最重要的三个属性：position、limit、capacity。牢记这三个属性的含义及读写切换时，设置值是如何变化的，Buffer的核心知识点就掌握了。]]></content>
      <tags>
        <tag>Java</tag>
        <tag>并发编程</tag>
        <tag>NIO</tag>
        <tag>buffer</tag>
        <tag>ByteBuffer</tag>
        <tag>socket</tag>
        <tag>Channel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【从入门到放弃-Java】并发编程-NIO-Channel]]></title>
    <url>%2F3c79e6b3.html</url>
    <content type="text"><![CDATA[前言上篇【从入门到放弃-Java】并发编程-NIO使用简单介绍了nio的基础使用，本篇将深入源码分析nio中channel的实现。 简介channel即通道，可以用来读、写数据，它是全双工的可以同时用来读写操作。这也是它与stream流的最大区别。 channel需要与buffer配合使用，channel通道的一端是buffer，一端是数据源实体，如文件、socket等。在nio中，通过channel的不同实现来处理 不同实体与数据buffer中的数据传输。 channel接口：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package java.nio.channels;import java.io.IOException;import java.io.Closeable;/** * A nexus for I/O operations. * * &lt;p&gt; A channel represents an open connection to an entity such as a hardware * device, a file, a network socket, or a program component that is capable of * performing one or more distinct I/O operations, for example reading or * writing. * * &lt;p&gt; A channel is either open or closed. A channel is open upon creation, * and once closed it remains closed. Once a channel is closed, any attempt to * invoke an I/O operation upon it will cause a &#123;@link ClosedChannelException&#125; * to be thrown. Whether or not a channel is open may be tested by invoking * its &#123;@link #isOpen isOpen&#125; method. * * &lt;p&gt; Channels are, in general, intended to be safe for multithreaded access * as described in the specifications of the interfaces and classes that extend * and implement this interface. * * * @author Mark Reinhold * @author JSR-51 Expert Group * @since 1.4 */public interface Channel extends Closeable &#123; /** * Tells whether or not this channel is open. * * @return &lt;tt&gt;true&lt;/tt&gt; if, and only if, this channel is open */ public boolean isOpen(); /** * Closes this channel. * * &lt;p&gt; After a channel is closed, any further attempt to invoke I/O * operations upon it will cause a &#123;@link ClosedChannelException&#125; to be * thrown. * * &lt;p&gt; If this channel is already closed then invoking this method has no * effect. * * &lt;p&gt; This method may be invoked at any time. If some other thread has * already invoked it, however, then another invocation will block until * the first invocation is complete, after which it will return without * effect. &lt;/p&gt; * * @throws IOException If an I/O error occurs */ public void close() throws IOException;&#125; 常见的channel实现有： FileChannel：文件读写数据通道 SocketChannel：TCP读写网络数据通道 ServerSocketChannel：服务端网络数据读写通道，可以监听TCP连接。对每一个新进来的连接都会创建一个SocketChannel。 DatagramChannel：UDP读写网络数据通道 FileChannel FileChannel是一个抽象类，它继承了AbstractInterruptibleChannel类，并实现了 SeekableByteChannel, GatheringByteChannel, ScatteringByteChannel接口。具体的实现类主要是sun.nio.ch.FileChannelImpl。下面详细分析下FileChannelImpl中每个方法的具体实现。 open1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374private FileChannelImpl(FileDescriptor var1, String var2, boolean var3, boolean var4, boolean var5, Object var6) &#123; //主要记载操作系统维护的文件描述符 this.fd = var1; //是否可读 this.readable = var3; //是否可写 this.writable = var4; //是否以追加的方式打开 this.append = var5; this.parent = var6; this.path = var2; //底层使用native的read和write来处理文件的 this.nd = new FileDispatcherImpl(var5);&#125;//FileInputStream::getChannel 调用 FileChannelImpl.open(fd, path, true, false, this) 获取只读channelpublic static FileChannel open(FileDescriptor var0, String var1, boolean var2, boolean var3, Object var4) &#123; return new FileChannelImpl(var0, var1, var2, var3, false, var4);&#125;//FileOutputStream::getChannel 调用 FileChannelImpl.open(fd, path, false, true, append, this) 获取只写channelpublic static FileChannel open(FileDescriptor var0, String var1, boolean var2, boolean var3, boolean var4, Object var5) &#123; return new FileChannelImpl(var0, var1, var2, var3, var4, var5);&#125;private FileChannelImpl(FileDescriptor fd, String path, boolean readable, boolean writable, boolean direct, Object parent)&#123; this.fd = fd; //是否可读 this.readable = readable; //是否可写 this.writable = writable; //对于从流创建的channel，在结束时要做不同的清理动作，（openJDK中才有，sun的jdk中没有） this.parent = parent; //源文件的path this.path = path; //是否使用DirectIO this.direct = direct; this.nd = new FileDispatcherImpl(); if (direct) &#123; assert path != null; this.alignment = nd.setDirectIO(fd, path); &#125; else &#123; this.alignment = -1; &#125; //当parent不存在时，则注册一个cleaner，否则交由parent做清理动作。 // Register a cleaning action if and only if there is no parent // as the parent will take care of closing the file descriptor. // FileChannel is used by the LambdaMetaFactory so a lambda cannot // be used here hence we use a nested class instead. this.closer = parent != null ? null : CleanerFactory.cleaner().register(this, new Closer(fd));&#125;// Used by FileInputStream.getChannel(), FileOutputStream.getChannel// and RandomAccessFile.getChannel()public static FileChannel open(FileDescriptor fd, String path, boolean readable, boolean writable, boolean direct, Object parent)&#123; return new FileChannelImpl(fd, path, readable, writable, direct, parent);&#125; open方法主要是返回一个新new的FileChannelImpl对象，初始化时设置fileDescriptor、readable、writable、append、parent、path等属性，看变量名很容易理解，在此不赘述变量含义。 read123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081//实现自SeekableByteChannel接口的方法，将文件中的内容读取到给定的byteBuffer中public int read(ByteBuffer dst) throws IOException &#123; //保证读写时，channel处于开启状态 ensureOpen(); //判断是否可读 if (!readable) throw new NonReadableChannelException(); synchronized (positionLock) &#123; if (direct) Util.checkChannelPositionAligned(position(), alignment); int n = 0; int ti = -1; try &#123; //开始阻塞，并注册为Interruptible，可以被中断 beginBlocking(); //将当前线程添加到NativeThreadSet中，并返回索引，方便后续操作。 //NativeThreadSet是一个线程安全的本地线程集合，方便管理，用来发送信号 ti = threads.add(); if (!isOpen()) return 0; do &#123; //当未被系统中断（即读取完毕）或channel未被关闭，则一直读，将内容写入到byteBuffer（dst）中 n = IOUtil.read(fd, dst, -1, direct, alignment, nd); &#125; while ((n == IOStatus.INTERRUPTED) &amp;&amp; isOpen()); return IOStatus.normalize(n); &#125; finally &#123; //把当前线程从set中移出 threads.remove(ti); //结束，释放锁 endBlocking(n &gt; 0); assert IOStatus.check(n); &#125; &#125;&#125;//实现自ScatteringByteChannel接口的方法，将文件中的内容依次读取到给定的byteBuffer数组中。public long read(ByteBuffer[] dsts, int offset, int length) throws IOException&#123; if ((offset &lt; 0) || (length &lt; 0) || (offset &gt; dsts.length - length)) throw new IndexOutOfBoundsException(); //保证读写时，channel处于开启状态 ensureOpen(); //判断是否可读 if (!readable) throw new NonReadableChannelException(); synchronized (positionLock) &#123; if (direct) Util.checkChannelPositionAligned(position(), alignment); long n = 0; int ti = -1; try &#123; //开始阻塞，并注册为Interruptible，可以被中断 beginBlocking(); //将当前线程添加到NativeThreadSet中，并返回索引，方便后续操作。 //NativeThreadSet是一个线程安全的本地线程集合，方便管理，用来发送信号 ti = threads.add(); if (!isOpen()) return 0; do &#123; //当未被系统中断（即读取完毕）或channel未被关闭，则一直读，将内容写入到byteBuffer（dst）中 n = IOUtil.read(fd, dsts, offset, length, direct, alignment, nd); &#125; while ((n == IOStatus.INTERRUPTED) &amp;&amp; isOpen()); return IOStatus.normalize(n); &#125; finally &#123; //把当前线程从set中移出 threads.remove(ti); //结束，释放锁 endBlocking(n &gt; 0); assert IOStatus.check(n); &#125; &#125;&#125; write123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778//实现自SeekableByteChannel接口的方法，将byteBuffer中的内容写入到文件中public int write(ByteBuffer src) throws IOException &#123; //保证写时，channel处于开启状态 ensureOpen(); //判断是否可写 if (!writable) throw new NonWritableChannelException(); synchronized (positionLock) &#123; if (direct) Util.checkChannelPositionAligned(position(), alignment); int n = 0; int ti = -1; try &#123; //开始阻塞，并注册为Interruptible，可以被中断 beginBlocking(); //将当前线程添加到NativeThreadSet中，并返回索引，方便后续操作。 //NativeThreadSet是一个线程安全的本地线程集合，方便管理，用来发送信号 ti = threads.add(); if (!isOpen()) return 0; do &#123; //当未被系统中断（即写入完毕）或channel未被关闭，则一直写，将内容写入到文件中 n = IOUtil.write(fd, src, -1, direct, alignment, nd); &#125; while ((n == IOStatus.INTERRUPTED) &amp;&amp; isOpen()); return IOStatus.normalize(n); &#125; finally &#123; //把当前线程从set中移出 threads.remove(ti); //结束，释放锁 assert IOStatus.check(n); &#125; &#125;&#125;//实现自GatheringByteChannel接口的方法，将byteBuffer数组中的内容依次写入到文件中public long write(ByteBuffer[] srcs, int offset, int length) throws IOException&#123; if ((offset &lt; 0) || (length &lt; 0) || (offset &gt; srcs.length - length)) throw new IndexOutOfBoundsException(); //保证写时，channel处于开启状态 ensureOpen(); //判断是否可写 if (!writable) throw new NonWritableChannelException(); synchronized (positionLock) &#123; if (direct) Util.checkChannelPositionAligned(position(), alignment); long n = 0; int ti = -1; try &#123; //开始阻塞，并注册为Interruptible，可以被中断 beginBlocking(); //将当前线程添加到NativeThreadSet中，并返回索引，方便后续操作。 //NativeThreadSet是一个线程安全的本地线程集合，方便管理，用来发送信号 ti = threads.add(); if (!isOpen()) return 0; do &#123; //当未被系统中断（即写入完毕）或channel未被关闭，则一直写，将内容写入到文件中 n = IOUtil.write(fd, srcs, offset, length, direct, alignment, nd); &#125; while ((n == IOStatus.INTERRUPTED) &amp;&amp; isOpen()); return IOStatus.normalize(n); &#125; finally &#123; //把当前线程从set中移出 threads.remove(ti); //结束，释放锁 assert IOStatus.check(n); &#125; &#125;&#125; position123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051//实现自SeekableByteChannel接口的方法，获取当前channel的positionpublic long position() throws IOException &#123; ensureOpen(); synchronized (positionLock) &#123; long p = -1; int ti = -1; try &#123; beginBlocking(); ti = threads.add(); if (!isOpen()) return 0; boolean append = fdAccess.getAppend(fd); do &#123; //append模式下，position在channel的末尾 // in append-mode then position is advanced to end before writing p = (append) ? nd.size(fd) : nd.seek(fd, -1); &#125; while ((p == IOStatus.INTERRUPTED) &amp;&amp; isOpen()); return IOStatus.normalize(p); &#125; finally &#123; threads.remove(ti); endBlocking(p &gt; -1); assert IOStatus.check(p); &#125; &#125;&#125;//实现自SeekableByteChannel接口的方法，设置当前channel的position为newPositionpublic FileChannel position(long newPosition) throws IOException &#123; ensureOpen(); if (newPosition &lt; 0) throw new IllegalArgumentException(); synchronized (positionLock) &#123; long p = -1; int ti = -1; try &#123; beginBlocking(); ti = threads.add(); if (!isOpen()) return null; do &#123; //设置当前position为newPosition p = nd.seek(fd, newPosition); &#125; while ((p == IOStatus.INTERRUPTED) &amp;&amp; isOpen()); return this; &#125; finally &#123; threads.remove(ti); endBlocking(p &gt; -1); assert IOStatus.check(p); &#125; &#125;&#125; size实现自SeekableByteChannel接口的方法，返回当前实体（文件）的大小 truncate实现自SeekableByteChannel接口的方法，用来截取文件至newSize大小 force实现自SeekableByteChannel接口的方法，用来将channel中尚未写入磁盘的数据强制落盘 transferTo将fileChannel中的数据传递至另一个channel transferFrom从其它channel读取数据至fileChannel SocketChannel open12345678910111213141516/** * Opens a socket channel. * * &lt;p&gt; The new channel is created by invoking the &#123;@link * java.nio.channels.spi.SelectorProvider#openSocketChannel * openSocketChannel&#125; method of the system-wide default &#123;@link * java.nio.channels.spi.SelectorProvider&#125; object. &lt;/p&gt; * * @return A new socket channel * * @throws IOException * If an I/O error occurs */public static SocketChannel open() throws IOException &#123; return SelectorProvider.provider().openSocketChannel();&#125; open方法是调用SelectorProvider中实现了java.nio.channels.spi.SelectorProvider#openSocketChannel的方法，底层实际是new SocketChannelImpl，调用native方法创建socket connect12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public boolean connect(SocketAddress sa) throws IOException &#123; //校验Address是否合法 InetSocketAddress isa = Net.checkAddress(sa); //获取系统安全管理器 SecurityManager sm = System.getSecurityManager(); if (sm != null) //校验IP和端口是否被允许连接 sm.checkConnect(isa.getAddress().getHostAddress(), isa.getPort()); InetAddress ia = isa.getAddress(); //如果是本机地址，则获取本机的host if (ia.isAnyLocalAddress()) ia = InetAddress.getLocalHost(); try &#123; //加读锁 readLock.lock(); try &#123; //加写锁 writeLock.lock(); try &#123; int n = 0; //是否阻塞 boolean blocking = isBlocking(); try &#123; //开启connect前的校验并设置为ST_CONNECTIONPENDING，如果blocking是true 即阻塞模式，则记录当前线程的ID，以便接收信号处理。 beginConnect(blocking, isa); do &#123; //调用native connect方法 n = Net.connect(fd, ia, isa.getPort()); &#125; while (n == IOStatus.INTERRUPTED &amp;&amp; isOpen()); &#125; finally &#123; //结束连接 endConnect(blocking, (n &gt; 0)); &#125; assert IOStatus.check(n); return n &gt; 0; &#125; finally &#123; //释放写锁 writeLock.unlock(); &#125; &#125; finally &#123; //释放读锁 readLock.unlock(); &#125; &#125; catch (IOException ioe) &#123; // connect failed, close the channel close(); throw SocketExceptions.of(ioe, isa); &#125;&#125; configureBlocking实现自SelectableChannel的接口方法，调用native方法设置socket的阻塞状态 register在AbstractSelectableChannel中定义，注册要监听的事件。12345678910111213141516171819202122232425262728public final SelectionKey register(Selector sel, int ops, Object att) throws ClosedChannelException&#123; if ((ops &amp; ~validOps()) != 0) throw new IllegalArgumentException(); if (!isOpen()) throw new ClosedChannelException(); synchronized (regLock) &#123; if (isBlocking()) throw new IllegalBlockingModeException(); synchronized (keyLock) &#123; // re-check if channel has been closed if (!isOpen()) throw new ClosedChannelException(); SelectionKey k = findKey(sel); if (k != null) &#123; k.attach(att); k.interestOps(ops); &#125; else &#123; // 向Selector中注册事件 // New registration k = ((AbstractSelector)sel).register(this, ops, att); addKey(k); &#125; return k; &#125; &#125;&#125; read123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172//实现自ReadableByteChannel接口的方法，从socket中读取数据至ByteBuffer@Overridepublic int read(ByteBuffer buf) throws IOException &#123; Objects.requireNonNull(buf); readLock.lock(); try &#123; boolean blocking = isBlocking(); int n = 0; try &#123; //检查channel是否开启并已经是connected的状态。如果blocking是true 即阻塞模式，则记录当前线程的ID，以便接收信号处理。 beginRead(blocking); // check if input is shutdown if (isInputClosed) return IOStatus.EOF; //如果是阻塞模式，则一直读取直到数据读取完毕；非阻塞模式则直接调用native方法不需要等待。 if (blocking) &#123; do &#123; n = IOUtil.read(fd, buf, -1, nd); &#125; while (n == IOStatus.INTERRUPTED &amp;&amp; isOpen()); &#125; else &#123; n = IOUtil.read(fd, buf, -1, nd); &#125; &#125; finally &#123; endRead(blocking, n &gt; 0); if (n &lt;= 0 &amp;&amp; isInputClosed) return IOStatus.EOF; &#125; return IOStatus.normalize(n); &#125; finally &#123; readLock.unlock(); &#125;&#125;//实现自ScatteringByteChannel接口的方法，从socket中依次读取数据至ByteBuffer数组@Overridepublic long read(ByteBuffer[] dsts, int offset, int length) throws IOException&#123; Objects.checkFromIndexSize(offset, length, dsts.length); readLock.lock(); try &#123; boolean blocking = isBlocking(); long n = 0; try &#123; beginRead(blocking); // check if input is shutdown if (isInputClosed) return IOStatus.EOF; //如果是阻塞模式，则一直读取直到数据读取完毕；非阻塞模式则直接调用native方法不需要等待。 if (blocking) &#123; do &#123; n = IOUtil.read(fd, dsts, offset, length, nd); &#125; while (n == IOStatus.INTERRUPTED &amp;&amp; isOpen()); &#125; else &#123; n = IOUtil.read(fd, dsts, offset, length, nd); &#125; &#125; finally &#123; endRead(blocking, n &gt; 0); if (n &lt;= 0 &amp;&amp; isInputClosed) return IOStatus.EOF; &#125; return IOStatus.normalize(n); &#125; finally &#123; readLock.unlock(); &#125;&#125; write123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990//实现自ReadableByteChannel接口的方法，将ByteBuffer中的数据写入socket@Overridepublic int write(ByteBuffer buf) throws IOException &#123; Objects.requireNonNull(buf); writeLock.lock(); try &#123; boolean blocking = isBlocking(); int n = 0; try &#123; beginWrite(blocking); //如果是阻塞模式，则一直读取直到数据读取完毕；非阻塞模式则直接调用native方法不需要等待。 if (blocking) &#123; do &#123; n = IOUtil.write(fd, buf, -1, nd); &#125; while (n == IOStatus.INTERRUPTED &amp;&amp; isOpen()); &#125; else &#123; n = IOUtil.write(fd, buf, -1, nd); &#125; &#125; finally &#123; endWrite(blocking, n &gt; 0); if (n &lt;= 0 &amp;&amp; isOutputClosed) throw new AsynchronousCloseException(); &#125; return IOStatus.normalize(n); &#125; finally &#123; writeLock.unlock(); &#125;&#125;@Overridepublic long write(ByteBuffer[] srcs, int offset, int length) throws IOException&#123; Objects.checkFromIndexSize(offset, length, srcs.length); writeLock.lock(); try &#123; boolean blocking = isBlocking(); long n = 0; try &#123; beginWrite(blocking); //如果是阻塞模式，则一直等待直到数据写入完毕；非阻塞模式则直接调用native方法不需要等待。 if (blocking) &#123; do &#123; n = IOUtil.write(fd, srcs, offset, length, nd); &#125; while (n == IOStatus.INTERRUPTED &amp;&amp; isOpen()); &#125; else &#123; n = IOUtil.write(fd, srcs, offset, length, nd); &#125; &#125; finally &#123; endWrite(blocking, n &gt; 0); if (n &lt;= 0 &amp;&amp; isOutputClosed) throw new AsynchronousCloseException(); &#125; return IOStatus.normalize(n); &#125; finally &#123; writeLock.unlock(); &#125;&#125;//实现自ReadableByteChannel接口的方法，将ByteBuffer数组中的数据依次写入socket/** * Writes a byte of out of band data. */int sendOutOfBandData(byte b) throws IOException &#123; writeLock.lock(); try &#123; boolean blocking = isBlocking(); int n = 0; try &#123; beginWrite(blocking); //如果是阻塞模式，则一直等待直到数据写入完毕；非阻塞模式则直接调用native方法不需要等待。 if (blocking) &#123; do &#123; n = sendOutOfBandData(fd, b); &#125; while (n == IOStatus.INTERRUPTED &amp;&amp; isOpen()); &#125; else &#123; n = sendOutOfBandData(fd, b); &#125; &#125; finally &#123; endWrite(blocking, n &gt; 0); if (n &lt;= 0 &amp;&amp; isOutputClosed) throw new AsynchronousCloseException(); &#125; return IOStatus.normalize(n); &#125; finally &#123; writeLock.unlock(); &#125;&#125; ServerSocketChannel socket12345678@Overridepublic ServerSocket socket() &#123; synchronized (stateLock) &#123; if (socket == null) socket = ServerSocketAdaptor.create(this); return socket; &#125;&#125; bind1234567891011121314151617181920212223242526@Overridepublic ServerSocketChannel bind(SocketAddress local, int backlog) throws IOException &#123; synchronized (stateLock) &#123; ensureOpen(); if (localAddress != null) throw new AlreadyBoundException(); InetSocketAddress isa = (local == null) ? new InetSocketAddress(0) : Net.checkAddress(local); SecurityManager sm = System.getSecurityManager(); if (sm != null) sm.checkListen(isa.getPort()); //绑定前做一些前置处理，如将tcp socket文件描述符转换成SDP NetHooks.beforeTcpBind(fd, isa.getAddress(), isa.getPort()); //绑定IP和地址 Net.bind(fd, isa.getAddress(), isa.getPort()); //开始监听，设置socket上最多可以挂起backlog个连接，若backlog小于1 则默认设置50个 Net.listen(fd, backlog &lt; 1 ? 50 : backlog); localAddress = Net.localAddress(fd); &#125; return this;&#125; accept123456789101112131415161718192021222324252627282930313233343536373839404142434445464748@Overridepublic SocketChannel accept() throws IOException &#123; acceptLock.lock(); try &#123; int n = 0; FileDescriptor newfd = new FileDescriptor(); InetSocketAddress[] isaa = new InetSocketAddress[1]; boolean blocking = isBlocking(); try &#123; begin(blocking); do &#123; //阻塞等待接收客户端链接 n = accept(this.fd, newfd, isaa); &#125; while (n == IOStatus.INTERRUPTED &amp;&amp; isOpen()); &#125; finally &#123; end(blocking, n &gt; 0); assert IOStatus.check(n); &#125; if (n &lt; 1) return null; //新接收的socket初始设置为阻塞模式（因此非阻塞模式的每次需要显示设置） // newly accepted socket is initially in blocking mode IOUtil.configureBlocking(newfd, true); InetSocketAddress isa = isaa[0]; //用新接收的socket创建SocketChannel SocketChannel sc = new SocketChannelImpl(provider(), newfd, isa); // check permitted to accept connections from the remote address SecurityManager sm = System.getSecurityManager(); if (sm != null) &#123; try &#123; sm.checkAccept(isa.getAddress().getHostAddress(), isa.getPort()); &#125; catch (SecurityException x) &#123; sc.close(); throw x; &#125; &#125; return sc; &#125; finally &#123; acceptLock.unlock(); &#125;&#125; ServerSocketChannel并没有read和write方法，只是继承了AbstractSelectableChannel，以便在selector中使用 DatagramChannel open12345678910111213141516171819public DatagramChannelImpl(SelectorProvider sp) throws IOException&#123; super(sp); ResourceManager.beforeUdpCreate(); try &#123; //如果不支持IPv6则使用IPv4 this.family = Net.isIPv6Available() ? StandardProtocolFamily.INET6 : StandardProtocolFamily.INET; //设置非流式的socket（tcp是流模式协议，udp是数据报模式协议） this.fd = Net.socket(family, false); this.fdVal = IOUtil.fdVal(fd); &#125; catch (IOException ioe) &#123; ResourceManager.afterUdpClose(); throw ioe; &#125;&#125; receive1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public SocketAddress receive(ByteBuffer dst) throws IOException &#123; if (dst.isReadOnly()) throw new IllegalArgumentException("Read-only buffer"); readLock.lock(); try &#123; boolean blocking = isBlocking(); int n = 0; ByteBuffer bb = null; try &#123; SocketAddress remote = beginRead(blocking, false); boolean connected = (remote != null); SecurityManager sm = System.getSecurityManager(); if (connected || (sm == null)) &#123; // connected or no security manager do &#123; n = receive(fd, dst, connected); &#125; while ((n == IOStatus.INTERRUPTED) &amp;&amp; isOpen()); if (n == IOStatus.UNAVAILABLE) return null; &#125; else &#123; // Cannot receive into user's buffer when running with a // security manager and not connected bb = Util.getTemporaryDirectBuffer(dst.remaining()); for (;;) &#123; do &#123; n = receive(fd, bb, connected); &#125; while ((n == IOStatus.INTERRUPTED) &amp;&amp; isOpen()); if (n == IOStatus.UNAVAILABLE) return null; InetSocketAddress isa = (InetSocketAddress)sender; try &#123; sm.checkAccept(isa.getAddress().getHostAddress(), isa.getPort()); &#125; catch (SecurityException se) &#123; // Ignore packet bb.clear(); n = 0; continue; &#125; bb.flip(); dst.put(bb); break; &#125; &#125; //sender:发送方地址， Set by receive0 (## ugh) assert sender != null; return sender; &#125; finally &#123; if (bb != null) Util.releaseTemporaryDirectBuffer(bb); endRead(blocking, n &gt; 0); assert IOStatus.check(n); &#125; &#125; finally &#123; readLock.unlock(); &#125;&#125; send123456789101112131415161718192021222324252627282930313233343536373839404142434445public int send(ByteBuffer src, SocketAddress target) throws IOException&#123; Objects.requireNonNull(src); InetSocketAddress isa = Net.checkAddress(target, family); writeLock.lock(); try &#123; boolean blocking = isBlocking(); int n = 0; try &#123; //当connect后，remote会设置为连接的地址 SocketAddress remote = beginWrite(blocking, false); if (remote != null) &#123; // connected if (!target.equals(remote)) &#123; throw new AlreadyConnectedException(); &#125; do &#123; n = IOUtil.write(fd, src, -1, nd); &#125; while ((n == IOStatus.INTERRUPTED) &amp;&amp; isOpen()); &#125; else &#123; // not connected SecurityManager sm = System.getSecurityManager(); if (sm != null) &#123; InetAddress ia = isa.getAddress(); if (ia.isMulticastAddress()) &#123; sm.checkMulticast(ia); &#125; else &#123; sm.checkConnect(ia.getHostAddress(), isa.getPort()); &#125; &#125; do &#123; n = send(fd, src, isa); &#125; while ((n == IOStatus.INTERRUPTED) &amp;&amp; isOpen()); &#125; &#125; finally &#123; endWrite(blocking, n &gt; 0); assert IOStatus.check(n); &#125; return IOStatus.normalize(n); &#125; finally &#123; writeLock.unlock(); &#125;&#125; connect12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061@Overridepublic DatagramChannel connect(SocketAddress sa) throws IOException &#123; InetSocketAddress isa = Net.checkAddress(sa, family); SecurityManager sm = System.getSecurityManager(); if (sm != null) &#123; InetAddress ia = isa.getAddress(); if (ia.isMulticastAddress()) &#123; sm.checkMulticast(ia); &#125; else &#123; sm.checkConnect(ia.getHostAddress(), isa.getPort()); sm.checkAccept(ia.getHostAddress(), isa.getPort()); &#125; &#125; readLock.lock(); try &#123; writeLock.lock(); try &#123; synchronized (stateLock) &#123; ensureOpen(); if (state == ST_CONNECTED) throw new AlreadyConnectedException(); int n = Net.connect(family, fd, isa.getAddress(), isa.getPort()); if (n &lt;= 0) throw new Error(); // Can't happen // connected remoteAddress = isa; state = ST_CONNECTED; // refresh local address localAddress = Net.localAddress(fd); // flush any packets already received. boolean blocking = isBlocking(); if (blocking) &#123; IOUtil.configureBlocking(fd, false); &#125; try &#123; ByteBuffer buf = ByteBuffer.allocate(100); while (receive(buf) != null) &#123; buf.clear(); &#125; &#125; finally &#123; if (blocking) &#123; IOUtil.configureBlocking(fd, true); &#125; &#125; &#125; &#125; finally &#123; writeLock.unlock(); &#125; &#125; finally &#123; readLock.unlock(); &#125; return this;&#125; udp是数据报模式的协议，是没有connect的。这里的connect实际上是在底层忽略了与其他地址的数据传输。在connect后，就可以像socketChannel似得使用read和write了 总结本文学习了各种channel的实现，主要是对底层native方法的一些封装，针对不同属性的实体（文件、socket），使用对应的channel与byteBuffer传输数据。再通过byteBuffer与byte数据进行转换。channel的实现中，封装了大量的native方法，重要的底层实现全在native中，后续可以深入学习下。 本文中出现的byteBuffer和selector将在接下来的文章中，单独分析。]]></content>
      <tags>
        <tag>Java</tag>
        <tag>并发编程</tag>
        <tag>NIO</tag>
        <tag>socket</tag>
        <tag>channel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【从入门到放弃-Java】并发编程-NIO使用]]></title>
    <url>%2Fe57ab61.html</url>
    <content type="text"><![CDATA[前言上文【从入门到放弃-SpringBoot】SpringBoot源码分析-请求过程中我们了解到，tomcat接收、返回请求的过程都是基于NIO实现的。日常工作中有很多基于NIO的使用，我们知道NIO可以提高系统的并发度，接下来的系列我们来深入学习下NIO，本文先从使用上简单概述。 NIO概述NIO即non-blocking（New IO），是指jdk1.4 及以上版本里提供的新api。 NIO和IO最大的区别：IO是以流的方式处理数据，而NIO是以块的方式处理数据；IO对事件的处理是阻塞的，NIO是非阻塞的 NIO的核心部分： Channel Buffer Selector NIO主要分为标准输入输出和网络请求 标准输入输出NIO读取1234567891011121314151617181920212223242526272829private static void readNio() &#123; try &#123; //1、开启文件读取流 FileInputStream fileInputStream = new FileInputStream("/Users/my/Desktop/123.txt"); //2、获取fileChannel FileChannel channel = fileInputStream.getChannel(); //3、设置ByteBuffer大小，一次能容纳capacity字节 int capacity = 9; ByteBuffer bf = ByteBuffer.allocate(capacity); //4、当read返回-1时，表示文件读取完毕 int length = -1; while ((length = channel.read(bf)) != -1) &#123; byte[] bytes = bf.array(); System.out.println(new String(bytes, 0, length)); //4、将bf position置为0，方便下次读取 bf.clear(); &#125; channel.close(); fileInputStream.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;&#125; 写入123456789101112131415161718192021222324252627282930313233343536private static void writeNio() &#123; try &#123; //1、打开文件写入流 FileOutputStream fileOutputStream = new FileOutputStream("/Users/my/Desktop/123.txt"); //2、获取fileChannel FileChannel channel = fileOutputStream.getChannel(); //3、初始化byteBuffer String str = "萨达案发生大大sdada34;sdds'"; ByteBuffer bf = ByteBuffer.allocate(1024); //4、将bf position置为0，方便下次读取 bf.clear(); //5、从byteBuffer的position位置填充byte bf.put(str.getBytes()); //6、将bf position置为0，limit设置为position避免写入内容过多 bf.flip(); int length = 0; //7、如果position小于limit即未写入完毕 while (bf.hasRemaining()) &#123; //8、将buffer内容写入channel length = channel.write(bf); System.out.println(bf); &#125; channel.close(); fileOutputStream.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;&#125; 网络NIO服务端123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105package com.my.tools.nio;import java.io.IOException;import java.net.InetSocketAddress;import java.nio.ByteBuffer;import java.nio.channels.SelectionKey;import java.nio.channels.Selector;import java.nio.channels.ServerSocketChannel;import java.nio.channels.SocketChannel;import java.util.Iterator;public class ServerSocket &#123; private static ServerSocket serverSocket; private Selector selector; public static void main(String[] args) throws Exception &#123; ServerSocket.getInstance().init(8001).listen(); &#125; public static ServerSocket getInstance() &#123; if (serverSocket == null) &#123; synchronized (ServerSocket.class) &#123; if (serverSocket == null) &#123; serverSocket = new ServerSocket(); &#125; &#125; &#125; return serverSocket; &#125; public ServerSocket init(int port) throws IOException &#123; //初始化channel ServerSocketChannel server = ServerSocketChannel.open(); //绑定本机8001端口 server.socket().bind(new InetSocketAddress(8001)); //设置为非阻塞模式 server.configureBlocking(false); //开启selector管理器 selector = Selector.open(); //将selector注册至server，并设置只处理accept事件 server.register(selector, SelectionKey.OP_ACCEPT); return this; &#125; public void listen() throws Exception &#123; System.out.println("server start"); //无限循环持续监听 while (true) &#123; //会阻塞 直到监听到注册的事件 selector.select(); //获取唤醒的事件 Iterator&lt;SelectionKey&gt; selectorKeys = selector.selectedKeys().iterator(); while (selectorKeys.hasNext()) &#123; SelectionKey key = selectorKeys.next(); //将已取出的SelectionKey删除，防止重复处理 selectorKeys.remove(); if (key.isAcceptable()) &#123; //获取到服务端的socket ServerSocketChannel serverSocketChannel = (ServerSocketChannel) key.channel(); //获取接收到的客户端socket SocketChannel socketChannel = serverSocketChannel.accept(); socketChannel.configureBlocking(false); //向客户端写消息 socketChannel.write(ByteBuffer.wrap(new String("hello, this is server").getBytes())); //注册监听read事件 socketChannel.register(selector, SelectionKey.OP_READ); System.out.println("accept"); &#125; else if (key.isReadable()) &#123; //使用selector获取channel SocketChannel socketChannel = (SocketChannel) key.channel(); socketChannel.configureBlocking(false); ByteBuffer buffer = ByteBuffer.allocate(1024); //读消息 int length = socketChannel.read(buffer); String string = new String(buffer.array(), 0 , length); System.out.println("read:" + socketChannel + string); //写消息 socketChannel.write(ByteBuffer.wrap(("server " + System.currentTimeMillis()).getBytes())); Thread.sleep(10000); &#125; &#125; &#125; &#125;&#125; 客户端12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182package com.my.tools.nio;import java.io.IOException;import java.net.InetSocketAddress;import java.nio.ByteBuffer;import java.nio.channels.SelectionKey;import java.nio.channels.Selector;import java.nio.channels.SocketChannel;import java.util.Iterator;public class ClientSocket &#123; public static ClientSocket clientSocket; private static Selector selector; public static void main(String[] args) throws Exception &#123; ClientSocket.getInstance().init("localhost", 8001).listen(); &#125; public static ClientSocket getInstance() &#123; if (clientSocket == null) &#123; synchronized (ClientSocket.class) &#123; if (clientSocket == null) &#123; clientSocket = new ClientSocket(); &#125; &#125; &#125; return clientSocket; &#125; public ClientSocket init(String ip, int port) throws IOException &#123; SocketChannel socketChannel = SocketChannel.open(); socketChannel.connect(new InetSocketAddress(ip, port)); socketChannel.configureBlocking(false); selector = Selector.open(); socketChannel.register(selector, SelectionKey.OP_CONNECT | SelectionKey.OP_READ); return this; &#125; public void listen() throws Exception &#123; System.out.println("client start"); while (true) &#123; selector.select(); Iterator&lt;SelectionKey&gt; selectionKeys = selector.selectedKeys().iterator(); while (selectionKeys.hasNext()) &#123; SelectionKey selectionKey = selectionKeys.next(); selectionKeys.remove(); if (selectionKey.isConnectable()) &#123; SocketChannel socketChannel = (SocketChannel) selectionKey.channel(); socketChannel.configureBlocking(false); ByteBuffer buffer = ByteBuffer.wrap(new String("hello, this is client").getBytes()); socketChannel.write(buffer); socketChannel.register(selector, SelectionKey.OP_READ); System.out.println("client write"); &#125; else if (selectionKey.isReadable()) &#123; SocketChannel socketChannel = (SocketChannel) selectionKey.channel(); socketChannel.configureBlocking(false); ByteBuffer buffer = ByteBuffer.allocate(1024); int length = socketChannel.read(buffer); System.out.println("client read: " + socketChannel + new String(buffer.array(), 0, length)); socketChannel.write(ByteBuffer.wrap(("client " + System.currentTimeMillis()).getBytes())); Thread.sleep(10000); &#125; &#125; &#125; &#125;&#125; 总结上述示例展示了最简单的文件NIO和网络NIO用法，接下来会深入分析每个方法的源码，并对性能进行调优。]]></content>
      <tags>
        <tag>Java</tag>
        <tag>并发编程</tag>
        <tag>NIO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【从入门到放弃-SpringBoot】SpringBoot源码分析-请求过程]]></title>
    <url>%2F788cd0c2.html</url>
    <content type="text"><![CDATA[前言前文【从入门到放弃-SpringBoot】SpringBoot源码分析-WebServer中以SpringBoot中内嵌的Tomcat为例了解了webserver的启动过程。 本文将分析下一条请求在SpringBoot中，从接受到返回都经历了那些过程。 Acceptor 在上文最后的connector启动时，会开始acceptor线程等待接收请求。 Acceptor::run123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384public void run() &#123; int errorDelay = 0; // Loop until we receive a shutdown command while (endpoint.isRunning()) &#123; // Loop if endpoint is paused while (endpoint.isPaused() &amp;&amp; endpoint.isRunning()) &#123; state = AcceptorState.PAUSED; try &#123; Thread.sleep(50); &#125; catch (InterruptedException e) &#123; // Ignore &#125; &#125; if (!endpoint.isRunning()) &#123; break; &#125; state = AcceptorState.RUNNING; try &#123; //if we have reached max connections, wait endpoint.countUpOrAwaitConnection(); // Endpoint might have been paused while waiting for latch // If that is the case, don't accept new connections if (endpoint.isPaused()) &#123; continue; &#125; U socket = null; try &#123; // Accept the next incoming connection from the server // socket socket = endpoint.serverSocketAccept(); &#125; catch (Exception ioe) &#123; // We didn't get a socket endpoint.countDownConnection(); if (endpoint.isRunning()) &#123; // Introduce delay if necessary errorDelay = handleExceptionWithDelay(errorDelay); // re-throw throw ioe; &#125; else &#123; break; &#125; &#125; // Successful accept, reset the error delay errorDelay = 0; // Configure the socket if (endpoint.isRunning() &amp;&amp; !endpoint.isPaused()) &#123; // setSocketOptions() will hand the socket off to // an appropriate processor if successful if (!endpoint.setSocketOptions(socket)) &#123; endpoint.closeSocket(socket); &#125; &#125; else &#123; endpoint.destroySocket(socket); &#125; &#125; catch (Throwable t) &#123; ExceptionUtils.handleThrowable(t); String msg = sm.getString("endpoint.accept.fail"); // APR specific. // Could push this down but not sure it is worth the trouble. if (t instanceof Error) &#123; Error e = (Error) t; if (e.getError() == 233) &#123; // Not an error on HP-UX so log as a warning // so it can be filtered out on that platform // See bug 50273 log.warn(msg, t); &#125; else &#123; log.error(msg, t); &#125; &#125; else &#123; log.error(msg, t); &#125; &#125; &#125; state = AcceptorState.ENDED;&#125; acceptor会一直监听端口，等待连接 setSocketOptions12345678910111213141516171819202122232425262728293031323334353637@Overrideprotected boolean setSocketOptions(SocketChannel socket) &#123; // Process the connection try &#123; //disable blocking, APR style, we are gonna be polling it socket.configureBlocking(false); Socket sock = socket.socket(); socketProperties.setProperties(sock); NioChannel channel = nioChannels.pop(); if (channel == null) &#123; SocketBufferHandler bufhandler = new SocketBufferHandler( socketProperties.getAppReadBufSize(), socketProperties.getAppWriteBufSize(), socketProperties.getDirectBuffer()); if (isSSLEnabled()) &#123; channel = new SecureNioChannel(socket, bufhandler, selectorPool, this); &#125; else &#123; channel = new NioChannel(socket, bufhandler); &#125; &#125; else &#123; channel.setIOChannel(socket); channel.reset(); &#125; getPoller0().register(channel); &#125; catch (Throwable t) &#123; ExceptionUtils.handleThrowable(t); try &#123; log.error(sm.getString("endpoint.socketOptionsError"), t); &#125; catch (Throwable tt) &#123; ExceptionUtils.handleThrowable(tt); &#125; // Tell to close the socket return false; &#125; return true;&#125; NioChannel：将捕获到的socket封装成NioChannel register123456789101112131415public void register(final NioChannel socket) &#123; socket.setPoller(this); NioSocketWrapper ka = new NioSocketWrapper(socket, NioEndpoint.this); socket.setSocketWrapper(ka); ka.setPoller(this); ka.setReadTimeout(getConnectionTimeout()); ka.setWriteTimeout(getConnectionTimeout()); ka.setKeepAliveLeft(NioEndpoint.this.getMaxKeepAliveRequests()); ka.setSecure(isSSLEnabled()); PollerEvent r = eventCache.pop(); ka.interestOps(SelectionKey.OP_READ);//this is what OP_REGISTER turns into. if ( r==null) r = new PollerEvent(socket,ka,OP_REGISTER); else r.reset(socket,ka,OP_REGISTER); addEvent(r);&#125; 将channel封装成一个PollerEvent投递到SynchronizedQueue队列中，等待poller进行消费 Poller::run123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960@Overridepublic void run() &#123; // Loop until destroy() is called while (true) &#123; boolean hasEvents = false; try &#123; if (!close) &#123; hasEvents = events(); if (wakeupCounter.getAndSet(-1) &gt; 0) &#123; //if we are here, means we have other stuff to do //do a non blocking select keyCount = selector.selectNow(); &#125; else &#123; keyCount = selector.select(selectorTimeout); &#125; wakeupCounter.set(0); &#125; if (close) &#123; events(); timeout(0, false); try &#123; selector.close(); &#125; catch (IOException ioe) &#123; log.error(sm.getString("endpoint.nio.selectorCloseFail"), ioe); &#125; break; &#125; &#125; catch (Throwable x) &#123; ExceptionUtils.handleThrowable(x); log.error(sm.getString("endpoint.nio.selectorLoopError"), x); continue; &#125; //either we timed out or we woke up, process events first if ( keyCount == 0 ) hasEvents = (hasEvents | events()); Iterator&lt;SelectionKey&gt; iterator = keyCount &gt; 0 ? selector.selectedKeys().iterator() : null; // Walk through the collection of ready keys and dispatch // any active event. while (iterator != null &amp;&amp; iterator.hasNext()) &#123; SelectionKey sk = iterator.next(); NioSocketWrapper attachment = (NioSocketWrapper)sk.attachment(); // Attachment may be null if another thread has called // cancelledKey() if (attachment == null) &#123; iterator.remove(); &#125; else &#123; iterator.remove(); processKey(sk, attachment); &#125; &#125;//while //process timeouts timeout(keyCount,hasEvents); &#125;//while getStopLatch().countDown();&#125; events：处理队列中的Poller Event，直接调用PollerEvent::run。如果返回true 则说明事件已经被处理，返回false说明队列是空的 processKey：请求处理的逻辑，将socket交给processSocket处理。 SocketProcessor：继承自SocketProcessorBase，调用run方法开始请求的处理逻辑。 图片来源：https://blog.csdn.net/TVwR8OfV0P/article/details/78851949 SocketProcessorBase的run方法是不是非常眼熟？在【从入门到放弃-MySQL】数据库连接过程分析-客户端中，我们断点调试一次查询请求的处理过程，调用栈入口就是这个方法。 SocketProcessor::run1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071@Overrideprotected void doRun() &#123; NioChannel socket = socketWrapper.getSocket(); SelectionKey key = socket.getIOChannel().keyFor(socket.getPoller().getSelector()); try &#123; int handshake = -1; try &#123; if (key != null) &#123; if (socket.isHandshakeComplete()) &#123; // No TLS handshaking required. Let the handler // process this socket / event combination. handshake = 0; &#125; else if (event == SocketEvent.STOP || event == SocketEvent.DISCONNECT || event == SocketEvent.ERROR) &#123; // Unable to complete the TLS handshake. Treat it as // if the handshake failed. handshake = -1; &#125; else &#123; handshake = socket.handshake(key.isReadable(), key.isWritable()); // The handshake process reads/writes from/to the // socket. status may therefore be OPEN_WRITE once // the handshake completes. However, the handshake // happens when the socket is opened so the status // must always be OPEN_READ after it completes. It // is OK to always set this as it is only used if // the handshake completes. event = SocketEvent.OPEN_READ; &#125; &#125; &#125; catch (IOException x) &#123; handshake = -1; if (log.isDebugEnabled()) log.debug("Error during SSL handshake",x); &#125; catch (CancelledKeyException ckx) &#123; handshake = -1; &#125; if (handshake == 0) &#123; SocketState state = SocketState.OPEN; // Process the request from this socket if (event == null) &#123; state = getHandler().process(socketWrapper, SocketEvent.OPEN_READ); &#125; else &#123; state = getHandler().process(socketWrapper, event); &#125; if (state == SocketState.CLOSED) &#123; close(socket, key); &#125; &#125; else if (handshake == -1 ) &#123; close(socket, key); &#125; else if (handshake == SelectionKey.OP_READ)&#123; socketWrapper.registerReadInterest(); &#125; else if (handshake == SelectionKey.OP_WRITE)&#123; socketWrapper.registerWriteInterest(); &#125; &#125; catch (CancelledKeyException cx) &#123; socket.getPoller().cancelledKey(key); &#125; catch (VirtualMachineError vme) &#123; ExceptionUtils.handleThrowable(vme); &#125; catch (Throwable t) &#123; log.error(sm.getString("endpoint.processing.fail"), t); socket.getPoller().cancelledKey(key); &#125; finally &#123; socketWrapper = null; event = null; //return to cache if (running &amp;&amp; !paused) &#123; processorCache.push(this); &#125; &#125;&#125; 判断socket是否握手完毕并根据socket的状态处理请求或关闭链接 ConnectionHandler::process：调用AbstractProcessorLight::process，判断socket的状态及协议是否需要upgrade（如HTTP 2，websocket等基于HTTP的协议），选择对应的Processor处理请求。 Http11Processor::service123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232public SocketState service(SocketWrapperBase&lt;?&gt; socketWrapper) throws IOException &#123; RequestInfo rp = request.getRequestProcessor(); rp.setStage(org.apache.coyote.Constants.STAGE_PARSE); // Setting up the I/O setSocketWrapper(socketWrapper); inputBuffer.init(socketWrapper); outputBuffer.init(socketWrapper); // Flags keepAlive = true; openSocket = false; readComplete = true; boolean keptAlive = false; SendfileState sendfileState = SendfileState.DONE; while (!getErrorState().isError() &amp;&amp; keepAlive &amp;&amp; !isAsync() &amp;&amp; upgradeToken == null &amp;&amp; sendfileState == SendfileState.DONE &amp;&amp; !protocol.isPaused()) &#123; // Parsing the request header try &#123; if (!inputBuffer.parseRequestLine(keptAlive, protocol.getConnectionTimeout(), protocol.getKeepAliveTimeout())) &#123; if (inputBuffer.getParsingRequestLinePhase() == -1) &#123; return SocketState.UPGRADING; &#125; else if (handleIncompleteRequestLineRead()) &#123; break; &#125; &#125; if (protocol.isPaused()) &#123; // 503 - Service unavailable response.setStatus(503); setErrorState(ErrorState.CLOSE_CLEAN, null); &#125; else &#123; keptAlive = true; // Set this every time in case limit has been changed via JMX request.getMimeHeaders().setLimit(protocol.getMaxHeaderCount()); if (!inputBuffer.parseHeaders()) &#123; // We've read part of the request, don't recycle it // instead associate it with the socket openSocket = true; readComplete = false; break; &#125; if (!protocol.getDisableUploadTimeout()) &#123; socketWrapper.setReadTimeout(protocol.getConnectionUploadTimeout()); &#125; &#125; &#125; catch (IOException e) &#123; if (log.isDebugEnabled()) &#123; log.debug(sm.getString("http11processor.header.parse"), e); &#125; setErrorState(ErrorState.CLOSE_CONNECTION_NOW, e); break; &#125; catch (Throwable t) &#123; ExceptionUtils.handleThrowable(t); UserDataHelper.Mode logMode = userDataHelper.getNextMode(); if (logMode != null) &#123; String message = sm.getString("http11processor.header.parse"); switch (logMode) &#123; case INFO_THEN_DEBUG: message += sm.getString("http11processor.fallToDebug"); //$FALL-THROUGH$ case INFO: log.info(message, t); break; case DEBUG: log.debug(message, t); &#125; &#125; // 400 - Bad Request response.setStatus(400); setErrorState(ErrorState.CLOSE_CLEAN, t); &#125; // Has an upgrade been requested? Enumeration&lt;String&gt; connectionValues = request.getMimeHeaders().values("Connection"); boolean foundUpgrade = false; while (connectionValues.hasMoreElements() &amp;&amp; !foundUpgrade) &#123; foundUpgrade = connectionValues.nextElement().toLowerCase( Locale.ENGLISH).contains("upgrade"); &#125; if (foundUpgrade) &#123; // Check the protocol String requestedProtocol = request.getHeader("Upgrade"); UpgradeProtocol upgradeProtocol = protocol.getUpgradeProtocol(requestedProtocol); if (upgradeProtocol != null) &#123; if (upgradeProtocol.accept(request)) &#123; response.setStatus(HttpServletResponse.SC_SWITCHING_PROTOCOLS); response.setHeader("Connection", "Upgrade"); response.setHeader("Upgrade", requestedProtocol); action(ActionCode.CLOSE, null); getAdapter().log(request, response, 0); InternalHttpUpgradeHandler upgradeHandler = upgradeProtocol.getInternalUpgradeHandler( socketWrapper, getAdapter(), cloneRequest(request)); UpgradeToken upgradeToken = new UpgradeToken(upgradeHandler, null, null); action(ActionCode.UPGRADE, upgradeToken); return SocketState.UPGRADING; &#125; &#125; &#125; if (getErrorState().isIoAllowed()) &#123; // Setting up filters, and parse some request headers rp.setStage(org.apache.coyote.Constants.STAGE_PREPARE); try &#123; prepareRequest(); &#125; catch (Throwable t) &#123; ExceptionUtils.handleThrowable(t); if (log.isDebugEnabled()) &#123; log.debug(sm.getString("http11processor.request.prepare"), t); &#125; // 500 - Internal Server Error response.setStatus(500); setErrorState(ErrorState.CLOSE_CLEAN, t); &#125; &#125; int maxKeepAliveRequests = protocol.getMaxKeepAliveRequests(); if (maxKeepAliveRequests == 1) &#123; keepAlive = false; &#125; else if (maxKeepAliveRequests &gt; 0 &amp;&amp; socketWrapper.decrementKeepAlive() &lt;= 0) &#123; keepAlive = false; &#125; // Process the request in the adapter if (getErrorState().isIoAllowed()) &#123; try &#123; rp.setStage(org.apache.coyote.Constants.STAGE_SERVICE); getAdapter().service(request, response); // Handle when the response was committed before a serious // error occurred. Throwing a ServletException should both // set the status to 500 and set the errorException. // If we fail here, then the response is likely already // committed, so we can't try and set headers. if(keepAlive &amp;&amp; !getErrorState().isError() &amp;&amp; !isAsync() &amp;&amp; statusDropsConnection(response.getStatus())) &#123; setErrorState(ErrorState.CLOSE_CLEAN, null); &#125; &#125; catch (InterruptedIOException e) &#123; setErrorState(ErrorState.CLOSE_CONNECTION_NOW, e); &#125; catch (HeadersTooLargeException e) &#123; log.error(sm.getString("http11processor.request.process"), e); // The response should not have been committed but check it // anyway to be safe if (response.isCommitted()) &#123; setErrorState(ErrorState.CLOSE_NOW, e); &#125; else &#123; response.reset(); response.setStatus(500); setErrorState(ErrorState.CLOSE_CLEAN, e); response.setHeader("Connection", "close"); // TODO: Remove &#125; &#125; catch (Throwable t) &#123; ExceptionUtils.handleThrowable(t); log.error(sm.getString("http11processor.request.process"), t); // 500 - Internal Server Error response.setStatus(500); setErrorState(ErrorState.CLOSE_CLEAN, t); getAdapter().log(request, response, 0); &#125; &#125; // Finish the handling of the request rp.setStage(org.apache.coyote.Constants.STAGE_ENDINPUT); if (!isAsync()) &#123; // If this is an async request then the request ends when it has // been completed. The AsyncContext is responsible for calling // endRequest() in that case. endRequest(); &#125; rp.setStage(org.apache.coyote.Constants.STAGE_ENDOUTPUT); // If there was an error, make sure the request is counted as // and error, and update the statistics counter if (getErrorState().isError()) &#123; response.setStatus(500); &#125; if (!isAsync() || getErrorState().isError()) &#123; request.updateCounters(); if (getErrorState().isIoAllowed()) &#123; inputBuffer.nextRequest(); outputBuffer.nextRequest(); &#125; &#125; if (!protocol.getDisableUploadTimeout()) &#123; int connectionTimeout = protocol.getConnectionTimeout(); if(connectionTimeout &gt; 0) &#123; socketWrapper.setReadTimeout(connectionTimeout); &#125; else &#123; socketWrapper.setReadTimeout(0); &#125; &#125; rp.setStage(org.apache.coyote.Constants.STAGE_KEEPALIVE); sendfileState = processSendfile(socketWrapper); &#125; rp.setStage(org.apache.coyote.Constants.STAGE_ENDED); if (getErrorState().isError() || protocol.isPaused()) &#123; return SocketState.CLOSED; &#125; else if (isAsync()) &#123; return SocketState.LONG; &#125; else if (isUpgrade()) &#123; return SocketState.UPGRADING; &#125; else &#123; if (sendfileState == SendfileState.PENDING) &#123; return SocketState.SENDFILE; &#125; else &#123; if (openSocket) &#123; if (readComplete) &#123; return SocketState.OPEN; &#125; else &#123; return SocketState.LONG; &#125; &#125; else &#123; return SocketState.CLOSED; &#125; &#125; &#125;&#125; 初始化各项配置及初始值 解析request的header信息，判断是否是upgrade协议，是的话设置http header信息 prepareRequest：校验协议、校验header（如user-agent、Connection、host等）设置filters getAdapter().service：getAdapter获取的是在Connector::initInternal中设置的CoyoteAdapter CoyoteAdapter::service123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128public void service(org.apache.coyote.Request req, org.apache.coyote.Response res) throws Exception &#123; Request request = (Request) req.getNote(ADAPTER_NOTES); Response response = (Response) res.getNote(ADAPTER_NOTES); if (request == null) &#123; // Create objects request = connector.createRequest(); request.setCoyoteRequest(req); response = connector.createResponse(); response.setCoyoteResponse(res); // Link objects request.setResponse(response); response.setRequest(request); // Set as notes req.setNote(ADAPTER_NOTES, request); res.setNote(ADAPTER_NOTES, response); // Set query string encoding req.getParameters().setQueryStringCharset(connector.getURICharset()); &#125; if (connector.getXpoweredBy()) &#123; response.addHeader("X-Powered-By", POWERED_BY); &#125; boolean async = false; boolean postParseSuccess = false; req.getRequestProcessor().setWorkerThreadName(THREAD_NAME.get()); try &#123; // Parse and set Catalina and configuration specific // request parameters postParseSuccess = postParseRequest(req, request, res, response); if (postParseSuccess) &#123; //check valves if we support async request.setAsyncSupported( connector.getService().getContainer().getPipeline().isAsyncSupported()); // Calling the container connector.getService().getContainer().getPipeline().getFirst().invoke( request, response); &#125; if (request.isAsync()) &#123; async = true; ReadListener readListener = req.getReadListener(); if (readListener != null &amp;&amp; request.isFinished()) &#123; // Possible the all data may have been read during service() // method so this needs to be checked here ClassLoader oldCL = null; try &#123; oldCL = request.getContext().bind(false, null); if (req.sendAllDataReadEvent()) &#123; req.getReadListener().onAllDataRead(); &#125; &#125; finally &#123; request.getContext().unbind(false, oldCL); &#125; &#125; Throwable throwable = (Throwable) request.getAttribute(RequestDispatcher.ERROR_EXCEPTION); // If an async request was started, is not going to end once // this container thread finishes and an error occurred, trigger // the async error process if (!request.isAsyncCompleting() &amp;&amp; throwable != null) &#123; request.getAsyncContextInternal().setErrorState(throwable, true); &#125; &#125; else &#123; request.finishRequest(); response.finishResponse(); &#125; &#125; catch (IOException e) &#123; // Ignore &#125; finally &#123; AtomicBoolean error = new AtomicBoolean(false); res.action(ActionCode.IS_ERROR, error); if (request.isAsyncCompleting() &amp;&amp; error.get()) &#123; // Connection will be forcibly closed which will prevent // completion happening at the usual point. Need to trigger // call to onComplete() here. res.action(ActionCode.ASYNC_POST_PROCESS, null); async = false; &#125; // Access log if (!async &amp;&amp; postParseSuccess) &#123; // Log only if processing was invoked. // If postParseRequest() failed, it has already logged it. Context context = request.getContext(); Host host = request.getHost(); // If the context is null, it is likely that the endpoint was // shutdown, this connection closed and the request recycled in // a different thread. That thread will have updated the access // log so it is OK not to update the access log here in that // case. // The other possibility is that an error occurred early in // processing and the request could not be mapped to a Context. // Log via the host or engine in that case. long time = System.currentTimeMillis() - req.getStartTime(); if (context != null) &#123; context.logAccess(request, response, time, false); &#125; else if (response.isError()) &#123; if (host != null) &#123; host.logAccess(request, response, time, false); &#125; else &#123; connector.getService().getContainer().logAccess( request, response, time, false); &#125; &#125; &#125; req.getRequestProcessor().setWorkerThreadName(null); // Recycle the wrapper request and response if (!async) &#123; updateWrapperErrorCount(request, response); request.recycle(); response.recycle(); &#125; &#125;&#125; 初始化request和response，如果没有则新new postParseRequest：保证在header解析后的request能被Container正常解析、使用。比如权限认证、字符编码、版本、session等的转换和校验 invoke：逐级调用StandardEngineValve、StandardHostValve、StandardHostValve、StandardWrapperValve的invoke实现 doFilter：StandardWrapperValve调用doFilter，会执行定义的Filter过滤器，执行完毕后执行HttpServlet::service HttpServlet::service12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364protected void service(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; String method = req.getMethod(); if (method.equals(METHOD_GET)) &#123; long lastModified = getLastModified(req); if (lastModified == -1) &#123; // servlet doesn't support if-modified-since, no reason // to go through further expensive logic doGet(req, resp); &#125; else &#123; long ifModifiedSince; try &#123; ifModifiedSince = req.getDateHeader(HEADER_IFMODSINCE); &#125; catch (IllegalArgumentException iae) &#123; // Invalid date header - proceed as if none was set ifModifiedSince = -1; &#125; if (ifModifiedSince &lt; (lastModified / 1000 * 1000)) &#123; // If the servlet mod time is later, call doGet() // Round down to the nearest second for a proper compare // A ifModifiedSince of -1 will always be less maybeSetLastModified(resp, lastModified); doGet(req, resp); &#125; else &#123; resp.setStatus(HttpServletResponse.SC_NOT_MODIFIED); &#125; &#125; &#125; else if (method.equals(METHOD_HEAD)) &#123; long lastModified = getLastModified(req); maybeSetLastModified(resp, lastModified); doHead(req, resp); &#125; else if (method.equals(METHOD_POST)) &#123; doPost(req, resp); &#125; else if (method.equals(METHOD_PUT)) &#123; doPut(req, resp); &#125; else if (method.equals(METHOD_DELETE)) &#123; doDelete(req, resp); &#125; else if (method.equals(METHOD_OPTIONS)) &#123; doOptions(req,resp); &#125; else if (method.equals(METHOD_TRACE)) &#123; doTrace(req,resp); &#125; else &#123; // // Note that this means NO servlet supports whatever // method was requested, anywhere on this server. // String errMsg = lStrings.getString("http.method_not_implemented"); Object[] errArgs = new Object[1]; errArgs[0] = method; errMsg = MessageFormat.format(errMsg, errArgs); resp.sendError(HttpServletResponse.SC_NOT_IMPLEMENTED, errMsg); &#125;&#125; 根据请求的method做对应的处理逻辑，其实doXXX底层都是调用的SpringMVC的FrameworkServlet::processRequest FrameworkServlet::processRequest对request做简单处理后，调用了doService，实现类为DispatcherServlet DispatcherServlet::doService123456789101112131415161718192021222324252627282930313233343536373839404142434445@Overrideprotected void doService(HttpServletRequest request, HttpServletResponse response) throws Exception &#123; logRequest(request); // Keep a snapshot of the request attributes in case of an include, // to be able to restore the original attributes after the include. Map&lt;String, Object&gt; attributesSnapshot = null; if (WebUtils.isIncludeRequest(request)) &#123; attributesSnapshot = new HashMap&lt;&gt;(); Enumeration&lt;?&gt; attrNames = request.getAttributeNames(); while (attrNames.hasMoreElements()) &#123; String attrName = (String) attrNames.nextElement(); if (this.cleanupAfterInclude || attrName.startsWith(DEFAULT_STRATEGIES_PREFIX)) &#123; attributesSnapshot.put(attrName, request.getAttribute(attrName)); &#125; &#125; &#125; // Make framework objects available to handlers and view objects. request.setAttribute(WEB_APPLICATION_CONTEXT_ATTRIBUTE, getWebApplicationContext()); request.setAttribute(LOCALE_RESOLVER_ATTRIBUTE, this.localeResolver); request.setAttribute(THEME_RESOLVER_ATTRIBUTE, this.themeResolver); request.setAttribute(THEME_SOURCE_ATTRIBUTE, getThemeSource()); if (this.flashMapManager != null) &#123; FlashMap inputFlashMap = this.flashMapManager.retrieveAndUpdate(request, response); if (inputFlashMap != null) &#123; request.setAttribute(INPUT_FLASH_MAP_ATTRIBUTE, Collections.unmodifiableMap(inputFlashMap)); &#125; request.setAttribute(OUTPUT_FLASH_MAP_ATTRIBUTE, new FlashMap()); request.setAttribute(FLASH_MAP_MANAGER_ATTRIBUTE, this.flashMapManager); &#125; try &#123; doDispatch(request, response); &#125; finally &#123; if (!WebAsyncUtils.getAsyncManager(request).isConcurrentHandlingStarted()) &#123; // Restore the original attribute snapshot, in case of an include. if (attributesSnapshot != null) &#123; restoreAttributesAfterInclude(request, attributesSnapshot); &#125; &#125; &#125;&#125; 主要调用方法为doDispatch DispatcherServlet::doDispatch123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081protected void doDispatch(HttpServletRequest request, HttpServletResponse response) throws Exception &#123; HttpServletRequest processedRequest = request; HandlerExecutionChain mappedHandler = null; boolean multipartRequestParsed = false; WebAsyncManager asyncManager = WebAsyncUtils.getAsyncManager(request); try &#123; ModelAndView mv = null; Exception dispatchException = null; try &#123; processedRequest = checkMultipart(request); multipartRequestParsed = (processedRequest != request); // Determine handler for the current request. mappedHandler = getHandler(processedRequest); if (mappedHandler == null) &#123; noHandlerFound(processedRequest, response); return; &#125; // Determine handler adapter for the current request. HandlerAdapter ha = getHandlerAdapter(mappedHandler.getHandler()); // Process last-modified header, if supported by the handler. String method = request.getMethod(); boolean isGet = "GET".equals(method); if (isGet || "HEAD".equals(method)) &#123; long lastModified = ha.getLastModified(request, mappedHandler.getHandler()); if (new ServletWebRequest(request, response).checkNotModified(lastModified) &amp;&amp; isGet) &#123; return; &#125; &#125; if (!mappedHandler.applyPreHandle(processedRequest, response)) &#123; return; &#125; // Actually invoke the handler. mv = ha.handle(processedRequest, response, mappedHandler.getHandler()); if (asyncManager.isConcurrentHandlingStarted()) &#123; return; &#125; applyDefaultViewName(processedRequest, mv); mappedHandler.applyPostHandle(processedRequest, response, mv); &#125; catch (Exception ex) &#123; dispatchException = ex; &#125; catch (Throwable err) &#123; // As of 4.3, we're processing Errors thrown from handler methods as well, // making them available for @ExceptionHandler methods and other scenarios. dispatchException = new NestedServletException("Handler dispatch failed", err); &#125; processDispatchResult(processedRequest, response, mappedHandler, mv, dispatchException); &#125; catch (Exception ex) &#123; triggerAfterCompletion(processedRequest, response, mappedHandler, ex); &#125; catch (Throwable err) &#123; triggerAfterCompletion(processedRequest, response, mappedHandler, new NestedServletException("Handler processing failed", err)); &#125; finally &#123; if (asyncManager.isConcurrentHandlingStarted()) &#123; // Instead of postHandle and afterCompletion if (mappedHandler != null) &#123; mappedHandler.applyAfterConcurrentHandlingStarted(processedRequest, response); &#125; &#125; else &#123; // Clean up any resources used by a multipart request. if (multipartRequestParsed) &#123; cleanupMultipart(processedRequest); &#125; &#125; &#125;&#125; getHandler：根据requestURI通过HandlerMapping(这里是requestMappingHandlerMapping)获取到处理此请求的handler以及拦截器（requestMapping对应的类和方法） getHandlerAdapter：获取能执行这个Handler的Adapter applyPreHandle：处理拦截器的前置拦截。 ha.handle：通过获取到的Adapter（这里是RequestMappingHandlerAdapter）执行Controller方法，得到ModelAndView processDispatchResult：直接调用render方法，将返回的ModelAndView交给ViewResolver视图解析器找到指定的视图（view），赋值给response返回给客户端显示。 RequestMappingHandlerAdapter::handleInternal123456789101112131415161718192021222324252627282930313233343536protected ModelAndView handleInternal(HttpServletRequest request, HttpServletResponse response, HandlerMethod handlerMethod) throws Exception &#123; ModelAndView mav; checkRequest(request); // Execute invokeHandlerMethod in synchronized block if required. if (this.synchronizeOnSession) &#123; HttpSession session = request.getSession(false); if (session != null) &#123; Object mutex = WebUtils.getSessionMutex(session); synchronized (mutex) &#123; mav = invokeHandlerMethod(request, response, handlerMethod); &#125; &#125; else &#123; // No HttpSession available -&gt; no mutex necessary mav = invokeHandlerMethod(request, response, handlerMethod); &#125; &#125; else &#123; // No synchronization on session demanded at all... mav = invokeHandlerMethod(request, response, handlerMethod); &#125; if (!response.containsHeader(HEADER_CACHE_CONTROL)) &#123; if (getSessionAttributesHandler(handlerMethod).hasSessionAttributes()) &#123; applyCacheSeconds(response, this.cacheSecondsForSessionAttributeHandlers); &#125; else &#123; prepareResponse(response); &#125; &#125; return mav;&#125; invokeHandlerMethod：执行Handler方法获取到ModelAndView，调用ServletInvocableHandlerMethod::invokeAndHandle ServletInvocableHandlerMethod::invokeAndHandle123456789101112131415161718192021222324252627282930public void invokeAndHandle(ServletWebRequest webRequest, ModelAndViewContainer mavContainer, Object... providedArgs) throws Exception &#123; Object returnValue = invokeForRequest(webRequest, mavContainer, providedArgs); setResponseStatus(webRequest); if (returnValue == null) &#123; if (isRequestNotModified(webRequest) || getResponseStatus() != null || mavContainer.isRequestHandled()) &#123; mavContainer.setRequestHandled(true); return; &#125; &#125; else if (StringUtils.hasText(getResponseStatusReason())) &#123; mavContainer.setRequestHandled(true); return; &#125; mavContainer.setRequestHandled(false); Assert.state(this.returnValueHandlers != null, "No return value handlers"); try &#123; this.returnValueHandlers.handleReturnValue( returnValue, getReturnValueType(returnValue), mavContainer, webRequest); &#125; catch (Exception ex) &#123; if (logger.isTraceEnabled()) &#123; logger.trace(formatErrorForReturnValue(returnValue), ex); &#125; throw ex; &#125;&#125; invokeForRequest: 调用InvocableHandlerMethod::doInvoke 通过反射的方式执行requestMapping对应的Controller方法 this.returnValueHandlers.handleReturnValue：将执行完的returnValue设置到response中，我们常用的json返回值就是在这里设置的。 总结通过对源码的分析我们了解了请求在Tomcat中的流转情况，以及SpringMVC如何完成一个请求的处理。一个请求在SpringMVC中的生命周期如下图所示： 1、用户发送请求，浏览器acceptor监听到请求，投递至PollerEvent队列，Poller进行消费，交给SocketProcessor（DispatcherServlet）处理。 2、分发器DispatcherServlet向映射器HandlerMapping发请求，根据requestURL获取Handler 3、映射器返回HandlerExecutionChain，包含了需要处理的Handler及HandlerInterceptor拦截器 4、根据Handler获取对应的适配器 5、适配器执行具体的Handler（即Controller的方法） 6、Handler执行完返回ModelAndModel 7、HandlerAdapter将ModelAndModel返回给分发器DispatcherServlet 8、DispatcherServlet将ModelAndView传给ViewReslover视图解析器 9、ViewReslover解析后返回具体的View模板 10、渲染视图，将数据填充至视图中 11、将渲染好的视图返回给用户，在浏览器客户端展现 参考：https://www.cnblogs.com/xiaoxi/p/6164383.html]]></content>
      <tags>
        <tag>Java</tag>
        <tag>SpringBoot</tag>
        <tag>Spring</tag>
        <tag>Tomcat</tag>
        <tag>SpringMvc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【从入门到放弃-SpringBoot】SpringBoot源码分析-WebServer]]></title>
    <url>%2F9707c128.html</url>
    <content type="text"><![CDATA[前言前文【从入门到放弃-SpringBoot】SpringBoot源码分析-启动中，我们分析了springboot的启动过程，在refreshContext中调用了onRefresh。在SERVLET类型应用中，实际实例化的应用上下文为ServletWebServerApplicationContext。其onRefresh中调用了createWebServer。我们在本文中一起分析下是一个web应用是如何启动的。 ServletWebServerApplicationContext::createWebServer123456789101112131415161718private void createWebServer() &#123; WebServer webServer = this.webServer; ServletContext servletContext = getServletContext(); if (webServer == null &amp;&amp; servletContext == null) &#123; ServletWebServerFactory factory = getWebServerFactory(); this.webServer = factory.getWebServer(getSelfInitializer()); &#125; else if (servletContext != null) &#123; try &#123; getSelfInitializer().onStartup(servletContext); &#125; catch (ServletException ex) &#123; throw new ApplicationContextException("Cannot initialize servlet context", ex); &#125; &#125; initPropertySources(); &#125; getWebServerFactory1234567891011121314151617protected ServletWebServerFactory getWebServerFactory() &#123; // Use bean names so that we don't consider the hierarchy String[] beanNames = getBeanFactory() .getBeanNamesForType(ServletWebServerFactory.class); if (beanNames.length == 0) &#123; throw new ApplicationContextException( "Unable to start ServletWebServerApplicationContext due to missing " + "ServletWebServerFactory bean."); &#125; if (beanNames.length &gt; 1) &#123; throw new ApplicationContextException( "Unable to start ServletWebServerApplicationContext due to multiple " + "ServletWebServerFactory beans : " + StringUtils.arrayToCommaDelimitedString(beanNames)); &#125; return getBeanFactory().getBean(beanNames[0], ServletWebServerFactory.class);&#125; 根据ServletWebServerFactory获取此类型的bean，默认获取的是TomcatServletWebServerFactory，是在ServletWebServerFactoryConfiguration中根据条件加载的。可以通过排除引入Tomcat包等方式切换使用Jetty或Undertow ServletWebServerApplicationContext::selfInitialize123456789private void selfInitialize(ServletContext servletContext) throws ServletException &#123; prepareWebApplicationContext(servletContext); registerApplicationScope(servletContext); WebApplicationContextUtils.registerEnvironmentBeans(getBeanFactory(), servletContext); for (ServletContextInitializer beans : getServletContextInitializerBeans()) &#123; beans.onStartup(servletContext); &#125;&#125; 配置servlet启动相关的filter、listener、属性、配置等各项bean。在server启动的时候一起启动 TomcatServletWebServerFactory::getWebServer1234567891011121314151617public WebServer getWebServer(ServletContextInitializer... initializers) &#123; Tomcat tomcat = new Tomcat(); File baseDir = (this.baseDirectory != null) ? this.baseDirectory : createTempDir("tomcat"); tomcat.setBaseDir(baseDir.getAbsolutePath()); Connector connector = new Connector(this.protocol); tomcat.getService().addConnector(connector); customizeConnector(connector); tomcat.setConnector(connector); tomcat.getHost().setAutoDeploy(false); configureEngine(tomcat.getEngine()); for (Connector additionalConnector : this.additionalTomcatConnectors) &#123; tomcat.getService().addConnector(additionalConnector); &#125; prepareContext(tomcat.getHost(), initializers); return getTomcatWebServer(tomcat);&#125; 实际上就是启动了一个Tomcat作为webserver，为了便于理解启动过程，可以先简单了解下Tomcat的架构设计，如下图 Servertomcat的顶级结构，Tomcat的所有动作都是运行在server中，一个JVM进程中只能启动一个server实例，负责管理整个Service的生命周期。 Service一个server可以创建多个service，但通常只创建一个。由service对外提供服务一个service由一个Container和多个Connector构成。通过Connector接收请求交给Container处理 Connector用于接收请求并封装成request和response的模块。底层用socket进行连接，可以在初始化时，自定义选择使用的协议如http、ajp等 Container是基础容器接口类，它的子类有Engine、Host、Context和Wrapper，从左到右依次是一对多的父子关系。 Engine是一个顶级容器，可以包含一个或多个Host，接收到Connector转发的请求后，根据请求头信息找到需要处理的Host，交给它处理 Host虚拟主机，接收对应host的请求发给context进行处理 Contentweb应用上下文，根据请求找到对应的servlet类 Warpper管理servlet实例，负责其装载、初始化、执行、回收等。一个warpper对应一个servlet实例 prepareContext12345678910111213141516171819202122232425262728293031323334353637protected void prepareContext(Host host, ServletContextInitializer[] initializers) &#123; File documentRoot = getValidDocumentRoot(); TomcatEmbeddedContext context = new TomcatEmbeddedContext(); if (documentRoot != null) &#123; context.setResources(new LoaderHidingResourceRoot(context)); &#125; context.setName(getContextPath()); context.setDisplayName(getDisplayName()); context.setPath(getContextPath()); File docBase = (documentRoot != null) ? documentRoot : createTempDir("tomcat-docbase"); context.setDocBase(docBase.getAbsolutePath()); context.addLifecycleListener(new FixContextListener()); context.setParentClassLoader( (this.resourceLoader != null) ? this.resourceLoader.getClassLoader() : ClassUtils.getDefaultClassLoader()); resetDefaultLocaleMapping(context); addLocaleMappings(context); context.setUseRelativeRedirects(false); configureTldSkipPatterns(context); WebappLoader loader = new WebappLoader(context.getParentClassLoader()); loader.setLoaderClass(TomcatEmbeddedWebappClassLoader.class.getName()); loader.setDelegate(true); context.setLoader(loader); if (isRegisterDefaultServlet()) &#123; addDefaultServlet(context); &#125; if (shouldRegisterJspServlet()) &#123; addJspServlet(context); addJasperInitializer(context); &#125; context.addLifecycleListener(new StaticResourceConfigurer(context)); ServletContextInitializer[] initializersToUse = mergeInitializers(initializers); host.addChild(context); configureContext(context, initializersToUse); postProcessContext(context);&#125; 初始化配置host 设置父加载器 设置默认的servlet 设置jsp的servlet（默认是空） 将Content添加至Host configureContext：配置Content的默认错误页面、MimeMap、session等内容 TomcatWebServer::initialize12345678910111213141516171819202122232425262728293031323334353637383940private void initialize() throws WebServerException &#123; logger.info("Tomcat initialized with port(s): " + getPortsDescription(false)); synchronized (this.monitor) &#123; try &#123; addInstanceIdToEngineName(); Context context = findContext(); context.addLifecycleListener((event) -&gt; &#123; if (context.equals(event.getSource()) &amp;&amp; Lifecycle.START_EVENT.equals(event.getType())) &#123; // Remove service connectors so that protocol binding doesn't // happen when the service is started. removeServiceConnectors(); &#125; &#125;); // Start the server to trigger initialization listeners this.tomcat.start(); // We can re-throw failure exception directly in the main thread rethrowDeferredStartupExceptions(); try &#123; ContextBindings.bindClassLoader(context, context.getNamingToken(), getClass().getClassLoader()); &#125; catch (NamingException ex) &#123; // Naming is not enabled. Continue &#125; // Unlike Jetty, all Tomcat threads are daemon threads. We create a // blocking non-daemon to stop immediate shutdown startDaemonAwaitThread(); &#125; catch (Exception ex) &#123; stopSilently(); throw new WebServerException("Unable to start embedded Tomcat", ex); &#125; &#125;&#125; 获取service绑定的Connector,保存后删除，这是删除的原因是：在下面start后，Connector就能接收请求了，但还service还未启动，因此先删除Connector达到延后启动的效果 启动Server/Service/Engine/Host/Context/Wrapper各级容器 之前配置的各项filter、listener等也都在此时启动 TomcatWebServer::startDaemonAwaitThread12345678910111213private void startDaemonAwaitThread() &#123; Thread awaitThread = new Thread("container-" + (containerCounter.get())) &#123; @Override public void run() &#123; TomcatWebServer.this.tomcat.getServer().await(); &#125; &#125;; awaitThread.setContextClassLoader(getClass().getClassLoader()); awaitThread.setDaemon(false); awaitThread.start();&#125; jvm虚拟机在所有的线程都是守护线程时，就会退出。因此需要创建一个用户线程（非守护线程），一直处于监听状态，当接收到关闭信号时，用户线程退出。仅剩下全部守护线程，整个jvm关闭。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127public void await() &#123; // Negative values - don't wait on port - tomcat is embedded or we just don't like ports if (getPortWithOffset() == -2) &#123; // undocumented yet - for embedding apps that are around, alive. return; &#125; if (getPortWithOffset() == -1) &#123; try &#123; awaitThread = Thread.currentThread(); while(!stopAwait) &#123; try &#123; Thread.sleep( 10000 ); &#125; catch( InterruptedException ex ) &#123; // continue and check the flag &#125; &#125; &#125; finally &#123; awaitThread = null; &#125; return; &#125; // Set up a server socket to wait on try &#123; awaitSocket = new ServerSocket(getPortWithOffset(), 1, InetAddress.getByName(address)); &#125; catch (IOException e) &#123; log.error(sm.getString("standardServer.awaitSocket.fail", address, String.valueOf(getPortWithOffset()), String.valueOf(getPort()), String.valueOf(getPortOffset())), e); return; &#125; try &#123; awaitThread = Thread.currentThread(); // Loop waiting for a connection and a valid command while (!stopAwait) &#123; ServerSocket serverSocket = awaitSocket; if (serverSocket == null) &#123; break; &#125; // Wait for the next connection Socket socket = null; StringBuilder command = new StringBuilder(); try &#123; InputStream stream; long acceptStartTime = System.currentTimeMillis(); try &#123; socket = serverSocket.accept(); socket.setSoTimeout(10 * 1000); // Ten seconds stream = socket.getInputStream(); &#125; catch (SocketTimeoutException ste) &#123; // This should never happen but bug 56684 suggests that // it does. log.warn(sm.getString("standardServer.accept.timeout", Long.valueOf(System.currentTimeMillis() - acceptStartTime)), ste); continue; &#125; catch (AccessControlException ace) &#123; log.warn(sm.getString("standardServer.accept.security"), ace); continue; &#125; catch (IOException e) &#123; if (stopAwait) &#123; // Wait was aborted with socket.close() break; &#125; log.error(sm.getString("standardServer.accept.error"), e); break; &#125; // Read a set of characters from the socket int expected = 1024; // Cut off to avoid DoS attack while (expected &lt; shutdown.length()) &#123; if (random == null) random = new Random(); expected += (random.nextInt() % 1024); &#125; while (expected &gt; 0) &#123; int ch = -1; try &#123; ch = stream.read(); &#125; catch (IOException e) &#123; log.warn(sm.getString("standardServer.accept.readError"), e); ch = -1; &#125; // Control character or EOF (-1) terminates loop if (ch &lt; 32 || ch == 127) &#123; break; &#125; command.append((char) ch); expected--; &#125; &#125; finally &#123; // Close the socket now that we are done with it try &#123; if (socket != null) &#123; socket.close(); &#125; &#125; catch (IOException e) &#123; // Ignore &#125; &#125; // Match against our command string boolean match = command.toString().equals(shutdown); if (match) &#123; log.info(sm.getString("standardServer.shutdownViaPort")); break; &#125; else log.warn(sm.getString("standardServer.invalidShutdownCommand", command.toString())); &#125; &#125; finally &#123; ServerSocket serverSocket = awaitSocket; awaitThread = null; awaitSocket = null; // Close the server socket and return if (serverSocket != null) &#123; try &#123; serverSocket.close(); &#125; catch (IOException e) &#123; // Ignore &#125; &#125; &#125;&#125; 用户线程创建一个socket，持续监听Tomcat状态，每十秒进行一次检测，如果接收到关闭信号则此线程关闭，整个Tomcat关闭。 TomcatWebServer::start12345678910111213141516171819202122232425262728293031public void start() throws WebServerException &#123; synchronized (this.monitor) &#123; if (this.started) &#123; return; &#125; try &#123; addPreviouslyRemovedConnectors(); Connector connector = this.tomcat.getConnector(); if (connector != null &amp;&amp; this.autoStart) &#123; performDeferredLoadOnStartup(); &#125; checkThatConnectorsHaveStarted(); this.started = true; logger.info("Tomcat started on port(s): " + getPortsDescription(true) + " with context path '" + getContextPath() + "'"); &#125; catch (ConnectorStartFailedException ex) &#123; stopSilently(); throw ex; &#125; catch (Exception ex) &#123; throw new WebServerException("Unable to start embedded Tomcat server", ex); &#125; finally &#123; Context context = findContext(); ContextBindings.unbindClassLoader(context, context.getNamingToken(), getClass().getClassLoader()); &#125; &#125;&#125; 还记得【从入门到放弃-SpringBoot】SpringBoot源码分析-启动中的finishRefresh吗？ServletWebServerApplicationContext重写了finishRefresh方法，在里面增加调用了startWebServer。 如上将之前删除并保存的connector添加回来并启动。 创建socket并绑定监听的端口（默认8080） 创建线程池开始接收并处理请求。 总结至此，springboot中webserver的启动过程我们已经大概清楚了，Tomcat还有很深的内容可以挖，下面可以学习下。]]></content>
      <categories>
        <category>个人学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>SpringBoot</tag>
        <tag>Spring</tag>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【从入门到放弃-SpringBoot】SpringBoot源码分析-启动]]></title>
    <url>%2F97473183.html</url>
    <content type="text"><![CDATA[前言上一篇我们一起简单了解了【从入门到放弃-MySQL】数据库连接过程分析-客户端，写完之后通读一遍，感觉分析的不是很透彻。有很多地方都没搞通，因此决定从Springboot源码开始从头研究下。 main 入口分析12345678910111213141516171819package com.springboot.demo;import org.mybatis.spring.annotation.MapperScan;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;import org.springframework.scheduling.annotation.EnableScheduling;@MapperScan("com.springboot.demo.repository.dao")@SpringBootApplication@EnableSchedulingpublic class DemoApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(DemoApplication.class, args); &#125;&#125; 一个简单的springboot项目启动文件中 一行代码就能搞定。我们从这一行代码开始看起。 SpringApplication::SpringApplication12345678910public SpringApplication(ResourceLoader resourceLoader, Class&lt;?&gt;... primarySources) &#123; this.resourceLoader = resourceLoader; Assert.notNull(primarySources, "PrimarySources must not be null"); this.primarySources = new LinkedHashSet&lt;&gt;(Arrays.asList(primarySources)); this.webApplicationType = WebApplicationType.deduceFromClasspath(); setInitializers((Collection) getSpringFactoriesInstances( ApplicationContextInitializer.class)); setListeners((Collection) getSpringFactoriesInstances(ApplicationListener.class)); this.mainApplicationClass = deduceMainApplicationClass();&#125; 主要是初始化成员变量，比如参数列表，应用类型、监听器等。 getSpringFactoriesInstances1234567891011private &lt;T&gt; Collection&lt;T&gt; getSpringFactoriesInstances(Class&lt;T&gt; type, Class&lt;?&gt;[] parameterTypes, Object... args) &#123; ClassLoader classLoader = getClassLoader(); // Use names and ensure unique to protect against duplicates Set&lt;String&gt; names = new LinkedHashSet&lt;&gt;( SpringFactoriesLoader.loadFactoryNames(type, classLoader)); List&lt;T&gt; instances = createSpringFactoriesInstances(type, parameterTypes, classLoader, args, names); AnnotationAwareOrderComparator.sort(instances); return instances;&#125; 这个方法主要是根据type类型获取配置中默认的类列表，并进行初始化。 loadFactoryNames：从META-INF/spring.factories中获取type对应的配置类名称列表。 createSpringFactoriesInstances：校验获取到的类是否是parameterTypes类的子类，并将获取到的类通过反射机制实例化， SpringApplication::run123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/** * Run the Spring application, creating and refreshing a new * &#123;@link ApplicationContext&#125;. * @param args the application arguments (usually passed from a Java main method) * @return a running &#123;@link ApplicationContext&#125; */public ConfigurableApplicationContext run(String... args) &#123; StopWatch stopWatch = new StopWatch(); stopWatch.start(); ConfigurableApplicationContext context = null; Collection&lt;SpringBootExceptionReporter&gt; exceptionReporters = new ArrayList&lt;&gt;(); configureHeadlessProperty(); SpringApplicationRunListeners listeners = getRunListeners(args); listeners.starting(); try &#123; ApplicationArguments applicationArguments = new DefaultApplicationArguments( args); ConfigurableEnvironment environment = prepareEnvironment(listeners, applicationArguments); configureIgnoreBeanInfo(environment); Banner printedBanner = printBanner(environment); context = createApplicationContext(); exceptionReporters = getSpringFactoriesInstances( SpringBootExceptionReporter.class, new Class[] &#123; ConfigurableApplicationContext.class &#125;, context); prepareContext(context, environment, listeners, applicationArguments, printedBanner); refreshContext(context); afterRefresh(context, applicationArguments); stopWatch.stop(); if (this.logStartupInfo) &#123; new StartupInfoLogger(this.mainApplicationClass) .logStarted(getApplicationLog(), stopWatch); &#125; listeners.started(context); callRunners(context, applicationArguments); &#125; catch (Throwable ex) &#123; handleRunFailure(context, ex, exceptionReporters, listeners); throw new IllegalStateException(ex); &#125; try &#123; listeners.running(context); &#125; catch (Throwable ex) &#123; handleRunFailure(context, ex, exceptionReporters, null); throw new IllegalStateException(ex); &#125; return context;&#125; StopWatch是一个计时器工具类，这里先记录了项目的启动时间 变量初始化各种变量初始化，用法在后面具体分析 configureHeadlessProperty设置系统java.awt.headless属性，为true的话是告知系统不要指望显示器、鼠标、键盘等可以正常运行，这是一个服务端程序，用到这些外设的时候需要靠自己模拟 getRunListeners获取SpringApplicationRunListeners各项监听器并实例化，这些监听器配置在spring.factories资源文件中。 prepareEnvironment配置环境，如配置文件、系统变量等并通过监听器广播环境变量准备完毕事件。 printBanner打印启动显示的banner。 createApplicationContext根据webApplicationType初始化spring上下文 exceptionReporters初始化配置的异常报告类 prepareContext123456789101112131415161718192021222324252627private void prepareContext(ConfigurableApplicationContext context, ConfigurableEnvironment environment, SpringApplicationRunListeners listeners, ApplicationArguments applicationArguments, Banner printedBanner) &#123; context.setEnvironment(environment); postProcessApplicationContext(context); applyInitializers(context); listeners.contextPrepared(context); if (this.logStartupInfo) &#123; logStartupInfo(context.getParent() == null); logStartupProfileInfo(context); &#125; // Add boot specific singleton beans ConfigurableListableBeanFactory beanFactory = context.getBeanFactory(); beanFactory.registerSingleton("springApplicationArguments", applicationArguments); if (printedBanner != null) &#123; beanFactory.registerSingleton("springBootBanner", printedBanner); &#125; if (beanFactory instanceof DefaultListableBeanFactory) &#123; ((DefaultListableBeanFactory) beanFactory) .setAllowBeanDefinitionOverriding(this.allowBeanDefinitionOverriding); &#125; // Load the sources Set&lt;Object&gt; sources = getAllSources(); Assert.notEmpty(sources, "Sources must not be empty"); load(context, sources.toArray(new Object[0])); listeners.contextLoaded(context);&#125; setEnvironment：设置上下文环境 postProcessApplicationContext：上下文后置处理，默认没做什么事。 applyInitializers：调用之前实例化的几个ApplicationContextInitializer类的initialize方法 contextPrepared：向listeners发送上下文已准备完毕的通知。 load：BeanDefinitionLoader可以加载各种bean，比如注解、XML、package等多种类型的bean，在后面利用BeanDefinitionLoader将这些beans都加载进上下文中。 contextLoaded：向listeners发送上下文已加载完毕的通知。 refreshContext12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364@Overridepublic void refresh() throws BeansException, IllegalStateException &#123; synchronized (this.startupShutdownMonitor) &#123; // Prepare this context for refreshing. prepareRefresh(); // Tell the subclass to refresh the internal bean factory. ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // Prepare the bean factory for use in this context. prepareBeanFactory(beanFactory); try &#123; // Allows post-processing of the bean factory in context subclasses. postProcessBeanFactory(beanFactory); // Invoke factory processors registered as beans in the context. invokeBeanFactoryPostProcessors(beanFactory); // Register bean processors that intercept bean creation. registerBeanPostProcessors(beanFactory); // Initialize message source for this context. initMessageSource(); // Initialize event multicaster for this context. initApplicationEventMulticaster(); // Initialize other special beans in specific context subclasses. onRefresh(); // Check for listener beans and register them. registerListeners(); // Instantiate all remaining (non-lazy-init) singletons. finishBeanFactoryInitialization(beanFactory); // Last step: publish corresponding event. finishRefresh(); &#125; catch (BeansException ex) &#123; if (logger.isWarnEnabled()) &#123; logger.warn("Exception encountered during context initialization - " + "cancelling refresh attempt: " + ex); &#125; // Destroy already created singletons to avoid dangling resources. destroyBeans(); // Reset 'active' flag. cancelRefresh(ex); // Propagate exception to caller. throw ex; &#125; finally &#123; // Reset common introspection caches in Spring's core, since we // might not ever need metadata for singleton beans anymore... resetCommonCaches(); &#125; &#125;&#125; prepareRefresh：一些准备工作，比如设置启动时间、初始化资源、必须属性校验等。 obtainFreshBeanFactory：通知子类刷新内置的beanFactory prepareBeanFactory：对容器的beanFactory做一些准备工作，比如设置classloader、设置&amp;取消设置一些类的bean invokeBeanFactoryPostProcessors： 主要看invokeBeanDefinitionRegistryPostProcessors方法，会调用ConfigurationClassPostProcessor::processConfigBeanDefinitions。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102public void processConfigBeanDefinitions(BeanDefinitionRegistry registry) &#123; List&lt;BeanDefinitionHolder&gt; configCandidates = new ArrayList&lt;&gt;(); String[] candidateNames = registry.getBeanDefinitionNames(); for (String beanName : candidateNames) &#123; BeanDefinition beanDef = registry.getBeanDefinition(beanName); if (ConfigurationClassUtils.isFullConfigurationClass(beanDef) || ConfigurationClassUtils.isLiteConfigurationClass(beanDef)) &#123; if (logger.isDebugEnabled()) &#123; logger.debug("Bean definition has already been processed as a configuration class: " + beanDef); &#125; &#125; else if (ConfigurationClassUtils.checkConfigurationClassCandidate(beanDef, this.metadataReaderFactory)) &#123; configCandidates.add(new BeanDefinitionHolder(beanDef, beanName)); &#125; &#125; // Return immediately if no @Configuration classes were found if (configCandidates.isEmpty()) &#123; return; &#125; // Sort by previously determined @Order value, if applicable configCandidates.sort((bd1, bd2) -&gt; &#123; int i1 = ConfigurationClassUtils.getOrder(bd1.getBeanDefinition()); int i2 = ConfigurationClassUtils.getOrder(bd2.getBeanDefinition()); return Integer.compare(i1, i2); &#125;); // Detect any custom bean name generation strategy supplied through the enclosing application context SingletonBeanRegistry sbr = null; if (registry instanceof SingletonBeanRegistry) &#123; sbr = (SingletonBeanRegistry) registry; if (!this.localBeanNameGeneratorSet) &#123; BeanNameGenerator generator = (BeanNameGenerator) sbr.getSingleton(CONFIGURATION_BEAN_NAME_GENERATOR); if (generator != null) &#123; this.componentScanBeanNameGenerator = generator; this.importBeanNameGenerator = generator; &#125; &#125; &#125; if (this.environment == null) &#123; this.environment = new StandardEnvironment(); &#125; // Parse each @Configuration class ConfigurationClassParser parser = new ConfigurationClassParser( this.metadataReaderFactory, this.problemReporter, this.environment, this.resourceLoader, this.componentScanBeanNameGenerator, registry); Set&lt;BeanDefinitionHolder&gt; candidates = new LinkedHashSet&lt;&gt;(configCandidates); Set&lt;ConfigurationClass&gt; alreadyParsed = new HashSet&lt;&gt;(configCandidates.size()); do &#123; parser.parse(candidates); parser.validate(); Set&lt;ConfigurationClass&gt; configClasses = new LinkedHashSet&lt;&gt;(parser.getConfigurationClasses()); configClasses.removeAll(alreadyParsed); // Read the model and create bean definitions based on its content if (this.reader == null) &#123; this.reader = new ConfigurationClassBeanDefinitionReader( registry, this.sourceExtractor, this.resourceLoader, this.environment, this.importBeanNameGenerator, parser.getImportRegistry()); &#125; this.reader.loadBeanDefinitions(configClasses); alreadyParsed.addAll(configClasses); candidates.clear(); if (registry.getBeanDefinitionCount() &gt; candidateNames.length) &#123; String[] newCandidateNames = registry.getBeanDefinitionNames(); Set&lt;String&gt; oldCandidateNames = new HashSet&lt;&gt;(Arrays.asList(candidateNames)); Set&lt;String&gt; alreadyParsedClasses = new HashSet&lt;&gt;(); for (ConfigurationClass configurationClass : alreadyParsed) &#123; alreadyParsedClasses.add(configurationClass.getMetadata().getClassName()); &#125; for (String candidateName : newCandidateNames) &#123; if (!oldCandidateNames.contains(candidateName)) &#123; BeanDefinition bd = registry.getBeanDefinition(candidateName); if (ConfigurationClassUtils.checkConfigurationClassCandidate(bd, this.metadataReaderFactory) &amp;&amp; !alreadyParsedClasses.contains(bd.getBeanClassName())) &#123; candidates.add(new BeanDefinitionHolder(bd, candidateName)); &#125; &#125; &#125; candidateNames = newCandidateNames; &#125; &#125; while (!candidates.isEmpty()); // Register the ImportRegistry as a bean in order to support ImportAware @Configuration classes if (sbr != null &amp;&amp; !sbr.containsSingleton(IMPORT_REGISTRY_BEAN_NAME)) &#123; sbr.registerSingleton(IMPORT_REGISTRY_BEAN_NAME, parser.getImportRegistry()); &#125; if (this.metadataReaderFactory instanceof CachingMetadataReaderFactory) &#123; // Clear cache in externally provided MetadataReaderFactory; this is a no-op // for a shared cache since it'll be cleared by the ApplicationContext. ((CachingMetadataReaderFactory) this.metadataReaderFactory).clearCache(); &#125;&#125; ConfigurationClassParser：处理@Configuration/@Component等注解，扫描、注册包下的类 ConfigurationClassBeanDefinitionReader：处理@Import/@ImportResource/@Bean等注解。 将所有扫描到的bean都装载至registry invokeBeanDefinitionRegistryPostProcessors：执行invokeBeanDefinitionRegistryPostProcessors回调 invokeBeanFactoryPostProcessors：执行invokeBeanFactoryPostProcessors回调 registerBeanPostProcessors：注册拦截创建bean的bean处理器 initMessageSource：初始化消息源 initApplicationEventMulticaster：初始化事件广播 onRefresh：对一些特殊子类上下文中初始化一些特殊的bean，比如在ServletWebServerApplicationContext中就做了createWebServer的操作 registerListeners：注册bean监听器 finishBeanFactoryInitialization：实例化所有单例bean，比如我们之前分析的【从入门到放弃-MySQL】数据库连接过程分析-客户端dataSource就是在这一步实例化的。 finishRefresh结束上下文更新，并发布事件。 callRunners如果有ApplicationRunner或者CommandLineRunner类型的bean，则触发run函数，启动任务。 总结至此，springboot就已经启动完毕。概述下主要的启动过程就是 初始化环境 初始化默认配置 初始化各类监听器、事件 创建上下文 在上下文中添加默认的bean 扫描文件、注解等各种类型的bean添加在上下文中 实例化各个bean 启动完毕]]></content>
      <categories>
        <category>个人学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>SpringBoot</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【从入门到放弃-MySQL】数据库连接过程分析-客户端]]></title>
    <url>%2F2dcf5558.html</url>
    <content type="text"><![CDATA[前言上文分析了【从入门到放弃-MySQL】数据库连接过程分析。本文我们一起来追一下客户端和服务端建联的过程。这里客户端使用JDBC8.0，在SpringBoot2.1.3下验证。 请求流程初始化SpringBoot2.1.3默认使用的HickriCP连接池 应用启动时，会先注册spring.datasource.driver-class-name配置的驱动，这里我们使用com.mysql.cj.jdbc.Driver 启动后，我们直接通过一个查询操作的http请求来验证一次查询操作中，客户端与服务端连接的过程 首次请求当使用到Dao请求时，开始建立连接调用堆栈如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101checkErrorMessage:752, NativeProtocol (com.mysql.cj.protocol.a)checkErrorMessage:741, NativeProtocol (com.mysql.cj.protocol.a)checkErrorMessage:709, NativeProtocol (com.mysql.cj.protocol.a)checkErrorMessage:132, NativeProtocol (com.mysql.cj.protocol.a)proceedHandshakeWithPluggableAuthentication:540, NativeAuthenticationProvider (com.mysql.cj.protocol.a)connect:202, NativeAuthenticationProvider (com.mysql.cj.protocol.a)connect:1449, NativeProtocol (com.mysql.cj.protocol.a)connect:165, NativeSession (com.mysql.cj)connectOneTryOnly:955, ConnectionImpl (com.mysql.cj.jdbc)createNewIO:825, ConnectionImpl (com.mysql.cj.jdbc)&lt;init&gt;:455, ConnectionImpl (com.mysql.cj.jdbc)getInstance:240, ConnectionImpl (com.mysql.cj.jdbc)connect:199, NonRegisteringDriver (com.mysql.cj.jdbc)getConnection:136, DriverDataSource (com.zaxxer.hikari.util)newConnection:369, PoolBase (com.zaxxer.hikari.pool)newPoolEntry:198, PoolBase (com.zaxxer.hikari.pool)createPoolEntry:467, HikariPool (com.zaxxer.hikari.pool)checkFailFast:541, HikariPool (com.zaxxer.hikari.pool)&lt;init&gt;:115, HikariPool (com.zaxxer.hikari.pool)getConnection:112, HikariDataSource (com.zaxxer.hikari)fetchConnection:157, DataSourceUtils (org.springframework.jdbc.datasource)doGetConnection:115, DataSourceUtils (org.springframework.jdbc.datasource)getConnection:78, DataSourceUtils (org.springframework.jdbc.datasource)openConnection:82, SpringManagedTransaction (org.mybatis.spring.transaction)getConnection:68, SpringManagedTransaction (org.mybatis.spring.transaction)getConnection:336, BaseExecutor (org.apache.ibatis.executor)prepareStatement:85, SimpleExecutor (org.apache.ibatis.executor)doQuery:62, SimpleExecutor (org.apache.ibatis.executor)queryFromDatabase:324, BaseExecutor (org.apache.ibatis.executor)query:156, BaseExecutor (org.apache.ibatis.executor)query:109, CachingExecutor (org.apache.ibatis.executor)query:83, CachingExecutor (org.apache.ibatis.executor)selectList:148, DefaultSqlSession (org.apache.ibatis.session.defaults)selectList:141, DefaultSqlSession (org.apache.ibatis.session.defaults)invoke0:-1, NativeMethodAccessorImpl (sun.reflect)invoke:62, NativeMethodAccessorImpl (sun.reflect)invoke:43, DelegatingMethodAccessorImpl (sun.reflect)invoke:498, Method (java.lang.reflect)invoke:433, SqlSessionTemplate$SqlSessionInterceptor (org.mybatis.spring)selectList:-1, $Proxy59 (com.sun.proxy)selectList:230, SqlSessionTemplate (org.mybatis.spring)executeForMany:144, MapperMethod (org.apache.ibatis.binding)execute:77, MapperMethod (org.apache.ibatis.binding)invoke:58, MapperProxy (org.apache.ibatis.binding)selectByCondition:-1, $Proxy60 (com.sun.proxy)getUserByCondition:40, UsersServiceImpl (com.springboot.demo.service.impl)getUsersByCondition:74, IndexController (com.springboot.demo.controller)invoke0:-1, NativeMethodAccessorImpl (sun.reflect)invoke:62, NativeMethodAccessorImpl (sun.reflect)invoke:43, DelegatingMethodAccessorImpl (sun.reflect)invoke:498, Method (java.lang.reflect)doInvoke:189, InvocableHandlerMethod (org.springframework.web.method.support)invokeForRequest:138, InvocableHandlerMethod (org.springframework.web.method.support)invokeAndHandle:102, ServletInvocableHandlerMethod (org.springframework.web.servlet.mvc.method.annotation)invokeHandlerMethod:895, RequestMappingHandlerAdapter (org.springframework.web.servlet.mvc.method.annotation)handleInternal:800, RequestMappingHandlerAdapter (org.springframework.web.servlet.mvc.method.annotation)handle:87, AbstractHandlerMethodAdapter (org.springframework.web.servlet.mvc.method)doDispatch:1038, DispatcherServlet (org.springframework.web.servlet)doService:942, DispatcherServlet (org.springframework.web.servlet)processRequest:1005, FrameworkServlet (org.springframework.web.servlet)doGet:897, FrameworkServlet (org.springframework.web.servlet)service:634, HttpServlet (javax.servlet.http)service:882, FrameworkServlet (org.springframework.web.servlet)service:741, HttpServlet (javax.servlet.http)internalDoFilter:231, ApplicationFilterChain (org.apache.catalina.core)doFilter:166, ApplicationFilterChain (org.apache.catalina.core)doFilter:53, WsFilter (org.apache.tomcat.websocket.server)internalDoFilter:193, ApplicationFilterChain (org.apache.catalina.core)doFilter:166, ApplicationFilterChain (org.apache.catalina.core)doFilterInternal:99, RequestContextFilter (org.springframework.web.filter)doFilter:107, OncePerRequestFilter (org.springframework.web.filter)internalDoFilter:193, ApplicationFilterChain (org.apache.catalina.core)doFilter:166, ApplicationFilterChain (org.apache.catalina.core)doFilterInternal:92, FormContentFilter (org.springframework.web.filter)doFilter:107, OncePerRequestFilter (org.springframework.web.filter)internalDoFilter:193, ApplicationFilterChain (org.apache.catalina.core)doFilter:166, ApplicationFilterChain (org.apache.catalina.core)doFilterInternal:93, HiddenHttpMethodFilter (org.springframework.web.filter)doFilter:107, OncePerRequestFilter (org.springframework.web.filter)internalDoFilter:193, ApplicationFilterChain (org.apache.catalina.core)doFilter:166, ApplicationFilterChain (org.apache.catalina.core)doFilterInternal:200, CharacterEncodingFilter (org.springframework.web.filter)doFilter:107, OncePerRequestFilter (org.springframework.web.filter)internalDoFilter:193, ApplicationFilterChain (org.apache.catalina.core)doFilter:166, ApplicationFilterChain (org.apache.catalina.core)invoke:200, StandardWrapperValve (org.apache.catalina.core)invoke:96, StandardContextValve (org.apache.catalina.core)invoke:490, AuthenticatorBase (org.apache.catalina.authenticator)invoke:139, StandardHostValve (org.apache.catalina.core)invoke:92, ErrorReportValve (org.apache.catalina.valves)invoke:74, StandardEngineValve (org.apache.catalina.core)service:343, CoyoteAdapter (org.apache.catalina.connector)service:408, Http11Processor (org.apache.coyote.http11)process:66, AbstractProcessorLight (org.apache.coyote)process:834, AbstractProtocol$ConnectionHandler (org.apache.coyote)doRun:1415, NioEndpoint$SocketProcessor (org.apache.tomcat.util.net)run:49, SocketProcessorBase (org.apache.tomcat.util.net)runWorker:1149, ThreadPoolExecutor (java.util.concurrent)run:624, ThreadPoolExecutor$Worker (java.util.concurrent)run:61, TaskThread$WrappingRunnable (org.apache.tomcat.util.threads)run:748, Thread (java.lang) 再次请求连接建立后，再次请求的调用栈 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485createProxyConnection:97, PoolEntry (com.zaxxer.hikari.pool)getConnection:185, HikariPool (com.zaxxer.hikari.pool)getConnection:155, HikariPool (com.zaxxer.hikari.pool)getConnection:128, HikariDataSource (com.zaxxer.hikari)fetchConnection:157, DataSourceUtils (org.springframework.jdbc.datasource)doGetConnection:115, DataSourceUtils (org.springframework.jdbc.datasource)getConnection:78, DataSourceUtils (org.springframework.jdbc.datasource)openConnection:82, SpringManagedTransaction (org.mybatis.spring.transaction)getConnection:68, SpringManagedTransaction (org.mybatis.spring.transaction)getConnection:336, BaseExecutor (org.apache.ibatis.executor)prepareStatement:85, SimpleExecutor (org.apache.ibatis.executor)doQuery:62, SimpleExecutor (org.apache.ibatis.executor)queryFromDatabase:324, BaseExecutor (org.apache.ibatis.executor)query:156, BaseExecutor (org.apache.ibatis.executor)query:109, CachingExecutor (org.apache.ibatis.executor)query:83, CachingExecutor (org.apache.ibatis.executor)selectList:148, DefaultSqlSession (org.apache.ibatis.session.defaults)selectList:141, DefaultSqlSession (org.apache.ibatis.session.defaults)invoke0:-1, NativeMethodAccessorImpl (sun.reflect)invoke:62, NativeMethodAccessorImpl (sun.reflect)invoke:43, DelegatingMethodAccessorImpl (sun.reflect)invoke:498, Method (java.lang.reflect)invoke:433, SqlSessionTemplate$SqlSessionInterceptor (org.mybatis.spring)selectList:-1, $Proxy59 (com.sun.proxy)selectList:230, SqlSessionTemplate (org.mybatis.spring)executeForMany:144, MapperMethod (org.apache.ibatis.binding)execute:77, MapperMethod (org.apache.ibatis.binding)invoke:58, MapperProxy (org.apache.ibatis.binding)selectByCondition:-1, $Proxy60 (com.sun.proxy)getUserByCondition:40, UsersServiceImpl (com.springboot.demo.service.impl)getUsersByCondition:74, IndexController (com.springboot.demo.controller)invoke0:-1, NativeMethodAccessorImpl (sun.reflect)invoke:62, NativeMethodAccessorImpl (sun.reflect)invoke:43, DelegatingMethodAccessorImpl (sun.reflect)invoke:498, Method (java.lang.reflect)doInvoke:189, InvocableHandlerMethod (org.springframework.web.method.support)invokeForRequest:138, InvocableHandlerMethod (org.springframework.web.method.support)invokeAndHandle:102, ServletInvocableHandlerMethod (org.springframework.web.servlet.mvc.method.annotation)invokeHandlerMethod:895, RequestMappingHandlerAdapter (org.springframework.web.servlet.mvc.method.annotation)handleInternal:800, RequestMappingHandlerAdapter (org.springframework.web.servlet.mvc.method.annotation)handle:87, AbstractHandlerMethodAdapter (org.springframework.web.servlet.mvc.method)doDispatch:1038, DispatcherServlet (org.springframework.web.servlet)doService:942, DispatcherServlet (org.springframework.web.servlet)processRequest:1005, FrameworkServlet (org.springframework.web.servlet)doGet:897, FrameworkServlet (org.springframework.web.servlet)service:634, HttpServlet (javax.servlet.http)service:882, FrameworkServlet (org.springframework.web.servlet)service:741, HttpServlet (javax.servlet.http)internalDoFilter:231, ApplicationFilterChain (org.apache.catalina.core)doFilter:166, ApplicationFilterChain (org.apache.catalina.core)doFilter:53, WsFilter (org.apache.tomcat.websocket.server)internalDoFilter:193, ApplicationFilterChain (org.apache.catalina.core)doFilter:166, ApplicationFilterChain (org.apache.catalina.core)doFilterInternal:99, RequestContextFilter (org.springframework.web.filter)doFilter:107, OncePerRequestFilter (org.springframework.web.filter)internalDoFilter:193, ApplicationFilterChain (org.apache.catalina.core)doFilter:166, ApplicationFilterChain (org.apache.catalina.core)doFilterInternal:92, FormContentFilter (org.springframework.web.filter)doFilter:107, OncePerRequestFilter (org.springframework.web.filter)internalDoFilter:193, ApplicationFilterChain (org.apache.catalina.core)doFilter:166, ApplicationFilterChain (org.apache.catalina.core)doFilterInternal:93, HiddenHttpMethodFilter (org.springframework.web.filter)doFilter:107, OncePerRequestFilter (org.springframework.web.filter)internalDoFilter:193, ApplicationFilterChain (org.apache.catalina.core)doFilter:166, ApplicationFilterChain (org.apache.catalina.core)doFilterInternal:200, CharacterEncodingFilter (org.springframework.web.filter)doFilter:107, OncePerRequestFilter (org.springframework.web.filter)internalDoFilter:193, ApplicationFilterChain (org.apache.catalina.core)doFilter:166, ApplicationFilterChain (org.apache.catalina.core)invoke:200, StandardWrapperValve (org.apache.catalina.core)invoke:96, StandardContextValve (org.apache.catalina.core)invoke:490, AuthenticatorBase (org.apache.catalina.authenticator)invoke:139, StandardHostValve (org.apache.catalina.core)invoke:92, ErrorReportValve (org.apache.catalina.valves)invoke:74, StandardEngineValve (org.apache.catalina.core)service:343, CoyoteAdapter (org.apache.catalina.connector)service:408, Http11Processor (org.apache.coyote.http11)process:66, AbstractProcessorLight (org.apache.coyote)process:834, AbstractProtocol$ConnectionHandler (org.apache.coyote)doRun:1415, NioEndpoint$SocketProcessor (org.apache.tomcat.util.net)run:49, SocketProcessorBase (org.apache.tomcat.util.net)runWorker:1149, ThreadPoolExecutor (java.util.concurrent)run:624, ThreadPoolExecutor$Worker (java.util.concurrent)run:61, TaskThread$WrappingRunnable (org.apache.tomcat.util.threads)run:748, Thread (java.lang) 因为两次请求的调用栈都比较深且有很大一部分重合路径，我们使用Beyond Compare将两次调用栈对比来看。 Mybatis处理如上图所示 selectByCondition 是请求中dao层的调用方法，这个方法调用之前是spring对http请求的处理动作，处理的流程暂不分析。直接看selectByCondition之后处理流程。 MyBatis通过SqlSessionFactoryBuilder对mybatis-config.xml进行解析，从中构建出SqlSessionFactory， 再创建出SqlSession实例， SqlSession调用Executor生成StatementHandler对象。 然后通过Spring框架的DataSourceUtils::getConnection方法获取连接。 连接对比两次请求不同的地方，对比HikariDataSource源码。 首次请求会调用112行，第二次会调用128行。 可以看到Hikari连接池使用了双重检查锁的方式来实现单例，避免重复创建连接池。 一次请求结束后，连接会放在连接池中，在连接池中，使用connectionBag控制一个连接“借出”、“归还”。详细信息可参考Hikari的线程池的生命周期 我们分析下首次调用建立连接的过程。 Hikari会先创建一个连接池，然后使用我们在启动时注册的驱动（com.mysql.cj.jdbc.Driver）创建连接。 可以从NonRegisteringDriver::connect一直追下去，可以看到com.mysql.cj.NativeSession::connect方法实现如下：123456789101112131415161718192021222324252627282930public void connect(HostInfo hi, String user, String password, String database, int loginTimeout, TransactionEventHandler transactionManager) throws IOException &#123; this.hostInfo = hi; // reset max-rows to default value this.setSessionMaxRows(-1); // TODO do we need different types of physical connections? SocketConnection socketConnection = new NativeSocketConnection(); socketConnection.connect(this.hostInfo.getHost(), this.hostInfo.getPort(), this.propertySet, getExceptionInterceptor(), this.log, loginTimeout); // we use physical connection to create a -&gt; protocol // this configuration places no knowledge of protocol or session on physical connection. // physical connection is responsible *only* for I/O streams if (this.protocol == null) &#123; this.protocol = NativeProtocol.getInstance(this, socketConnection, this.propertySet, this.log, transactionManager); &#125; else &#123; this.protocol.init(this, socketConnection, this.propertySet, transactionManager); &#125; // use protocol to create a -&gt; session // protocol is responsible for building a session and authenticating (using AuthenticationProvider) internally this.protocol.connect(user, password, database); // error messages are returned according to character_set_results which, at this point, is set from the response packet this.protocol.getServerSession().setErrorMessageEncoding(this.protocol.getAuthenticationProvider().getEncodingForHandshake()); this.isClosed = false;&#125; 先创建一个socket与服务端建立连接 通过NativeProtocol.getInstance初始化MySQL协议相关信息 调用NativeProtocol::connect方法根据MySQL账号、密码、使用数据库等信息向服务端请求认证。 使用proceedHandshakeWithPluggableAuthentication对返回的数据包根据MySQL协议进行解析。 调用NativeProtocol::checkErrorMessage对解析后的内容做判断，如果没问题则正常连接，如果返回错误信息则抛出异常。 连接建立后，通过Hikari连接池保存，下次使用直接用（如对比文件所示）。MySQL协议详解可参考：http://hutaow.com/blog/2013/11/06/mysql-protocol-analysis/ 事务处理org.springframework.jdbc.datasource.DataSourceTransactionManager会维护一个DataSourceTransactionObject。里面存放事务请求的连接。保证事务里的所有请求都是同一个连接在执行。 总结通过对数据库连接过程的分析，对数据库服务端、客户端的连接过程有了一个初步的认识，脑海中有个大概的体系，但还是不够深入，如MySQL协议的具体协议内容、连接鉴权的细节、Hikari连接池、Jdbc。。。都需要大量时间去深入研究，接下来要逐步去学习、沉淀下来。]]></content>
      <categories>
        <category>个人学习</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【从入门到放弃-MySQL】数据库连接过程分析]]></title>
    <url>%2F6c0fa14d.html</url>
    <content type="text"><![CDATA[前言上周出现了几次连接超时、连接池满还有dbc连接事务模板失败的问题。所以有必要深入了解下MySQL的连接过程。 正好，上周研究了怎么用Clion调试MySQL源码，接下来通过调试来研究一下吧。 服务端启动sql/main.cc123extern int mysqld_main(int argc, char **argv);int main(int argc, char **argv) &#123; return mysqld_main(argc, argv); &#125; main：入口文件，仅调用了mysqld_main函数 sql/mysqld.cc12345678910111213141516int mysqld_main(int argc, char **argv)#endif&#123;if (my_init()) // init my_sys library &amp; pthreads &#123; ... &#125; ... if (load_defaults(MYSQL_CONFIG_NAME, load_default_groups, &amp;argc, &amp;argv, &amp;argv_alloc)) &#123; ... &#125; mysqld_socket_acceptor-&gt;connection_event_loop(); mysqld_exit(signal_hand_thr_exit_code);&#125; mysql_main：MySQL服务端启动逻辑的主要处理函数 my_init：系统库和线程初始化 load_defaults：加载my.cnf各参数 connection_event_loop：循环监听套接字。 sql/conn_handler/connection_acceptor.h1234567891011/** Connection acceptor loop to accept connections from clients.*/void connection_event_loop() &#123; Connection_handler_manager *mgr = Connection_handler_manager::get_instance(); while (!connection_events_loop_aborted()) &#123; Channel_info *channel_info = m_listener-&gt;listen_for_connection_event(); if (channel_info != NULL) mgr-&gt;process_new_connection(channel_info); &#125;&#125; connection_event_loop：通过socket_connection.cc::listen_for_connection_event循环监听，直到有新的连接，开始connection_handler_manager.cc::process_new_connection新连接的处理过程。新连接服务端一直处于监听状态，当有新连接请求时，调用process_new_connection处理新连接。 sql/conn_handler/connection_handler_manager.cc1234567891011121314void Connection_handler_manager::process_new_connection( Channel_info *channel_info) &#123; if (connection_events_loop_aborted() || !check_and_incr_conn_count(channel_info-&gt;is_admin_connection())) &#123; channel_info-&gt;send_error_and_close_channel(ER_CON_COUNT_ERROR, 0, true); delete channel_info; return; &#125; if (m_connection_handler-&gt;add_connection(channel_info)) &#123; inc_aborted_connects(); delete channel_info; &#125;&#125; connection_events_loop_aborted：先判断是否已取消监听 check_and_incr_conn_count：再判断（会加锁）是否现有连接数是否大于连接最大值（连接池满），未满，则将线程数加一，满了则拒绝连接。（注意，这里的判断逻辑使MySQL的实际最大连接数是max_connections + 1） add_connection：调用add_connection添加连接 sql/conn_handler/connection_handler_pre_thread.cc123456789101112131415161718192021222324252627282930313233bool Per_thread_connection_handler::add_connection(Channel_info *channel_info) &#123; int error = 0; my_thread_handle id; DBUG_ENTER("Per_thread_connection_handler::add_connection"); // Simulate thread creation for test case before we check thread cache DBUG_EXECUTE_IF("fail_thread_create", error = 1; goto handle_error;); if (!check_idle_thread_and_enqueue_connection(channel_info)) DBUG_RETURN(false); /* There are no idle threads avaliable to take up the new connection. Create a new thread to handle the connection */ channel_info-&gt;set_prior_thr_create_utime(); error = mysql_thread_create(key_thread_one_connection, &amp;id, &amp;connection_attrib, handle_connection, (void *)channel_info);#ifndef DBUG_OFFhandle_error:#endif // !DBUG_OFF if (error) &#123; ... //错误处理，略 &#125; Global_THD_manager::get_instance()-&gt;inc_thread_created(); DBUG_PRINT("info", ("Thread created")); DBUG_RETURN(false);&#125; 调用check_idle_thread_and_enqueue_connection查看是否有空闲的线程，有则将本次连接信息加入等待队列，并给空闲线程发送唤醒信号；否则新建线程处理本次连接 在新线程中，调用handle_connection函数开始进行逻辑处理。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687static void *handle_connection(void *arg) &#123; Global_THD_manager *thd_manager = Global_THD_manager::get_instance(); Connection_handler_manager *handler_manager = Connection_handler_manager::get_instance(); Channel_info *channel_info = static_cast&lt;Channel_info *&gt;(arg); bool pthread_reused MY_ATTRIBUTE((unused)) = false; if (my_thread_init()) &#123; ... //错误处理，略 &#125; for (;;) &#123; THD *thd = init_new_thd(channel_info); if (thd == NULL) &#123; ... //错误处理，略 &#125;#ifdef HAVE_PSI_THREAD_INTERFACE if (pthread_reused) &#123; ... //错误处理，略 &#125;#endif#ifdef HAVE_PSI_THREAD_INTERFACE /* Find the instrumented thread */ PSI_thread *psi = PSI_THREAD_CALL(get_thread)(); /* Save it within THD, so it can be inspected */ thd-&gt;set_psi(psi);#endif /* HAVE_PSI_THREAD_INTERFACE */ mysql_thread_set_psi_id(thd-&gt;thread_id()); mysql_thread_set_psi_THD(thd); mysql_socket_set_thread_owner( thd-&gt;get_protocol_classic()-&gt;get_vio()-&gt;mysql_socket); thd_manager-&gt;add_thd(thd); if (thd_prepare_connection(thd)) handler_manager-&gt;inc_aborted_connects(); else &#123; while (thd_connection_alive(thd)) &#123; if (do_command(thd)) break; &#125; end_connection(thd); &#125; close_connection(thd, 0, false, false); thd-&gt;get_stmt_da()-&gt;reset_diagnostics_area(); thd-&gt;release_resources(); // Clean up errors now, before possibly waiting for a new connection.#ifndef HAVE_WOLFSSL#if OPENSSL_VERSION_NUMBER &lt; 0x10100000L ERR_remove_thread_state(0);#endif /* OPENSSL_VERSION_NUMBER &lt; 0x10100000L */#endif thd_manager-&gt;remove_thd(thd); Connection_handler_manager::dec_connection_count();#ifdef HAVE_PSI_THREAD_INTERFACE /* Delete the instrumentation for the job that just completed. */ thd-&gt;set_psi(NULL); PSI_THREAD_CALL(delete_current_thread)();#endif /* HAVE_PSI_THREAD_INTERFACE */ delete thd; // Server is shutting down so end the pthread. if (connection_events_loop_aborted()) break; channel_info = Per_thread_connection_handler::block_until_new_connection(); if (channel_info == NULL) break; pthread_reused = true; if (connection_events_loop_aborted()) &#123; ... //错误处理，略 &#125; &#125; my_thread_end(); my_thread_exit(0); return NULL;&#125; 会对连接进行thd_prepare_connection预处理操作，没问题后继续下面的逻辑。 当连接未被关闭，就会一直do_command处理请求。 当连接关闭，则走下面关闭逻辑 执行sql/sql_parse.cc12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697bool do_command(THD *thd) &#123; bool return_value; int rc; NET *net = NULL; enum enum_server_command command; COM_DATA com_data; DBUG_ENTER("do_command"); DBUG_ASSERT(thd-&gt;is_classic_protocol()); /* indicator of uninitialized lex =&gt; normal flow of errors handling (see my_message_sql) */ thd-&gt;lex-&gt;set_current_select(0); /* XXX: this code is here only to clear possible errors of init_connect. Consider moving to prepare_new_connection_state() instead. That requires making sure the DA is cleared before non-parsing statements such as COM_QUIT. */ thd-&gt;clear_error(); // Clear error message thd-&gt;get_stmt_da()-&gt;reset_diagnostics_area(); /* This thread will do a blocking read from the client which will be interrupted when the next command is received from the client, the connection is closed or "net_wait_timeout" number of seconds has passed. */ net = thd-&gt;get_protocol_classic()-&gt;get_net(); my_net_set_read_timeout(net, thd-&gt;variables.net_wait_timeout); net_new_transaction(net); /* Synchronization point for testing of KILL_CONNECTION. This sync point can wait here, to simulate slow code execution between the last test of thd-&gt;killed and blocking in read(). The goal of this test is to verify that a connection does not hang, if it is killed at this point of execution. (Bug#37780 - main.kill fails randomly) Note that the sync point wait itself will be terminated by a kill. In this case it consumes a condition broadcast, but does not change anything else. The consumed broadcast should not matter here, because the read/recv() below doesn't use it. */ DEBUG_SYNC(thd, "before_do_command_net_read"); /* Because of networking layer callbacks in place, this call will maintain the following instrumentation: - IDLE events - SOCKET events - STATEMENT events - STAGE events when reading a new network packet. In particular, a new instrumented statement is started. See init_net_server_extension() */ thd-&gt;m_server_idle = true; rc = thd-&gt;get_protocol()-&gt;get_command(&amp;com_data, &amp;command); thd-&gt;m_server_idle = false; if (rc) &#123; ... //错误处理，略 &#125; char desc[VIO_DESCRIPTION_SIZE]; vio_description(net-&gt;vio, desc); DBUG_PRINT("info", ("Command on %s = %d (%s)", desc, command, command_name[command].str)); DBUG_PRINT("info", ("packet: '%*.s'; command: %d", thd-&gt;get_protocol_classic()-&gt;get_packet_length(), thd-&gt;get_protocol_classic()-&gt;get_raw_packet(), command)); if (thd-&gt;get_protocol_classic()-&gt;bad_packet) DBUG_ASSERT(0); // Should be caught earlier // Reclaim some memory thd-&gt;get_protocol_classic()-&gt;get_output_packet()-&gt;shrink( thd-&gt;variables.net_buffer_length); /* Restore read timeout value */ my_net_set_read_timeout(net, thd-&gt;variables.net_read_timeout); return_value = dispatch_command(thd, &amp;com_data, command); thd-&gt;get_protocol_classic()-&gt;get_output_packet()-&gt;shrink( thd-&gt;variables.net_buffer_length);out: /* The statement instrumentation must be closed in all cases. */ DBUG_ASSERT(thd-&gt;m_digest == NULL); DBUG_ASSERT(thd-&gt;m_statement_psi == NULL); DBUG_RETURN(return_value);&#125; 主要的处理逻辑为dispatch_command，根据不同的command类型进行分发。 123456789101112131415161718192021222324252627282930313233343536373839/** Perform one connection-level (COM_XXXX) command. @param thd connection handle @param command type of command to perform @param com_data com_data union to store the generated command @todo set thd-&gt;lex-&gt;sql_command to SQLCOM_END here. @todo The following has to be changed to an 8 byte integer @retval 0 ok @retval 1 request of thread shutdown, i. e. if command is COM_QUIT*/bool dispatch_command(THD *thd, const COM_DATA *com_data, enum enum_server_command command) &#123; ... //太长不看 switch (command) &#123; case ... //太长不看 case COM_QUERY: &#123; ... //太长不看 mysql_parse(thd, &amp;parser_state); ... //太长不看 DBUG_PRINT("info", ("query ready")); break; &#125; case ... //太长不看 default: my_error(ER_UNKNOWN_COM_ERROR, MYF(0)); break; &#125;&#125; 主要看COM_QUERY这个逻辑，我们要用到的DDL、DML都会走这个流程，这个流程中主要是调用mysql_parse方法 123456789101112131415161718192021222324252627282930/** Parse a query. @param thd Current session. @param parser_state Parser state.*/void mysql_parse(THD *thd, Parser_state *parser_state) &#123; ... //太长不看 mysql_reset_thd_for_next_command(thd); if (!err) &#123; err = parse_sql(thd, parser_state, NULL); ... //太长不看 &#125; if (!err) &#123; mysql_rewrite_query(thd); ... //太长不看 &#125; if (!err) &#123; ... error = mysql_execute_command(thd, true); ... &#125;&#125; 主要是SQL语法解析和执行 mysql_reset_thd_for_next_command是对下一次执行做准备，重置线程各变量 mysql_rewrite_query看着像是SQL优化？待定 还没追进去，记个TODO 词法解析前不应该有缓存吗？没有找到缓存的逻辑，记个TODO（后续：原来MySQL8.0取消了query cache，详见：https://mysqlserverteam.com/mysql-8-0-retiring-support-for-the-query-cache/） 关闭连接sql/conn_handler/connection_handler_pre_thread.cc123456789101112131415161718192021222324252627282930313233343536373839Channel_info *Per_thread_connection_handler::block_until_new_connection() &#123; Channel_info *new_conn = NULL; mysql_mutex_lock(&amp;LOCK_thread_cache); if (blocked_pthread_count &lt; max_blocked_pthreads &amp;&amp; !shrink_cache) &#123; /* Don't kill the pthread, just block it for reuse */ DBUG_PRINT("info", ("Blocking pthread for reuse")); /* mysys_var is bound to the physical thread, so make sure mysys_var-&gt;dbug is reset to a clean state before picking another session in the thread cache. */ DBUG_POP(); DBUG_ASSERT(!_db_is_pushed_()); // Block pthread blocked_pthread_count++; while (!connection_events_loop_aborted() &amp;&amp; !wake_pthread &amp;&amp; !shrink_cache) mysql_cond_wait(&amp;COND_thread_cache, &amp;LOCK_thread_cache); blocked_pthread_count--; if (shrink_cache &amp;&amp; blocked_pthread_count &lt;= max_blocked_pthreads) &#123; mysql_cond_signal(&amp;COND_flush_thread_cache); &#125; if (wake_pthread) &#123; wake_pthread--; if (!waiting_channel_info_list-&gt;empty()) &#123; new_conn = waiting_channel_info_list-&gt;front(); waiting_channel_info_list-&gt;pop_front(); DBUG_PRINT("info", ("waiting_channel_info_list-&gt;pop %p", new_conn)); &#125; else &#123; DBUG_ASSERT(0); // We should not get here. &#125; &#125; &#125; mysql_mutex_unlock(&amp;LOCK_thread_cache); return new_conn;&#125; 如果阻塞的线程数小于最大阻塞线程数，则此线程不回收，而是进入阻塞状态（等待），等待新连接来的时候重复使用。 否则关闭线程。 客户端【从入门到放弃-MySQL】数据库连接过程分析-客户端 参考文献：https://www.cnblogs.com/FateTHarlaown/p/8676166.html]]></content>
      <categories>
        <category>个人学习</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【从入门到放弃-Hadoop】Hadoop基础学习]]></title>
    <url>%2F76483f54.html</url>
    <content type="text"><![CDATA[前言目前人工智能和大数据火热，使用的场景也越来越广，日常开发中也逐渐接触了更多与大数据相关的开发需求。因此对大数据知识也有必要进行一些学习理解 基础概念大数据的本质一、数据的存储：分布式文件系统（分布式存储） 二、数据的计算：分部署计算 基础知识学习大数据需要具备Java知识基础及Linux知识基础 学习路线Java基础和Linux基础Hadoop的学习：体系结构、原理、编程第一阶段： HDFS、MapReduce、HBase（NoSQL数据库） 第二阶段： 数据分析引擎 -&gt; Hive、Pig 数据采集引擎 -&gt; Sqoop、Flume 第三阶段： HUE：Web管理工具 ZooKeeper：实现Hadoop的HA Oozie：工作流引擎 Spark的学习第一阶段：Scala编程语言 第二阶段：Spark Core -&gt; 基于内存、数据的计算 第三阶段：Spark SQL -&gt; 类似于mysql 的sql语句 第四阶段：Spark Streaming -&gt;进行流式计算：比如：自来水厂 Apache Storm 类似Spark Streaming -&gt;进行流式计算 NoSQLRedis基于内存的数据库 HDFS分布式文件系统 解决以下问题： 1、硬盘不够大：多几块硬盘，理论上可以无限大 2、数据不够安全：冗余度，hdfs默认冗余为3 ，用水平复制提高效率，传输按照数据库为单位：Hadoop1.x 64M，Hadoop2.x 128M 管理员：NameNode 硬盘：DataNode MapReduce基础编程模型：把一个大任务拆分成小任务，再进行汇总 MR任务：Job = Map + Reduce Map的输出是Reduce的输入、MR的输入和输出都是在HDFS MapReduce数据流程分析： Map的输出是Reduce的输入，Reduce的输入是Map的集合 HBase什么是BigTable？: 把所有的数据保存到一张表中，采用冗余 —&gt; 好处：提高效率 1、因为有了bigtable的思想：NoSQL：HBase数据库 2、HBase基于Hadoop的HDFS的 3、描述HBase的表结构 核心思想是：利用空间换效率 Hadoop环境搭建环境准备Linux环境、JDK、http://mirrors.shu.edu.cn/apache/hadoop/common/hadoop-3.0.0/hadoop-3.0.0-src.tar.gz 安装1、安装jdk、并配置环境变量 vim /etc/profile 末尾添加 2、解压hadoop-3.0.0.tar.gz、并配置环境变量 tar -zxvf hadoop-3.0.0.tar.gz -C /usr/local/ mv hadoop-3.0.0/ hadoop vim /etc/profile 末尾添加 配置Hadoop有三种安装模式： 本地模式 ： 1台主机 不具备HDFS，只能测试MapReduce程序 伪分布模式： 1台主机 具备Hadoop的所有功能，在单机上模拟一个分布式的环境 （1）HDFS：主：NameNode，数据节点：DataNode （2）Yarn：容器，运行MapReduce程序 主节点：ResourceManager 从节点：NodeManager 全分布模式： 至少3台 我们以伪分布模式为例配置： 修改hdfs-site.xml：冗余度1、权限检查false&lt;!--配置冗余度为1--&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;!--配置权限检查为false--&gt; &lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; 修改core-site.xml&lt;!--配置HDFS的NameNode--&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://192.168.56.102:9000&lt;/value&gt; &lt;/property&gt; &lt;!--配置DataNode保存数据的位置--&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/tmp&lt;/value&gt; &lt;/property&gt; 修改mapred-site.xml&lt;!--配置MR运行的框架--&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yar&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=/usr/local/hadoop&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.map.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=/usr/local/hadoop&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.reduce.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=/usr/local/hadoop&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.application.classpath&lt;/name&gt; &lt;value&gt; /usr/local/hadoop/etc/hadoop, /usr/local/hadoop/share/hadoop/common/*, /usr/local/hadoop/share/hadoop/common/lib/*, /usr/local/hadoop/share/hadoop/hdfs/*, /usr/local/hadoop/share/hadoop/hdfs/lib/*, /usr/local/hadoop/share/hadoop/mapreduce/*, /usr/local/hadoop/share/hadoop/mapreduce/lib/*, /usr/local/hadoop/share/hadoop/yarn/*, /usr/local/hadoop/share/hadoop/yarn/lib/*, &lt;/value&gt; &lt;/property&gt; 修改yarn-site.xml&lt;!--配置ResourceManager地址--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;192.168.56.102&lt;/value&gt; &lt;/property&gt; &lt;!--配置NodeManager执行任务的方式--&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-service&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; 格式化NameNodehdfs namenode -format 看到common.Storage: Storage directory /usr/local/hadoop/tmp/dfs/name has been successfully formatted表示格式化成功 启动start-all.sh HDFS：存储数据 YARN： 访问 命令行 Java Api WEB Console HDFS: http://192.168.56.102:50070 Yarn: http://192.168.56.102:8088 查看HDFS管理界面和yarn资源管理系统 基本操作：HDFS相关命令-mkdir 在HDFD创建目录 hdfs dfs -mkdir /data -ls 查看目录 hdfs dfs -ls -ls -R 查看目录与子目录 hdfs dfs -ls -R -put 上传一个文件 hdfs dfs -put data.txt /data/input -copyFromLocal 上传一个文件 与-put一样 -moveFromLocal 上传一个文件并删除本地文件 -copyToLocal 下载文件 hdfs dfs -copyTolocal /data/input/data.txt -put 下载文件 hdfs dfs -put/data/input/data.txt -rm 删除文件 hdfs dfs -rm -getmerge 将目录所有文件先合并再下载 -cp 拷贝 -mv 移动 -count 统计目录下的文件个数 -text、-cat 查看文件 -balancer 平衡操作 MapReduce示例 结果： 如上 一个最简单的MapReduce示例就执行成功了 思考Hadoop是基于Java语言的，日常开发是用的PHP(写文章时，博主主要是用PHP，现在已经转Java了)，在使用、查找错误时还是蛮吃力的。工作之余还是需要多补充点其它语言的相关知识，编程语言是我们开发、学习的工具，而不应成为限制我们技术成长的瓶颈]]></content>
      <categories>
        <category>个人学习</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【从入门到放弃-PHP】foreach 引用的坑]]></title>
    <url>%2F76b41bca.html</url>
    <content type="text"><![CDATA[背景描述先看一段代码。1234567891011121314151617181920212223242526272829&lt;?php/*$arr = [ 'jack' =&gt; '20', 'tom' =&gt; '21', 'marry' =&gt; '54', 'less' =&gt; '23'];$b = &amp;$arr['jack'];$a = $arr;$b = 30;var_dump($a);$str = '20';$c = &amp;$str;$d = $str;$c = 30;var_dump($d);*/$arr = [ 'jack' =&gt; '20', 'tom' =&gt; '21', 'marry' =&gt; '54', 'less' =&gt; '23'];foreach ($arr as &amp;$val) &#123; echo $val;&#125;foreach ($arr as $val) &#123; echo $val;&#125;print_r($arr); 想一下应该输出什么呢？ 运行一下脚本，真实结果和你想的是否一致呢？ 在foreach中使用了引用后再次foreach发现$arr[‘less’]的值变成了54，常规理解应该是23才对。 猜测可能是因为使用引用导致该值变为54 但本着知其然更要知其所以然 我们一起追一下php源码 是什么原因导致的 环境准备工欲善其事必先利其器，先下载调试工具及源码 下载Visual Studio 2017，并安装下载地址：https://www.visualstudio.com/zh-hans/downloads/ 下载php源码http://cn2.php.net/distributions/php-7.0.27.tar.bz2 从文件夹创建解决方案创建成功后如下图所示 源码追踪先搜索关键字foreach 可以在zend_language_parser.c 中看到， 语法解析时 foreach会当做T_FOREACH 在zend_language_parser.y可以看到语法解析的具体方式 ZEND_AST_FOREACH 查找zend_ast_create zend_ast.c中： zend_ast_create 函数是创建一个抽象语法树（abstract syntax tree）返回的zend_ast结构如下： 具体的赋值操作如下： 接下来在zend_compile.c中根据抽象语法树生成opcode： 通过上图及语法解析的分析可知，foreach在编译阶段会生成如上图的四个zend_ast节点，分别表示：要遍历的数组或对象expr_ast，要遍历的value value_ast，要遍历的key key_ast，循环体stmt_ast如： 12345$arr = [1, 2, 3];foreach ($arr as $key =&gt; $val) &#123; echo $val;&#125; expr_ast 是可理解为是$arr编译时对应的ast结构 value_ast对应$val key_ast对应$key stmt_ast对应”echo $val;” copy一份要遍历的数组或对象，如果是引用则把原数组或对象设为引用类型如：123foreach ($arr as $k =&gt; $v) &#123; echo $v;&#125; copy一份$arr用于遍历，从arData的首元素起，把bucket.zval.value赋值给$v,把bucket.h或key赋值给$k，然后将下一个元素的位置记录在zval.u2.fe_iter_idx中，下次遍历从该位置开始 当u2.fe_iter_idex到了arData的末尾则遍历结束并销毁copy的$arr副本 如果$v是引用 则在循环前，将原$arr设置为引用类型 即：123foreach ($arr as $k =&gt; &amp;$v) &#123; echo $v;&#125; 编译copy的数组、对象操作的指令：增加一条opcode指令 ZEND_FE_RESET_R（如果value是引用则用ZEND_FE_RESET_RW） 。执行时如果发现遍历的不是数组、对象 则抛出一个warning，然后跳出循环。 编译fetch数组、对象当前单元key 、value的opcode : ZEND_FE_FETCH_R（如果value是引用则用ZEND_FE_FETCH_RW）。此opcode需要知道当遍历到达数组末尾时跳出遍历的位置。此外还会对key和value分配他们在内存中的位置，如果value不是一CV个变量，还会编译其它操作的opcode 如果定义了key，则会编译一条opcode，对key进行赋值 编译循环体statement 编译跳回遍历开始时的opcode，一次遍历结束后跳到步骤2编译的opcode进行下次遍历 设置步骤1、2两条opcode如果出错要跳到的opcode 结束循环 编译ZEND_FE_FREE用于释放1中copy的数组或对象结论分析编译后的结构： 运行时步骤： (1) 执行ZEND_FE_RESET_R，过程上面已经介绍了； (2) 执行ZEND_FE_FETCH_R，此opcode的操作主要有三个：检查遍历位置是否到达末尾、将数组元素的value赋值给$value、将数组元素的key赋值给一个临时变量(注意与value不同)； (3) 如果定义了key则执行ZEND_ASSIGN，将key的值从临时变量赋值给$key，否则跳到步骤(4)； (4) 执行循环体的statement； (5) 执行ZEND_JMPNZ跳回步骤(2)； (6) 遍历结束后执行ZEND_FE_FREE释放数组。 因此根据上面的分析：赋值的核心操作是ZEND_FE_FETCH_RW 上面的例子可等价于123456789101112131415$arr = [ 'jack' =&gt; '20', 'tom' =&gt; '21', 'marry' =&gt; '54', 'less' =&gt; '23'];$val = &amp;$arr['jack'];$val = &amp;$arr['tom'];$val = &amp;$arr['marry'];$val = &amp;$arr['less'];$val = $arr['jack'];$val = $arr['tom'];$val = $arr['marry'];$val = $arr['less'];print_r($arr); 等价于：12345678910$arr = [ 'jack' =&gt; '20', 'tom' =&gt; '21', 'marry' =&gt; '54', 'less' =&gt; '23'];$val = &amp;$arr['less']; (23)$val = $arr['marry']; (54，并且此时因为引用 $arr['less']也变为54了)$val = $arr['less']; (54)print_r($arr); 因此 为了避免出现不必要的错误，建议在使用完&amp;后，unset掉变量以取消对地址的引用 思维发散：针对以上情况，如果不取消对变量的引用，而是将数组赋值给一个新的变量再foreach。是否可行？ 先看一段代码：123456&lt;?php$str = '20';$c = &amp;$str;$a = $str;$c = 30;var_dump($a); 输出20 没有任何问题如果换成数组：1234567891011&lt;?php$arr = [ 'jack' =&gt; '20', 'tom' =&gt; '21', 'marry' =&gt; '54', 'less' =&gt; '23'];$b = &amp;$arr;$a = $arr;$b['jack'] = 30;var_dump($a); 还是20 符合预期但如果这样呢：1234567891011&lt;?php$arr = [ 'jack' =&gt; '20', 'tom' =&gt; '21', 'marry' =&gt; '54', 'less' =&gt; '23'];$b = &amp;$arr['jack'];$a = $arr;$b = 30;var_dump($a) 值却变成了30我们加上xdebug_debug_zval看看发生了什么12345678910111213&lt;?php$arr = [ 'jack' =&gt; '20', 'tom' =&gt; '21', 'marry' =&gt; '54', 'less' =&gt; '23'];$b = &amp;$arr;$a = $arr;$b['jack'] = 30;var_dump($a);xdebug_debug_zval('a');xdebug_debug_zval('arr'); 可以看出，直接引用数组， $b = &amp;$arr, $arr 的is_ref是1，refcount是2, 给$a = $arr时，发生分离，$a 与$arr指向不同的zval，$b 与 $arr指向相同的zval，因此给$b[‘jack’] = 30, $a的值不会发生改变12345678910111213&lt;?php$arr = [ 'jack' =&gt; '20', 'tom' =&gt; '21', 'marry' =&gt; '54', 'less' =&gt; '23'];$b = &amp;$arr['jack'];$a = $arr;$b = 30;var_dump($a);xdebug_debug_zval('a');xdebug_debug_zval('arr') 可以看出，对数组中一个元素引用时，数组的is_ref是0，因为$a = $arr 因此refcount是2 ，指向同一个zval，改变$b的值时，因为$arr[‘jack’]是一个引用，zval的值改变，$a和$arr的zval相同，$a[‘jack’]也变为30同理可以回答最开始提出的疑问：如果我不取消对变量的引用，而是将数组赋值给一个新的变量再foreach。是否可行？答：不行。123456789101112131415&lt;?php$arr = [ 'jack' =&gt; '20', 'tom' =&gt; '21', 'marry' =&gt; '54', 'less' =&gt; '23'];foreach ($arr as &amp;$val) &#123; echo $val;&#125;$a = $arr;foreach ($a as $val) &#123; echo $val;&#125;print_r($a); 因为$arr与$a指向同一份zval，还是会出现$a[‘less’] = 54的结果。因此，在foreach使用完&amp;后，还是unset掉变量 取消对地址的引用再进行下一步操作吧 参考文献：https://github.com/pangudashu/php7-internal/blob/master/4/loop.md]]></content>
      <categories>
        <category>个人学习</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>PHP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【从入门到放弃-MySQL】mysql 数据恢复实战&踩坑记录]]></title>
    <url>%2F6e2383f1.html</url>
    <content type="text"><![CDATA[背景介绍线下环境的两个mysql实例安装部署在同一台测试机器上使用不同端口，某天，机器硬盘故障无法启动、并且无法重装系统，需要将重要数据备份重新部署mysql并恢复 操作步骤备份数据首先联系pe同学通过带外方式启动故障主机并将硬盘挂载，通过scp方式将两个mysql实例的data目录下所有文件copy备份注意 切勿仅copy MYD，MYI，frm及ibd文件 准备环境原主机硬盘故障无法重装系统，需要到现场维修。所以新申请了一台主机使用安装与原mysql版本一致的mysql历史原因原主机安装的版本分别为5.1.40 和5.6.26主机自带5.1.40版本的mysql不需要自己再安装，直接将备份的data目录覆盖copy新机器的data目录 并修改好文件权限即可 安装mysql高版本的mysql需要重新安装 。步骤如下 下载glibc版wget https://cdn.mysql.com/archives/mysql-5.6/mysql-5.6.26-linux-glibc2.5-x86_64.tar.gz 解压并移动tar -zxvf mysql-5.6.26-linux-glibc2.5-x86_64.tar.gzmv mysql-5.6.26-linux-glibc2.5-x86_64.tar.gz /usr/local/mysql 修改权限并初始化1234chown -R mysql:mysql /usr/local/mysql cd /usr/local/mysql/bin# 修改/usr/local/mysql/my.cnf 修改启动端口和文件存放路径 1234sudo ./mysqld --initialize --user=mysql --basedir=/usr/local/mysql --datadir=/usr/local/mysql/data --lc_messages_dir=/usr/local/mysql/share --lc_messages=en_US --defaults-file=/usr/local/mysql/my.cnf #（如忘记密码可执行以下操作免密码登录）sudo /usr/bin/mysqld_safe --skip-grant-tables &amp; 将原数据data目录覆盖并修改权限登录验证至此 mysql数据恢复工作已经完成 踩坑记录背景之前两台测试环境mysql分别安装在不同的主机上，其中一台为虚拟主机，硬盘容量只有50G，出现过数据不断累积导致硬盘容量不足的情况，同时因为测试机器资源紧张，考虑将两个mysql实例安装在同一台物理主机上因物理主机上使用的mysql版本过低 所以新的mysql实例决定升级为高版本 安装时出现的问题在mysql官网现在了最新的稳定版mysql，解压、进行安装出现以下报错123456sudo ./mysqld --initialize --user=mysql --basedir=/usr/local/mysql --datadir=/usr/local/mysql/data --lc_messages_dir=/usr/local/mysql/share --lc_messages=en_US --defaults-file=/usr/local/mysql/my.cnf ./mysqld: /usr/lib64/libnuma.so.1: no version information available (required by ./mysqld)./mysqld: /usr/lib64/libnuma.so.1: no version information available (required by ./mysqld)./mysqld: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.11&apos; not found (required by ./mysqld)./mysqld: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.9&apos; not found (required by ./mysqld)./mysqld: /lib64/libc.so.6: version `GLIBC_2.10&apos; not found (required by ./mysqld) 看报错内容是一些依赖库版本过低导致，当时准备升级版本库在网上找到了 高版本的libstdc++ 、glibc等进行编译升级成功安装好了高版本 libstdc++后继续编译安装glibc编译安装好后，在删除原libc-2.5.so 更改软链为高版本libc时，悲剧出现了！因缺少libc库，所有的ls、ln、cp、sudo等命令全都无法使用了在网上找解决办法，可以在执行命令前使用LD_PRELOAD=/lib64/libc-2.5.so提前载入链接库来执行命令，ls、cp等命令可以用了但是使用ln命令时，发现权限不够ok，没关系 我们在sudo前 也提前载入链接库不就行了？执行：？？？ 尴尬了 竟然不行！查阅资料发现 sudo命令因为安全原因 不能使用LD_PRELOAD的方式 。我当时是在admin用户下 也无法sudo su切换到root 用户陷入了死循环、不切换到root用户就没权限恢复libc-2.5.so库 不恢复libc-2.5.so就没办法切换到root用户。。。无解，只能找pe同学帮忙，通过带外的方式恢复libc-2.5.so 解决方式系统恢复正常了，但是我们高版本的mysql还是没装上，系统库是不敢随便乱动了，那咋办呢？查看下glibc库版本天无绝人之路，发现有https://cdn.mysql.com/archives/mysql-5.6/mysql-5.6.26-linux-glibc2.5-x86_64.tar.gz glibc2.5版本的glibc版mysql安装包安装试试？通过上文 “安装mysql”中的方式 安装成功接下来修改用户密码、权限，通过mysqldump将原虚拟机中数据库的数据导入到5.6.26版本的数据库中 一台虚拟机上运行两个不同版本实例数据库大功告成~ mysql数据文件介绍表结构 .frm.frm文件：存储数据表的框架结构，文件名与表名相同，每个表对应一个同名frm文件，与操作系统和存储引擎无关 MyISAM数据文件.MYD文件：即MY Data，表数据文件.MYI文件：即MY Index，索引文件.log文件：日志文件 InnoDB数据文件ibdata1、ibdata2等：系统表空间文件，存储InnoDB系统信息和用户数据库表数据和索引，所有表共用.ibd文件：单表表空间文件，每个表使用一个表空间文件（file per table），存放用户数据库表数据和索引日志文件： ib_logfile1、ib_logfile2在备份和恢复数据时，我发现两个不同版本的数据库，ibdata1文件的大小相差很大查阅资料后发现原来InnoDB有两种不同的数据存储方式：共享表空间: 某一个数据库的所有的表数据，索引文件全部放在一个文件中，默认这个共享表空间的文件路径在data目录下。 默认的文件名为:ibdata1 初始化为10M。独占表空间: 每一个表都将会生成以独立的文件方式来进行存储（.ibd文件，这个文件包括了单独一个表的数据内容以及索引内容)。 存储内容比较使用独占表空间之后：每个表对应的数据、索引和插入缓冲 存放在独占表空间（.idb文件）每个表对应的撤销（undo）信息，系统事务信息，二次写缓冲等还是存放在了原来的共享表空间内（ibdata1文件） 特点比较具体的共享表空间和独立表空间优缺点如下：共享表空间：优点：可以放表空间分成多个文件存放到各个磁盘上（表空间文件大小不受表大小的限制，如一个表可以分布在不同的文件上）。数据和文件放在一起方便管理。缺点：所有的数据和索引存放到一个文件中，则将有一个很常大的文件，虽然可以把一个大文件分成多个小文件，但是多个表及索引在表空间中混合存储，这样对于一个表做了大量删除操作后表空间中将会有大量的空隙，特别是对于统计分析，日志系统这类应用最不适合用共享表空间。独立表空间：（在配置文件（my.cnf）中设置 innodb_file_per_table）优点：每个表都有自已独立的表空间。每个表的数据和索引都会存在自已的表空间中。可以实现单表在不同的数据库中移动。空间可以回收对于使用独立表空间的表，不管怎么删除，表空间的碎片不会太严重的影响性能，而且还有机会处理。a)Drop table操作自动回收表空间b）如果对于统计分析或是日值表，删除大量数据后可以通过:alter table TableName engine=innodb;回缩不用的空间。c) 对于使innodb-plugin的Innodb使用truncate table也会使空间收缩。5、在服务器资源有限,单表数据不是特别多的情况下, 独立表空间明显比共享方式效率更高 . 但是MySQL 默认是共享表空间 。缺点：单表体积可能过大，如超过100个G。查看innodb_file_per_table配置可以看到两个mysql的配置不一样，一个使用的共享表空间，一个使用的独占表空间，这就是为什么两个ibdata1文件大小相差很大注意：因为.frm、.ibd、.MYD、.MYI文件都存在于与database同名的文件夹下，我们通常会注意到而ibdata1文件是直接在data目录下，不理解其是什么文件的情况下很容易被忽略，所以 这就是在上文备份和恢复数据中提到需要注意的地方 参考文献http://www.jb51.net/article/134901.htm]]></content>
      <categories>
        <category>个人学习</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【从入门到放弃-MySQL】mysql中要避免使用大事务]]></title>
    <url>%2Fb382ba8d.html</url>
    <content type="text"><![CDATA[前言在日常工作中经常会使用一些比较“大”的数据库查询和操作，这里的“大”主要是指 执行时间长：含有较多的逻辑处理、存在较耗时操作等 操作数据多：需要查询或更新操作的数量记录较多，会锁定大量数据造成阻塞和锁超时等。 本文会和大家一起探讨下，为什么 在数据库中要避免使用这些大查询。 事务大家都清楚事务具备ACID特性（即原子性、一致性、隔离性、持久性），针对隔离性，在数据库事务隔离标准中，定义了四种隔离级别：读未提交、读提交、可重复读、串行化。MySQL默认的事务隔离级别是可重复读，我们以此来展开分析 事务隔离的实现多版本并发控制（MMVC）每行记录后面会有两个隐藏列，记录创建版本号及删除版本号。创建本本号记为row trx_id 对于一个事务来说，启动时（申请完事务id后），MySQL会给此事务创建一个活跃事务（即已启动但还未提交的事务）id数组。数组中的最小值记为minTid，最大值记为maxTid。 如果minTid &gt; row trx_id，则数据是可见的。 如果maxTid &lt; row trx_id，则数据是不可见的。 如果minTid &lt;= row trx_id &lt;= maxTid，且： row trx_id在数组中，则说明启动时，此事务未提交，数据不可见 row trx_id不在数组中，则说明启动是，此事务已提交，数据可见 如：当前事务id为50，活跃id数组为[35, 43, 44, 45, 46, 50, 51, 52]则 row trx_id小于35的数据为可见 row trx_id大于52的数据不可见 35 &lt;= row trx_id &lt;= 52且在数组中的数据不可见，不在数组中的数据可见。 对于不可见的数据，则需要依次去数据上一个版本查询，直到查询到可用版本数据为止。 只有在新的RW事务建立的时候 才会新建一个视图 否则继续使用上次创建的视图。 回滚日志（undo log）上面提到对于不可见数据需要依次查询上一版本来获取到可用数据。我们知道数据库的数据更新是非常频繁的，不可能将每一版本的数据都存下来，那样数据量会巨大查询也会非常的缓慢。MySQL通过undo log来获取历史版本的数据。undo log不会记录每个版本的最终数据，它是一个逻辑日志，是反向将之前的操作取消掉。比如对insert的会进行执行delete，delete的执行insert，对于update的数据会执行一个反向update，将之前修改的内容改回去。 例如： S1时刻，事务34启动，进行insert i = 5 操作后，commit，数据记录为D1：i = 5，row_id为34； S2时刻，事务36启动； S3时刻，事务37启动，进行update i + 3 操作后，commit，数据记录为D3：i = 8，row_id为37； S4时刻，事务42启动 S5时刻，事务54启动，进行update i * 2 操作后，commit，数据记录为D5：i = 16，row_id为54 此时，如果事务42需要查询i的数据，因为当前i = 16，row_id为54，数据不可见，因此需要根据undo log查询上一版本的数据。update i / 2，得到row_id为37。可见，获取i = 8如果事务36需要查询i的数据，需要update i / 2, 查到row_id = 37,不可见，继续回滚 update i - 3，查到row_id = 34，可见，获取到i = 5 只有当回滚日志不再需要时，才会删除。系统会判断，当没有事务再需要这些回滚日志的时候，才会删除。 所以长事务意味着系统里面会存在很多非常老的事务视图，因为这些事务可能会访问数据库中的任何数据，所以在这个事务提交之前，系统不得不保留它之后可能用到的所有回滚记录。这就会占用大量的存储空间。 事务启动autocommit参数控制事务是否自动提交，MySQL默认set autocommit=1，开启自动提交，即每条select、update都会自动提交。所以我们日常使用的SQL语句其实等价于123begin;select * from table where xxx;commit; 但有些客户端连接框架默认会在连接成功后执行一条set autocommit = 0，这样会导致你只有执行一条select语句其实就开启了事务。这样会意外导致长事务的出现。因此还是建议set autocommit = 1配合begin来显示的启动事务。 锁大事务还会长时间、大量占用锁资源，阻塞DML、DDL操作、造成锁超时影响系统并发能力，并且很容易引发死锁问题。 连接数大事务会长时间占用数据库连接，并发情况下容易造成连接数满的问题 拖垮整个应用 主备延迟MySQL主备复制只会在事务执行完毕后才会进行，即binlog在事务commit后才会生成（两阶段提交）。大事务执行多久就会造成多长时间的主备延迟，主备延迟的时间越长带来的风险也就越高 缓存MySQL的buffer pool对查询具有缓存效果，对于很多高频查询可以直接从缓存返回不需要查找磁盘文件。但是当有大量数据需要返回时通常有很多顺序查询，记录在同一磁盘页中就会命中缓存机制 对缓存造成一定影响MySQL buffer pool的缓存机制是使用的改良LRU算法（主要增加了访问时间控制） 内存&amp;CPUMySQL数据返回默认是边取边发，因此数据较多，传输时间较长也也会引发长事务带来的问题。还有如果返回大量数据给客户端处理，对客户端的内存及CPU也会带来较大的压力。 超时和超出大小限制容易引起超时的问题和超出max_binlog_cache_size导致执行失败。（还要注意，避免出现为了让主库大事务顺利进行，临时调大主库max_binlog_cache_size，忽略备库导致的服务宕掉等严重后果） 回滚回滚大事务也是非常耗时和占用内存的，需要注意 总结应该尽量避免使用大事务，开发时要注意尽量 如果可以，将一个大事务拆分成多个小事务执行 将事务中可以提出的select查询放在事务外执行]]></content>
      <categories>
        <category>个人学习</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【从入门到放弃-MySQL】CLion调试MySQL8.0源码]]></title>
    <url>%2Fc4ec5dd3.html</url>
    <content type="text"><![CDATA[前言想对的MySQL底层实现做一些了解，奈何没有用过C++不知道怎么调试一个大型项目，一日和大神交流时大神扔给我了一份《XCode调试MySQL8秘籍》。于是在几经波折（主要是因为菜）之后终于打开了MySQL的调试大门。 环境搭建准备MacOS： 10.14.5：因为根据大神秘籍，要使用Xcode，但Xcode下载目前只支持10.14.3，因此在10.13.6下强升的系统版本，Xcode下MySQL成功编译运行成功，但是遇到了诡异的调试无法的问题，排查无果最后转用CLionCLion：2019.1.3mysql源码：https://github.com/mysql/mysql-servercmake和boost：brew install cmake boost 编译MySQL源码目录：/var/workspace/mysql/mysql-8.0.16/boost目录：/usr/local/Cellar/boost/1.68.0_1 12345678910111213cd /var/workspace/mysql/mysql-8.0.16/mkdir workcd workcmake . -DWITH_DEBUG=1 -DCMAKE_INSTALL_PREFIX=/var/workspace/mysql/mysql-8.0.16/work -DMYSQL_DATADIR=/var/workspace/mysql/mysql-8.0.16/work/data -DWITH_INNOBASE_STORAGE_ENGINE=1 -DWITH_ARCHIVE_STORAGE_ENGINE=1 -DWITH_ARCHIVE_STORAGE_ENGINE=1 -DWITH_READLINE=1 -DWITH_LIBWRAP=0 -DMYSQL_UNIX_ADDR=/tmp/mysql.sock -DENABLED_LOCAL_INFILE=1 -DENABLED_LOCAL_INFILE=1 -DENABLE_DOWNLOADS=1 -DDOWNLOAD_BOOST=1 -DWITH_BOOST=/usr/local/Cellar/boost/1.68.0_1 -DFORCE_INSOURCE_BUILD=1make -j 4make install -j 4cd /var/workspace/mysql/mysql-8.0.16/worksudo bin/mysqld --basedir=/var/workspace/mysql/mysql-8.0.16/work --datadir=/var/workspace/mysql/mysql-8.0.16/work/data --initialize-insecure --user=mysql 如果最后一步执行出错可以参考https://dev.mysql.com/doc/refman/8.0/en/binary-installation.html新建mysql-files并修改权限 可能会有各种神奇的报错，主要是权限问题！搞不定的话参考下面有最终的目录权限截图 配置导入mysql-8.0.16项目，配置cmake参数 options参考编译过程中的cmake参数 选择mysqld 并编辑启动参数 arguments如下：1--basedir=/var/workspace/mysql/mysql-8.0.16/work --datadir=/var/workspace/mysql/mysql-8.0.16/work/data --user=mysql 启动调试，此时可能还会报错 还是权限问题。。。clion无法以root权限启动debug，尝试各种方法无效。因此把mysql的data目录改为777最终目录权限如下图： 再此点击debug按钮，启动成功（注意没error了，可以用客户端测试启动成功了）。 调试 我们在代码中打上断点，客户端执行SQL语句时就能在断点处看到各变量信息了，比如图中的SQL解析。 可以看到执行阻塞了 Clion代码调试的具体方法不做赘述了，网上一堆。 总结之前一直想调试MySQL，但是总是没有迈出第一步，代码下载下来就完事儿了。这次一鼓作气走了下来，希望能开个好头，养成各种代码调试的好习惯。看代码中细节比任何文档中都来的扎实（当然，时间充裕前提下）。搭建环境的过程中遇到了很多问题，Google、百度无数遍都没有能解决问题，最终还是通过MySQL的官方手册找到的答案。MySQL的官方手册简直神器，大家可以好好利用起来。C++的知识仅停留在大学课本阶段，阅读源码简直困难，要能坚持下去，加油！]]></content>
      <categories>
        <category>个人学习</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【从入门到放弃-Redis】redis底层数据结构和对象]]></title>
    <url>%2F9d3df67e.html</url>
    <content type="text"><![CDATA[简单动态字符串SDSRedis 的默认字符串 替代C字符串 区别： 通过len属性直接获取到长度 杜绝缓冲区溢出，字符串拼接时会先检查sds的长度 减少修改字符串时带来的内存冲分配次数，即空间预分配和惰性空间释放策略1、空间预分配：当sds被修改并需要扩容时，若len将小于1M（扩容后），扩展和len相同的free空间，若将大于1M 则扩展1M的free空间 2、惰性空间释放：当sds被修改需求缩短时，并不回收多出来的内存，而是记在free属性中等待将来使用 是二进制安全的，可以存空字符等 兼容部分C字符串函数链表双向无环链表、带头尾指针、带长度计数器、每个节点支持多种数据类型由一个list结构和多个listNode结构组成字典使用哈希表作为底层实现，一个哈希表里可以有多个哈希节点，每一个哈希节点保存了字典中的一个键值对 rehash：当哈希表中的键值对太多或太少时，为了使哈希表的负载因子在一个合理的范围，会调整负载因子、表空间后进行重新散列 rehash条件 服务器当前没执行BGSAVE或BGREWRITEAOF命令且哈希表的负载因子大于等于1 服务器正在执行BGSAVE或BGREWRITEAOF命令且哈希表的负载因子大于等于5负载因子 = 哈希表已保存节点数量/哈希表大小load_factor = ht[0].used / ht[0].size渐进式rehash避免一次性、集中式的产生大量的运算对服务造成影响，每次在对字典进行增删改查时，会执行一个键值对的rehash工作。 新增的都会加在h1，查询会先查h0再查h1跳跃表只在两个地方用到，有序集合和集群节点中做内部数据结构由zskiplist和zskiplistNode两个结构组成，zskiplist用于保存跳跃表信息，而zskiplistNode用于表示跳跃表节点每个跳跃表节点的层高都是1～32直接的随机数同一个跳跃表中 多个节点可以包含相同的分值，但是每个几点的成员对象必须是唯一的跳跃表中的节点按照分支大小排序，当分值相同时，按照成员对象的大小排序整数集合用于保存整数值的集合，从小到大有序排列且不包含重复项升级当添加新元素时，新元素的类型比整数集合的现有元素的类型都长时，整数集合需要先升级 才能将新元素添加到整数集合里 根据新元素类型，扩展整数集合底层数组的空间大小，并为新元素分配空间 将底层数组现有的所有元素都转换成与新元素相同的类型，并将转换后的元素放到正确的位置 将新元素添加到底层数组中升级的好处：提高灵活性可以适配多种整数类型，节约内存只在必要的时候升级整数集合不支持降级操作压缩列表当一个列表只包含少量列表项，且每个列表项不是小整数就是短字符串，则会使用压缩列表做列表的底层实现当一个哈希结构只包含少量键值对且键值对的键和值都是小整数或短字符串，也会使用压缩列表来做底层实现压缩列表是为了节约内存而开发的顺序型数据结构压缩列表可以包含多个节点，每个节点可以保存一个字节数组或整数值添加或删除节点可能会引发连锁更新，但出现的机率不高对象type属性表示redis的五种属性之一encoding属性表示底层对象实现所用的数据结构字符串对象编码可以是int、raw、embstrembstr专门用于保存短字符串的优化编码方式Int在修改为非int时，会变为rawembstr在修改时 会变为raw列表对象列表对象的编码可以是ziplist或linkedlist列表对象同时满足两个条件时，使用ziplist编码： 列表对象保存的所有字符串元素都小于64字节 列表对象保存的元素数量小于512个其它条件使用linkedlist编码可以使用list-max-ziplist-value和list-max-ziplist-entries选项修改哈希对象哈希对象的编码可以是ziplist或hashtable哈希对象同时满足两个条件时，使用ziplist编码： 哈希对象保存的所有键值对的键和值的字符串都小于64字节 哈希对象保存的键值对数量小于512个其它条件使用hashtable编码可以使用hash-max-ziplist-value和hash-max-ziplist-entries选项修改集合对象集合对象的编码可以是intset或hashtableintset编码的集合对象是使用整数集合作为底层实现，集合对象包含的所有元素都被保存在整数集合里Hashtable 编码的集合对象使用字典作为底层实现，每个字典的每个键都是一个字符串对象，每个字符串对象包含了一个集合元素，字典的值全被设为null当集合对象可以同时满足以上两个条件时，对象使用intset编码 集合对象保存的所有元素都是整数值 集合对象保存的元素数量不超过512个其它情况使用hashtable可以使用set-max-intset-entires选项设置有序集合对象有序集合使用ziplist或skiplistziplist编码的有序集合底层使用压缩列表作为底层实现，每个集合元素使用两个紧挨着的压缩列表节点保存，第一个节点保存元素的成员，第二个节点保存元素的分值压缩列表内的集合按分值从小到大排列skiplist编码的有序集合使用zset结构作为底层实现，一个zset结构包含一个字典和一个跳跃表zset结构中的zsl跳跃表按分值从小到大保存了所有集合元素，每个跳跃表节点都保存了一个集合元素，跳跃表节点的object属性保存了元素的成员，score属性保存了元素的分值。通过这个跳跃表，程序可以对有序集合进行范围查找此外，zset结构中的dict字典还保存了一个从成员到分值的映射，字典的键保存元素的成员，值使用double类型保存分值当有序集合对象同时满足以下两个条件时，使用ziplist编码： 有序集合保存的元素数量小于128个 有序集合保存的所有元素成员长度都小于64字节其它条件使用skiplist编码可以使用zset-max-ziplist-entires和zest-max-ziplist-value选项设置类型检查与命令多态reids中用于操作键的命令基本可以分为两种： 可以对任何类型的键执行，如DEL，EXPIRE，RENAME，TYPE，OBJECT 只能对特定类型的键执行，如SET，GET，APPEND，STRLEN等只能对字符串键执行类型检查的实现在执行一个命令之前，redis会对输入键的类型做检查，通过redisObject结构的type属性来实现多态命令的实现redis会根据值对象的类型来判断键是否能执行指定命令之外，还会根据值对象的编码方式，选择正确的命令实现代码来执行命令DEL，EXPIRE和LLEN等命令的区别在于前者是基于类型的多态-一个命令可以同时用于处理多种不同类型的键，后者是基于编码的多态-一个命令可以同时处理多种不同编码内存回收通过引用计数的方式 创建一个新对象时：饮用计数的值会被初始化为1 当对象被一个新程序使用时，引用计数的值加一 对象不再被一个程序使用时，引用计数的值减一 对象的引用计数值为0时，对象所占用的内存会被释放对象共享]]></content>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【从入门到放弃-PHP】答题助手 PHP SWOOLE WebSocket 实战]]></title>
    <url>%2Fd59cfb42.html</url>
    <content type="text"><![CDATA[前言2018年伊始，各种答题赚钱热流席卷了朋友圈，先有百万英雄、冲顶大会成为风口浪尖，又有全民答题推波助澜。 公司适时推出了 《XX答题助手》采用“真人工 · 智能“的方式在直播答题的同时为大家提供参考答案，帮助大家稳、准、快的答对12道题目，分得真金白银。接下来我们以xx答题助手为例聊一聊websocket的应用 背景介绍 答题助手目前的planA如上 客户端采用轮询的方式每秒向服务器发送2次http请求，若有正确答案返回则展示在页面上。这种方式给服务器带来了很大的压力 答题直播时，每一个使用答题助手的用户会给服务器带来2qps的压力。经过优化目前每台服务器能抗约1w/qps的并发，当用户数上来后只能加机器。 分析业务场景，其实大多数的请求是无效的，客户端只需要在答题结果出来后，接收答案并展示即可。因此完全可以采用websocket的方法，在服务端获取到答案时，主动push消息到客户端即可。 Websocket协议Websocket协议是基于tcp协议的应用层协议 主要是为了实现客户端与服务端的全双工通信。 Websocket与HTTPWebsocket与HTTP都是基于TCP协议的应用层协议，Websocket在建立连接时需要先发送HTTP请求与服务器握手，待服务器返回101进行协议转换，从HTTP切成Websocket协议进行通信 握手过程如上：一、客户端：申请协议升级 客户端发起协议升级请求。采用HTTP报文，且仅支持GET方式 含义： Request Method: GET 使用get的方式 Connection: Upgrade 表示要升级协议 Upgrade: websocket 表示要升级的协议是websocket Sec-WebScoket-Version: 13 websocket协议支持的版本号。如果服务端不支持该版本，服务端需要返回一个Sec-WebScoket-Version Header，里面包含支持的版本号 Sec-WebSocket-Key: 7PMYxFH/jxrVsZvKeSTW1Q== 采用base64编码的随机16字节长的字符序列 Sec-WebScocket-Extensions: permessage-deflate; client_max_window_bits 希望采用的扩展协议 二、服务端：响应协议升级 Connection: upgrade 同意升级 Sec-WebSocket-Accept: E1rL2SuyYrDeuDYc5kUQApGBsyg= 服务端根据请求首部的Sec-WebSocket-Key计算出来的 计算方式如下： 1、将Sec-WebSocket-Key和 258EAFA5-E914-47DA-95CA-C5AB0DC85B11 进行拼接 2、经过SHA1计算并转成base64 Sec-WebSocket-Version: 13 升级版本为13 Upgrade: websocket 升级的协议是websocket 三、客户端验证客户端同样通过将Sec-WebSocket-Key和258EAFA5-E914-47DA-95CA-C5AB0DC85B11 拼接经过SHA1计算，转为base64后与Sec-WebSocket-Accept对比，相等则验证成功 至此连接建立，由HTTP协议切换为WebSocket协议 技术方案在了解WebSocket协议原理后 可基于TCP socket通过处理协议及数据帧搭建WebSocket服务。因swoole扩展已经对WebSocket进行了很好的封装以及进程的管理，同时是以C来实现的WebSocket，性能及稳定性都经过了很多大公司的检验，最终选用Swoole进行开发 Swoole扩展安装：下载swoole-2.0.12 table版 解压 进入解压目录 按以下步骤安装 (swoole2.0.12版本起不再支持php5，扩展编译需gcc-4.4+版本) 安装完毕在php.ini配置文件中加上extension=swoole.so即可启用swoole扩展 可使用下面几行代码实现一个最简单的WebSocket Server: 在on方法中注册事件以及其对应的回调函数进行处理。其中onMessage回调函数为必选，否则服务不会启动。用户可以onHandShake回调自定义握手协议，否则将使用Swoole默认的协议握手 广播：直接发http请求能触发onRequest回调，可在回调中遍历connections属性广播请求 注意：connections是一个迭代器对象 并且依赖pcre库，若编译时为安装prce库，此属性无法使用。需yum install pcre-devel 后重新编译swoole扩展使用 动态路由：搭建通用服务，使用一个websocket server提供多种类型的服务，需要根据路由动态选择服务类型和处理逻辑 可在onOpen时获取request对象的request_uri属性来根据url选择不同的路由做处理 onMessage时，无法获取request 需要在消息体内指定request_uri选择路由 性能压测主要测试WebSocket Server能抗住多少长连接和并发，以及push消息时的速度和消息到达率 fork N个进程使用异步非阻塞客户端进行压测 在32核 128G机器上测得部分数据如下： 并发数为55000时，cpu的idle峰值约为87%，链接建立后保持连接时约为99% push消息时97% push55000条消息平均需要200ms，消息送达率为100% 可见WebSocket长连接对机器资源的消耗非常小。 监控为了能在系统负载过大、无法申请到内存、程序被误杀等情况下 能重新拉起server需要有脚本监测 自动启动主进程，脚本如下 停止脚本 需要在Server启动后设置进程名称 至此WebSocket服务的搭建及压测监控都已完成。接下来在完成服务的稳定性、上下线等运维相关的工作后会计划灰度上线 后期结论会继续同步]]></content>
      <categories>
        <category>个人学习</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>WebSocket</tag>
        <tag>Swoole</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【从入门到放弃-MySQL】InnoDB源码分析-基础数据结构和算法]]></title>
    <url>%2F82459e58.html</url>
    <content type="text"><![CDATA[前言最近在学习mysql数据库InnoDB引擎的相关知识，在学习的过程中做了一些摘抄和笔记，同时也有一些自己的理解，希望能分享给大家，共同学习，欢迎指正 InnoDB源码文件先来看一些相关文件及其说明 文件名 说明 dyn0dyn.* 动态数组的实现 fut0fut.* 基于文件的工具集，实现了基于磁盘地址而不是内存地址的双链表 fut0lst.* 操作 ha0ha.* 用于哈希索引系统的哈希表实现 hash0hash.* 简单哈希表实现，用于fil、buf和recv等模块中 mem0. 内存管理系统，包括内存池的实现 ut0byte.* 字节操作实现 ut0dbg.c debug工具 ut0mem.* 内存管理基元，内存块实现 ut0rnd.* 随机值和哈希值操作实现 ut0ut.* 其它常用操作，包括时间、打印、对数和指数操作 ut0sort.h 标准排序算法的宏定义，基于合并排序 ut0lst.h 双向线性链表实现 内存管理 InnoDB存储引擎采用内存堆的方式来进行内存对象的管理（而不是直接使用malloc和free），其优点是可以一次性分配大块的内存，而不是按需分配。这样的分配方式可以将多次的内存分配合并为单次进行，之后的内存请求救可以在InnoDB引擎内部进行，从而减少了频繁调用函数malloc和free进行的时间与潜在的性能开销。 此外，InnoDB存储引擎还允许从缓冲池中分配内存来建立内存堆，这样可以更快的请求整个内存页（通常为16K）。这种分配方法称为缓冲池分配 在这，我联想到之前看到过在php的内核中，内存管理也是采取这样的方式，因为不管是php还是mysql都要经常的申请和释放内存，而只有操作系统才能使用malloc和free操作内存，普通的应用程序是无法直接对内存进行访问的，需要向操作系统申请，向操作系统申请会引发系统调用，在系统调用时，cpu要从用户态切换到内核态，而这个切换的开销是比较大的，如果频繁切换则会有很大性能开销。所以，在对内存有频繁操作的应用程序中，通常每次申请一大块内存来自己维护，同样的在这些内存使用完之后也不立刻归还给操作系统，而是自己留着以备下次使用 内存块的数据结构 内存堆相当于一个栈，通过不断的增加内存块对象来增长空间，InnoDB使用mem_block_t来表示内存堆中每次从操作系统或者缓冲池中分配的内存块。 /innobase/include/mem0mem.h line:43123/* A block of a memory heap consists of the info structurefollowed by an area of memory */typedef struct mem_block_info_t mem_block_t; mem_block_info_t的数据结构如下：12345678910111213141516171819202122232425262728293031323334353637383940414243### /innobase/include/mem0mem.h line:371/** The info structure stored at the beginning of a heap block */struct mem_block_info_t &#123; ulint magic_n;/* magic number for debugging */#ifdef UNIV_DEBUG, char file_name[8];/* file name where the mem heap was created */ ulint line; /*!&lt; line number where the mem heap was created */#endif /* UNIV_DEBUG */ UT_LIST_BASE_NODE_T(mem_block_t) base; /* In the first block in the the list this is the base node of the list of blocks; in subsequent blocks this is undefined */ UT_LIST_NODE_T(mem_block_t) list; /* This contains pointers to next and prev in the list. The first block allocated to the heap is also the first block in this list, though it also contains the base node of the list. */ ulint len; /*!&lt; physical length of this block in bytes */ ulint total_size; /*!&lt; physical length in bytes of all blocks in the heap. This is defined only in the base node and is set to ULINT_UNDEFINED in others. */ ulint type; /*!&lt; type of heap: MEM_HEAP_DYNAMIC, or MEM_HEAP_BUF possibly ORed to MEM_HEAP_BTR_SEARCH */ ulint free; /*!&lt; offset in bytes of the first free position for user data in the block */ ulint start; /*!&lt; the value of the struct field 'free' at the creation of the block */#ifndef UNIV_HOTBACKUP void* free_block; /* if the MEM_HEAP_BTR_SEARCH bit is set in type, and this is the heap root, this can contain an allocated buffer frame, which can be appended as a free block to the heap, if we need more space; otherwise, this is NULL */ void* buf_block; /* if this block has been allocated from the buffer pool, this contains the buf_block_t handle; otherwise, this is NULL */#endif /* !UNIV_HOTBACKUP */#ifdef MEM_PERIODIC_CHECK UT_LIST_NODE_T(mem_block_t) mem_block_list; /* List of all mem blocks allocated; protected by the mem_comm_pool mutex */#endif&#125;; UT_LIST_BASE_NODE_T ### /innobase/include/ut0lst.h line:751234567891011121314This macro expands to the unnamed type definition of a struct which actsas the two-way list base node. The base node contains pointersto both ends of the list and a count of nodes in the list (excludingthe base node from the count).@param TYPE the name of the list node data type */template &lt;typename TYPE&gt;struct ut_list_base &#123; typedef TYPE elem_type; ulint count; /*!&lt; count of nodes in list */ TYPE* start; /*!&lt; pointer to list start, NULL if empty */ TYPE* end; /*!&lt; pointer to list end, NULL if empty */&#125;;#define UT_LIST_BASE_NODE_T(TYPE) ut_list_base&lt;TYPE&gt; 可以看到，有两个指针指向链表头和链表尾部，count存储节点个数 UT_LIST_NODE_T在 ### /innobase/include/ut0lst.h line:751234567template &lt;typename TYPE&gt;struct ut_list_node &#123; TYPE* prev; /*!&lt; pointer to the previous node, NULL if start of list */ TYPE* next; /*!&lt; pointer to next node, NULL if end of list */&#125;;#define UT_LIST_NODE_T(TYPE) ut_list_node&lt;TYPE&gt; 可以看到有两个指针指向前一个和后一个结点 InnoDB 存储引擎定义了三种内存堆类型： /innobase/include/mem0mem.h line:52123456789#define MEM_HEAP_DYNAMIC 0 /* the most common type 堆的内存调用通过内存池接口申请*/#define MEM_HEAP_BUFFER 1 /* 堆的内存调用通过缓冲池申请 */#define MEM_HEAP_BTR_SEARCH 2 /* this flag can optionally be ORed to MEM_HEAP_BUFFER, in which case heap-&gt;free_block is used in some cases for memory allocations, and if it's NULL, the memory allocation functions can return NULL. 是MEM_HEAP_BUFFER的子类型，仅在自适应哈希索引中使用，并且此类型会多维护一个free_block缓冲块 */ 内存堆的创建是由函数mem_heap_create_func完成 /innobase/include/mem0mem.ic line:431123456789101112131415161718192021222324252627282930313233343536/*****************************************************************//**NOTE: Use the corresponding macros instead of this function. Creates amemory heap. For debugging purposes, takes also the file name and line asargument.@return own: memory heap, NULL if did not succeed (only possible forMEM_HEAP_BTR_SEARCH type heaps) */UNIV_INLINEmem_heap_t*mem_heap_create_func(/*=================*/ ulint n, /*!&lt; in: desired start block size, this means that a single user buffer of size n will fit in the block, 0 creates a default size block */#ifdef UNIV_DEBUG const char* file_name, /*!&lt; in: file name where created */ ulint line, /*!&lt; in: line where created */#endif /* UNIV_DEBUG */ ulint type) /*!&lt; in: heap type */&#123; mem_block_t* block; if (!n) &#123; n = MEM_BLOCK_START_SIZE; &#125; block = mem_heap_create_block(NULL, n, type, file_name, line); if (block == NULL) &#123; return(NULL); &#125; UT_LIST_INIT(block-&gt;base); /* Add the created block itself as the first block in the list */ UT_LIST_ADD_FIRST(list, block-&gt;base, block);#ifdef UNIV_MEM_DEBUG mem_hash_insert(block, file_name, line);#endif return(block);&#125; 从上面代码中可以看到mem_heap_create_func主要是调用mem_heap_create_block函数实现内存块的创建的 /innobase/include/mem0mem.ic line:321234567# define mem_heap_create_block(heap, n, type, file_name, line) \ mem_heap_create_block_func(heap, n, file_name, line, type)# define mem_heap_create_at(N, file_name, line) \ mem_heap_create_func(N, file_name, line, MEM_HEAP_DYNAMIC)#else /* UNIV_DEBUG */# define mem_heap_create_block(heap, n, type, file_name, line) \ mem_heap_create_block_func(heap, n, type) 再来追mem_heap_create_block_func函数 /innobase/mem/mem0mem.cc line:296123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116/***************************************************************//**Creates a memory heap block where data can be allocated.@return own: memory heap block, NULL if did not succeed (only possiblefor MEM_HEAP_BTR_SEARCH type heaps) */UNIV_INTERNmem_block_t*mem_heap_create_block_func(/*=======================*/ mem_heap_t* heap, /*!&lt; in: memory heap or NULL if first block should be created */ ulint n, /*!&lt; in: number of bytes needed for user data */#ifdef UNIV_DEBUG const char* file_name,/*!&lt; in: file name where created */ ulint line, /*!&lt; in: line where created */#endif /* UNIV_DEBUG */ ulint type) /*!&lt; in: type of heap: MEM_HEAP_DYNAMIC or MEM_HEAP_BUFFER */&#123;#ifndef UNIV_HOTBACKUP buf_block_t* buf_block = NULL;#endif /* !UNIV_HOTBACKUP */ mem_block_t* block; ulint len; ut_ad((type == MEM_HEAP_DYNAMIC) || (type == MEM_HEAP_BUFFER) || (type == MEM_HEAP_BUFFER + MEM_HEAP_BTR_SEARCH)); if (heap &amp;&amp; heap-&gt;magic_n != MEM_BLOCK_MAGIC_N) &#123; mem_analyze_corruption(heap); &#125; /* In dynamic allocation, calculate the size: block header + data. */ len = MEM_BLOCK_HEADER_SIZE + MEM_SPACE_NEEDED(n);#ifndef UNIV_HOTBACKUP if (type == MEM_HEAP_DYNAMIC || len &lt; UNIV_PAGE_SIZE / 2) &#123; ut_ad(type == MEM_HEAP_DYNAMIC || n &lt;= MEM_MAX_ALLOC_IN_BUF); block = static_cast&lt;mem_block_t*&gt;( mem_area_alloc(&amp;len, mem_comm_pool)); &#125; else &#123; len = UNIV_PAGE_SIZE; if ((type &amp; MEM_HEAP_BTR_SEARCH) &amp;&amp; heap) &#123; /* We cannot allocate the block from the buffer pool, but must get the free block from the heap header free block field */ buf_block = static_cast&lt;buf_block_t*&gt;(heap-&gt;free_block); heap-&gt;free_block = NULL; if (UNIV_UNLIKELY(!buf_block)) &#123; return(NULL); &#125; &#125; else &#123; buf_block = buf_block_alloc(NULL); &#125; block = (mem_block_t*) buf_block-&gt;frame; &#125; if(!block) &#123; ib_logf(IB_LOG_LEVEL_FATAL, &quot; InnoDB: Unable to allocate memory of size %lu.\n&quot;, len); &#125; block-&gt;buf_block = buf_block; block-&gt;free_block = NULL;#else /* !UNIV_HOTBACKUP */ len = MEM_BLOCK_HEADER_SIZE + MEM_SPACE_NEEDED(n); block = ut_malloc(len); ut_ad(block);#endif /* !UNIV_HOTBACKUP */ block-&gt;magic_n = MEM_BLOCK_MAGIC_N; ut_d(ut_strlcpy_rev(block-&gt;file_name, file_name, sizeof(block-&gt;file_name))); ut_d(block-&gt;line = line);#ifdef MEM_PERIODIC_CHECK mutex_enter(&amp;(mem_comm_pool-&gt;mutex)); if (!mem_block_list_inited) &#123; mem_block_list_inited = TRUE; UT_LIST_INIT(mem_block_list); &#125; UT_LIST_ADD_LAST(mem_block_list, mem_block_list, block); mutex_exit(&amp;(mem_comm_pool-&gt;mutex));#endif mem_block_set_len(block, len); mem_block_set_type(block, type); mem_block_set_free(block, MEM_BLOCK_HEADER_SIZE); mem_block_set_start(block, MEM_BLOCK_HEADER_SIZE); if (UNIV_UNLIKELY(heap == NULL)) &#123; /* This is the first block of the heap. The field total_size should be initialized here */ block-&gt;total_size = len; &#125; else &#123; /* Not the first allocation for the heap. This block&apos;s total_length field should be set to undefined. */ ut_d(block-&gt;total_size = ULINT_UNDEFINED); UNIV_MEM_INVALID(&amp;block-&gt;total_size, sizeof block-&gt;total_size); heap-&gt;total_size += len; &#125; ut_ad((ulint)MEM_BLOCK_HEADER_SIZE &lt; len); return(block);&#125; 这个就是最终的创建内存块的函数，主要是对mem_block_t结构的一些赋值操作 到这一步mem_block_t结构的构成、创建和赋值已经算是简单了解完了。可能在学习方法或某些方面会存在不足，希望大家能批评指正。]]></content>
      <tags>
        <tag>MySQL</tag>
        <tag>C++</tag>
        <tag>InnoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【从入门到放弃-Redis】redis的key设计原则]]></title>
    <url>%2F13e11d0d.html</url>
    <content type="text"><![CDATA[设计原则redis key设计技巧1：把表名转换为key前缀 如tag:2：第2段放置用于区分出key的字段-对应mysql中的主键的列名，如userid3：第3段放置主键值，如2,3,4…4：第4段，写要存储的列名 用户表user，转换为key-value存储userid username password email9 lisi 12345 lisi@163.com set user:userid:9:username lisiset user:userid:9:password 12345set user:userid:9:email lisi@163.com keys user:userid:9* 在关系型数据库中，除主键外，还有可能其他列也要查询在k-v数据中，则要相应的生成一条按照该列为主的k-vset user:username:lisi:uid 9 这样我们可以根据user:username:lisi:uid 查出userid=9再查 user:userid:9:password/email 等 就可以了（通过数据冗余来实现） 这样设计在分布式存储时，有个好处，可以避免缓存无底洞现象即：当服务器增多时，key也被散落在更多的节点上，导致要连接的节点也越多于是出现缓存无底洞现象。解决方案：把某一组key，按其共同前缀，分布在同一个节点上 仿微博key-value设计注册用户：userincr global:useridset user:userid:1:username zhangsanset user:userid:1:password 111111 set user:username:zhangsan:userid 1 发微博post:postid:1:time timestamppost:postid:1:userid 3post:postid:1:content ‘this is my home’ incr global:postid 关注关系每人有自己的粉丝记录 set每人有自己的关注记录 set aid 关注 bidfollowing:userid (bid)follower:userdi (aid) 推送表 lpush(‘receivepost:userid’, postid);receivepost:userid 每增加一条微博，将微博id推送至用户接受信息链表 拉取表pull:userid问：上次拉取了A-&gt;5,6,7三条微博，下次刷新时，如何从大于7出开始拉？答：拉取时，设定一个lastpull点，下次拉取时，取大于lastpull的微博 问：关注了很多人，怎么取？答：循环自己的关注列表，逐个取出 问：取出来的微博放在那里？答：pull:userid链表里 问：如何保证个人中心只有一千条答：用ltrim只去前1000条的 问：如果用hash结构存储微博，那么拉取多人的微博内容，在时间上是交错的，如何做到按时间排序答：同步时，取微博后，记录本次取得微博的最大id，下次同步时，只取比最大id更大的微博 ================每人微博的前1000条存于redis,更旧的存于数据库================思路：每人1000条以前的都推到一个global:store链表中用定时任务，取出global：store中的前1000条，存入数据库]]></content>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【从入门到放弃-Redis】redis运维知识]]></title>
    <url>%2Ff885f748.html</url>
    <content type="text"><![CDATA[常用命令time 时间戳及微秒数dbsize 当前数据库下key的数量bgrewriteaof 手动重写aofsave 阻塞当前进程，保存rdb快照bgsave 后台保存rdb快照lastsave 上次保存时间flushall 清空所有dbflushdb 清空当前dbkeys * 查询所有的字段shutdown [save|nosave] 如果不小心运行了flushall，立即shutdown nosave，关闭服务器然后手工编辑aof文件，去掉文件中的”flushall”相关行，然后开启服务器，就可以导入原来数据如果flushall后，系统恰好bgrewriteaof了，那么aof就清空了，数据丢失info 查看redis服务器的信息slowlog get 显示慢查询 其时间是由 slowlog-log-slower-than 10000 来指定，（单位是微秒)问：服务器储存多少条慢查询的记录？答：由slowlog-max-len 128 来限制 redis运维时应注意的参数： 内存Memoryused_memory:952768 数据结构的空间，占用的空间used_memory_rss:919224 理论上应该占用的空间mem_fragmentation_ratio:0.96 前两者的比例，一点几为最佳，如果此值过大，说明redis的内存碎片化严重，可以导出再导入一次 主从复制Replicationrole:slavemaster_host:192.168.1.128master_port:6379master_link_status:up 持久化Persistencerdb_changes_since_last_save:0rdb_lat_save_time:1444737904 config get 配置名 ：获取配置名的值config set 配置名 值 ：把配置名设置为值 rdb服务器间迁移：rdb文件要在没被占用的时候复制 运行时更改master-slave修改一台slave为master1：命令该服务不做其他redis服务的slave命令：slaveof no one2：修改其readonly为yes sentine运维监控配置sentinel monitor def_master 127.0.0.1 6679 2sentinel auth-pass def_master 密码 sentinel down-after-milliseconds de_master 30000 设置实例被认定失效的时间间隔sentinel can-failover def_master yes 当前sentinel 是否允许实施failover(故障转移) sentinel notification-script mymaster /var/redis/notify.sh 出故障后的执行脚本 Redis在PHP中使用php-redis扩展编译 下载最新stable版扩展 解压 进入解压目录下执行/php/path/bin/phpize（检测php的内核版本，并为扩展生成相应的编译配置） configure –with-php-config=/php/path/bin/php-config make&amp;&amp;make install 在php.ini中添加扩展配置 引入编译出的redis.so文件 &lt;?php $redis = new Redis(); //创建一个redis对象 $redis-&gt;open(‘localhost’, 6379); //建立一个连接 $redis-&gt;set(‘name’, ‘lisi’); //set var_dump($redis-&gt;get(‘name’)); //get]]></content>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【从入门到放弃-Redis】redis常见操作]]></title>
    <url>%2Ffacb08f9.html</url>
    <content type="text"><![CDATA[redis的事务及锁应用：开启事务：multi语句：普通命令失败：discard 取消成功：exec 注意：discard和mysql中rollback的区别如果已经成功执行了两条语句，第三条出错，rollback后，前两条语句的影响消失，discard只是结束本次事务，前两条语句的影响仍然存在 在multi后面的语句中，语句出错有两种可能1：语法有问题exec时，报错，所有语句得不到执行 2：语法本身没有问题，但适用对象有问题，比如，zadd操作link对象exec之后，会执行正确的语句，并跳过有不适当的语句 锁的概念：悲观锁：世界上充满危险，肯定有人和我抢，给值上锁，只有我能操作乐观锁：没有那么多人和我抢，我只要注意，在操作的时候有没有人更改值就好 redis使用乐观锁watch key1 key2 … 使用watch监控key1,key2..，如果其中值有变，则不执行事务unwatch 取消所有监控 redis频道发布和消息订阅：publish channel contents 在channel 发布contents 返回值为当前接收订阅人数subscribe channel 订阅channel频道psubscribe channel 订阅channel频道，可以监听多个频道 rdb快照持久化：持久化：把数据存储于断电后不会丢失的设备中，通常是硬盘 常见持久化方式：主从：通过从服务器保存和持久化，如mongoDB的replication sets配置日志：操作生成相关日志，并通过日志来恢复数据 rdb的工作原理：每隔N分钟或者N次写操作后，从内存dump数据形成rdb文件，压缩后反正备份目录 (棕色部分可通过参数来配置） rdb快照相关参数：save 900 1 #刷新快照到硬盘中，必须满足两点要求才会触发，即900秒之后至少有1个关键字发生变化save 300 10 #必须是300秒之后至少10个关键字发生变化save 60 10000 #必须是60秒之后至少10000个关键字发生变化stop-writes-on-basave-error yes #后台存储错误停止写rdbcompression yes #使用LZF压缩rdb文件rdbchecknum yes #存储和加载rdb文件时校验dbfilename dump.rdb #设置rdb文件名dir ./ #设置工作目录，rdb文件写入该目录 aof日志持久化(将操作过的命令，都写到文本文件里)：aof配置：appendonly no #是否打开aof日志功能 appendfsync always #每1个命令，都立即同步到aof。安全、速度慢appendfsync everysec #折衷方案，每秒写入一次appendfsync no #写入工作交给操作系统，由操作系统判断缓冲区大小，统一写到aof，同步频率低，速度快 no-appendfsync-on-write yes #正在导出rdb快照的过程中，要不要停止同步aofauto-aof-rewrite-percentage 100 #aof文件大小比起上次重写时的大小，增长率100%时，重写auto-aof-rewrite-min-size 64mb #aof文件，至少超过64M时，重写 bgrewriteaof 重写日志命令 问：在dump.rdb过程中，aof如果停止同步，会不会丢失？答：不会，所有的操作缓存在内存的队列里，dump完成后，统一操作 问：aof重写是指什么？答：aof重写是指把内存中的数据，逆化成命令，写入到.aof日志里，以解决aof日志过大问题 问：2种持久化是否可以同时用？答：可以，而且推荐这么做，aof和rdb都有，以aof来恢复 问：恢复时rdb和aof恢复那个快？答：rdb快，因为其是数据的内存映射，直接载入到内存，而aof是命令，需要逐条执行 redis主从复制：集群的作用：1：主从备份 防止主机宕机2：读写分离，分担master的任务3：任务分离，如从服务器分别分担备份工作与计算工作 master配置：1：关闭rdb快照（备份工作交给slave)2：可以开启aof slave配置：1：声明slave-of2：配置密码[如果master有密码]3：[某1个]slave打开rdb快照功能4：配置是否只读[slave-read-only]5：pid port 要改 主从复制缺陷：每次slave断开后，再连接master，都要master全部dump出来rdb，再aof，即同步的过程都要重新执行1遍所以多台slave不要同时启动，否则master可能io剧增]]></content>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【从入门到放弃-Redis】redis基础]]></title>
    <url>%2F7d67e2aa.html</url>
    <content type="text"><![CDATA[redis是开源的,bsd许可，高级的ky-value存储系统。 可以用来存储字符串，哈希结构，链表，集合，因此常用来提供数据结构服务 redis和memcached相比的独特之处：1：redis可以用来做存储（storage），而memcacahed是来做缓存（cache）,这个特点主要是因为其有“持久化”功能 2.存储的数据有“结构”，对于memcached来说，存储的结构，只有一种类型-“字符串”，而redis则可以存储字符链表，哈希结构，集合，有序结合 redis安装不用config 直接make就好，注意时间信息make PREFIX=/usr/local/redis install安装目录下：redis-benchmark ：性能测试工具redis-check-aof：检查aof日志的工具redis-checkdump：检查rdb日志的工具redis-cli：链接用的客户端redis-server：链接用的服务进程 redis默认有16个数据库 从0编号 默认在0号数据库操作 通用命令：keys (pattern) 查询相应的key在redis里允许模糊查询key有3个通配符,?,[] ：通配任意多个字符?：通配单个 字符[]：通配括号内的某一个字符 set key value 设置k-vrandomkey 返回随机的keyexists key 判断有没有该key 返回1有,0没有type key 判断key的数据类型del key 删除该keyrename key newkey 给key改名字renamenx key newkey 如果newkey不存在则改名move key db 把key移动到db号数据库select db 选择db号数据库 生命周期：expire key 整型值 设置key的生命周期为多少秒pexpire key 整型值 设置key的生命周期为多少毫秒ttl key 查询key的生命周期 以秒返回生命周期 对不存在的、已过期的和永久有效的都返回-1pttl key 查询key的生命周期 以毫秒返回生命周期 对不存在的、已过期的和永久有效的都返回-1persist key 把key设置成永久有效 字符串类型的操作：set key value [ex 秒数]/[px 毫秒数] [nx]/[xx]如果ex px同时写，以后面的为准nx：表示key 不存在时候，执行操作xx：表示key存在的时候，执行操作（相当于更改） mset multi set 一次性设置多个键值如：mset k1 v1 k2 v2 k3 v3… get key 获取key的valuemget key1 key2 key3… 获取多个key的value setrange key n value 把key的偏移n位开始设置成valuegetrange key start stop 在[start, stop]中获取字符串append key value 在key后追加value getset key newvalue 获取到key的值并把value设置成newvalue incr key key加一decr key key减一 incrby key 整型值 key增加整型值decrby key 整型值 key减去整型值 incrbyfloat key 浮点数 key增加浮点数 setbit keyoffset value 设置offset对应的二进制位上的值，返回该位上的旧值注意：如果offset过大则会在中间填充0offset最大为2^32-1 可推断出字符串最大为512M char的大小写转换：A 65 0100 0001a 97 0110 0001所以将第二位的值做改变就oksetbit char 2 1 变小写setbit char 2 0 变大写 bitop opreation destkey key1 [key2…] 对多个key进行位操作，结果放在destkey中operation 有 and、or、not、xor等操作 链表类型的操作：lpush list value [value2, value3, …] 在list左边插入valuerpush list value [value2, value3, …] 在list右边插入value lpop list value 把list最左边的元素删除rpop list value 把list最右边的元素删除 lrange list start end 查看list中[start, stop]的元素小技巧：lrange list 0 -1 查看list中的所有元素 lrem key count value 从链表中删除value值，删除count的绝对值个value后结束count&gt;0 从表头开始删count&lt;0 从表尾开始删 ltrim list start end 截取list中[start, end]的元素作为list的元素 lindex list n 获取list中索引n处的值 llen list 计算list中元素的个数 linsert list after|before search value 在list中寻找search，并在search值之后 | 之前，插入value注：一旦找到search之后会直接插入，不会插入多个value rpoplpush source dest 把source的尾部取出来放在dest的头部场景：task+bak 双链表完成安全队列业务逻辑：1：rpoplpush task bak2：接收返回值，并作业务处理3：如果成功则rpop bak 清除任务，如果不成功，下次从bak表中取任务 brpop | blpop list timeout 等待弹出list的尾 | 头元素 timeout为等待超时时间 如果timeout为0则一直等待场景：长轮询Ajax，在线聊天时能用到 redis 应用场景：1：一亿个用户，用户有频繁登陆的，也有不经常登陆的2：如何来记录用户的登录信息3：如何来查询活跃用户，【如一周内连续登陆】 思路：用位图法，使用位来存储，0表示当天未登陆，1表示当天登陆 redis 实现 setbit mon 100000000 0 初始化一亿位的0setbit mon 1 1 设置1号用户的对应位为1，表示当天已登陆 优点：节省存储空间、计算统计方便 无序集合类型的操作：集合特点：无序性、确定性、唯一性注意：string和list可以通过range来访问其中的某几个字符或某几个元素，但因为集合的无序性，无法通过下标或者范围来访问部分元素因此，想看集合中的元素，要么随机选一个，要么全选 sadd key value1 value2 往集合中添加元素srem key value1 value2 从集合中删除元素 spop key 随机返回并删除集合key中的一个元素适合场景：抽签 每次抽出一个元素 srandmember key 随机返回集合key中的一个元素，不删除smembers key 返回key中所有元素sismember key value 判断value是否在key中 ，存在返回1，不存在返回0 scard key 返回key中元素的个数 smove source dest vlaue 把source集合中的value移动到dest集合中 sinter key1 key2 key3 返回key1、key2、key3的交集 sunion key1 key2 key3 返回key1、key2、key3的并集 sdiff key1 key2 key3 返回key1、key2、key3的差集 sinterstore dest key1 key2 key3 将key1、key2、key3的交集保存至dest sunionstore dest key1 key2 key3 将key1、key2、key3的并集保存至dest sdiffstore dest key1 key2 key3 将key1、key2、key3的差集保存至dest 有序集合类型的操作：zadd key score1 value1 score2 value2 …添加有序链表至key，score为权重，系统会根据score自动排序 zrange key start stop [withscores]把集合排序后，返回名次[start,stop]的元素，默认是升序排列，withscores把score打印出来 zrangebyscore key min max [withscores] limit offset N集合（升序）排序后，取出score在[min, max]内的元素，并跳过offset个，取出N个 zrank key value 返回value在key中的位置 zremrangebyrank key start end 按照排名删除元素，删除名次在[start, end]之间的 zremrangebyscore key min max 按照score来删除元素，删除score在[min, max]之间的 zrem key value 删除key中值为value的元素 zcard key 返回key中元素的个数 zcount key min max 返回在区间[min, max]中的元素的个数 zinterstore|zunionstore dest numkeys key1 [key2…][WEIGHTS weight1 [weight2…] ][AGGREGATE SUM|MIN|MAX]求key1 key2的交|并集，key1 key2的权重分别为weight1 weight2聚合方法用：sum|min|max聚合结果，保存在dest集合内 注意：weights,aggregate 如何理解？答：如果有交集，交集元素又有score,score如何处理aggregate sum-&gt;score相加，min求最小score,max求最大score另：可以通过weight设置不同key的权重，交集时，score*weights Hash类型的操作：hset key field value 设置key下字段field的值为valuehmset key field1 value1 [field2 value2 …] 设置多个键值对 hget key field 返回key中field域的值hmget key field1 field2 field3… 返回key中field1 filed2 field3…的值 hgetall key 返回key中，所有域与其值 hdel key field 删除key中field域 hlen key 返回key中元素的数量 hexists key field 判断key中有没有field域 hincrby key field value 把key中的field域的值增长整型值valuehincrbyfloat key field value 把key中的field域的值增长浮点型值value hkeys key 返回key中所有的filedhvals key 返回key中所有的value]]></content>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
</search>
